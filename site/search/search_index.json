{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lernsituationen zum Lernfeld 10c f\u00fcr den Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) Auf den folgenden Seiten finden Sie die SchuCu konformen Lernsituationen f\u00fcr den Fachinformatiker f\u00fcr Daten- und Prozessanalyse. Kompetenzformulierung gem\u00e4\u00df Rahmenlehrplan \" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Lernsituationen als PDF Die Lernsituationen werden automatisch generiert aus Markdown Dateien und k\u00f6nnen auch als pdf herunter geladen werden. LS1 KI im Alltag LS2 Datenexploration und -Visualisierung LS3 Entscheidungsb\u00e4ume LS4 K-mean Clustering LS5 Regressionsanalyse LS6 K-nearest Neighbor (KNN) LS7 Neuronale Netze LS8 Cloud KI Systeme nutzen LS9 Verst\u00e4rkendes Lernen","title":"Home"},{"location":"#lernsituationen-zum-lernfeld-10c-fur-den-fachinformatiker-fur-daten-und-prozessanalyse-fidp","text":"Auf den folgenden Seiten finden Sie die SchuCu konformen Lernsituationen f\u00fcr den Fachinformatiker f\u00fcr Daten- und Prozessanalyse.","title":"Lernsituationen zum Lernfeld 10c f\u00fcr den Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP)"},{"location":"#kompetenzformulierung-gema-rahmenlehrplan","text":"\" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte.","title":"Kompetenzformulierung gem\u00e4\u00df Rahmenlehrplan"},{"location":"#lernsituationen-als-pdf","text":"Die Lernsituationen werden automatisch generiert aus Markdown Dateien und k\u00f6nnen auch als pdf herunter geladen werden. LS1 KI im Alltag LS2 Datenexploration und -Visualisierung LS3 Entscheidungsb\u00e4ume LS4 K-mean Clustering LS5 Regressionsanalyse LS6 K-nearest Neighbor (KNN) LS7 Neuronale Netze LS8 Cloud KI Systeme nutzen LS9 Verst\u00e4rkendes Lernen","title":"Lernsituationen als PDF"},{"location":"Dateien_als_Datenquelle/","text":"Dateien als Datenquellen Handlungssituation Eine der ersten Kunde der neu eingef\u00fchrten Abteilung \"Daten- und Prozessanalyse\" der ChangeIT GmbH ist eine gro\u00dfe berufsbildenden Schule. Diese Schule m\u00f6chte gerne die Leistungsdaten eines Jahrgangs ausgewertet haben. Diese Daten liegen sowohl als csv , xml und json vor. Daten der Sch\u00fclergruppe CSV Darstellung Eine Datei Moodle.csv enth\u00e4lt die Daten in Form von CSV. ID-Nummer;Abteilung;Klassenarbeit;K1;K2;K3;K4;K5;K6;K7 1;J;100.00 %;90.00 %;100.00 %;92.86 %;100.00 %;100.00 %;100.00 %;100.00 % 2;J;80.62 %;70.00 %;100.00 %;64.29 %;72.22 %;77.27 %;83.33 %;93.58 % 3;J;95.07 %;95.00 %;100.00 %;100.00 %;100.00 %;100.00 %;100.00 %;100.00 % JSON Darstellung Eine Weitere Datei Moodle.json enth\u00e4lt die Daten in JSON Form. [ { \"ID-Nummer\" : 1 , \"Abteilung\" : \"J\" , \"Klassenarbeit\" : \"100.00 %\" , \"K1\" : \"90.00 %\" , \"K2\" : \"100.00 %\" , \"K3\" : \"92.86 %\" , \"K4\" : \"100.00 %\" , \"K5\" : \"100.00 %\" , \"K6\" : \"100.00 %\" , \"K7\" : \"100.00 %\" } ] XML Darstellung Eine weitere Datei Moodle.xml erh\u00e4lt die Daten in Form einer XML Datei. <root> <row> <ID-Nummer> 1 </ID-Nummer> <Abteilung> J </Abteilung> <Klassenarbeit> 100.00 % </Klassenarbeit> <K1> 90.00 % </K1> <K2> 100.00 % </K2> <K3> 92.86 % </K3> <K4> 100.00 % </K4> <K5> 100.00 % </K5> <K6> 100.00 % </K6> <K7> 100.00 % </K7> </row> </root> Attribute des Datensatzes Die einzelnen Attribute des Datensatzes haben dabei folgende Bedeutung. ID-Nummer : Eindeutige ID eines Sch\u00fclers Abteilung : Jeweilige Klasse des Sch\u00fclers (J-N) Klassenarbeit : Ergebnis der Klassenarbeit K1 bis K7 : Ergebnisse in einzelnen Kapitel-Tests. Arbeitsprozess Im weiteren Verlauf der Kurses werden wir zun\u00e4chst immer wieder folgenden Arbeitsprozess anwenden. Wir werden zun\u00e4chst die Rohdaten aus einer Datenquelle einlesen. In unserem Fall ist das aktuell eine Datei. Anschlie\u00dfend werden wir die Daten anpassen, bereinigen oder vorauswerten. Abschlie\u00dfend werden wir das Ergebnis ansprechend f\u00fcr den Kunden visualisieren. Einlesen der Dateien Hinweise F\u00fcr die Analyse und Visualisierung der Daten verwenden wir das Python Paket pandas verwenden. Wenn Sie diese Pakete noch nicht installiert haben, so holen Sie dieses bitte nach. Ein Paket in Python installiert man \u00fcber den Python Paket Manger PIP wie folgt: pip install {Name des Paketes} Aufgabe 1: Daten einlesen Entscheiden Sie sich f\u00fcr ein Dateiformat welches Sie bearbeiten wollen? Schreiben Sie ein Python Programm welches die jeweilige Datei einlie\u00dft. Dabei sollten die Daten gleich in einem DataFrame (Datenformat f\u00fcr die Auswertung mit pandas ) eingelesen werden. Geben Sie nach dem Einlesen des Datensatzes das Dataframe im Jupyter Notebook aus. Die Ausgabe sollte der folgenden Abbildung entsprechen. Diskutieren Sie in ihrer Gruppe, ob die Daten bereits so wie Sie vorliegen verwendet werden k\u00f6nnen, oder ob noch Bereinigungen der Daten notwendig sind? Bereinigen der Dateien Leider sind die Daten f\u00fcr die weitere Bearbeitung nicht geeignet und m\u00fcssen entsprechend angepasst werden. Folgende Probleme wurden in den Daten festgestellt. Die Notenwerte liegen in den Daten als Zeichenketten vor, sinnvoller w\u00e4re es hier, wenn die Daten als float vorliegen w\u00fcrden. Wenn ein Sch\u00fcler einen Kapitel-Test nicht mitgeschrieben hat, so ist der Datensatz mit '-' gekennzeichnet. Sinnvoller w\u00e4re es, wenn hier der Wert 0.0 eingetragen w\u00e4re. Aufgabe 2: Daten umwandeln / bereinigen Wandeln Sie die Werte des DataFrames entsprechend der oben durchgef\u00fchrten \u00dcberlegungen um und geben Sie anschlie\u00dfend das DataFrame Objekt im Jupyter Notebook aus und kontrollieren Sie die Ausgabe. Hinweis : \u00dcber dftypes erhalten Sie Auskunft \u00fcber die Datentypen eines Dataframes print ( df . dtypes ) Erste Statistische Grunddaten ermitteln Wir wollen nun die Daten einer ersten statistischen Untersuchung unterziehen. Dazu sollen wichtige statistische Grunddaten ermittelt werden. Das arithmetische Mittel eines Datenmenge berechnet sich wie folgt: \\(\\bar{x}=\\frac{1}{N}*\\sum_{i=1}^{N}x_{i}\\) Die Varianz \\(var(x)\\) einer Datenmenge X berechnet sich wie folgt: \\(var(x)=\\frac{1}{N-1}*\\sum_{i=1}^{N}( x_{i} - \\bar{x})^{2}\\) Die Standardabweichung \\(\\sigma\\) ist dann die Quadratwurzel der Varianz. \\(\\sigma=\\sqrt{var(x))}\\) Dies Werte lassen sich auch mittels der pandas -Bibliothek ermitteln: mean(column) ermittelt den arithmetischen Mittelwert einer Spalte median(column) ermittelt das Mittel (mean) einer Spalte std(column) ermittelt die Standardabweichung einer Spalte Aufgabe 3: statistische Grunddaten ermitteln Ermitteln Sie mit Hilfe von pandas folgende Daten. Durchschnittswert (in %) der Klassenarbeit Standradabweichung der Klassenarbeit Durchschnittswert (in %) der Klassenarbeit der Gruppe \"J\" Notenspiegel der Klassenarbeit (also wie viele Sch\u00fcler haben ein \"seht gut\", wie viele Sch\u00fcler ein \"gut\" usw.) Schreiben Sie diese Werte f\u00fcr den Auftraggeber in ein Word Dokument! Diese Word Dokument soll dabei nir nur den Zahlenwert beinhalten, sondern eine kurze Beschreibung f\u00fcr den Auftraggeber zu diesem Wert enthalten. Hinweise : - Nutzen Sie zum ermitteln der Daten die Dokumentation zum pandas Dataframe . - Zum bestimmen der Noten soll der IHK Notenschl\u00fcssel herangezogen werden (siehe unten) ungen\u00fcgend mangelhaft ausreichend befriedigend gut sehr-gut 0 - < 30 30 - < 50 50 - < 66 66 - < 81 81 - < 92 92 - 100 Daten visualisieren Der Auftraggeber (die Schule) m\u00f6chte die Daten zur weiteren Verarbeitung in Dokumenten visualisiert haben. Zur Visualisierung von Daten nutzen wir das python Paket plot aus dem pandas Paket. F\u00fcr die folgenden Aufgaben stellt Ihnen der Leiter der Abteilung Datenanalyse folgenden Beispielcode zur Verf\u00fcgung. Aufgabe 4 Daten visualisieren Der Auftraggeber w\u00fcrde gerne das Ergebnis der Klassenarbeit in geeigneter Form visualisiert haben. Entscheiden Sie sich f\u00fcr ein geeignete Darstellungsform und erstellen Sie mit Hilfe eines Python Programms die Grafik. Hinweis : In dem unten dargestellten Beispiel werden Werte in Form eines Tortendiagramms dargestellt. import pandas as pd import matplotlib.pyplot as plt data = pd . DataFrame ([ 1 , 2 , 3 , 4 , 5 , 6 ]) pieimage = data . plot . pie ( subplots = True ) ax = pieimage [ 0 ] ax . set_title ( \"Titel der Grafik\" ) ax . legend ([ \"A\" , \"B\" , \"C\" , \"D\" , \"E\" , \"F\" ]) plt . show () Hinweis : In dem unten dargestellten Beispiel werden Werte in Form eines Balkendiagramms dargestellt. import pandas as pd data = pd . DataFrame ([ 1 , 2 , 3 , 4 , 5 , 6 ]) barimage = data . plot . bar () barimage . set_title ( \"Titel Deiner Grafik\" ) Aufgabe 5 Daten visualisieren Es stellt dich die Frage, ob Sch\u00fcler die im arithmetischen Mittel in den Kapitel-Tests eine gute Note schreiben auch in der Klassenarbeit eine gute Note schreiben. Versuchen Sie auf diese Fragestellung eine Antwort zu finden und visualisieren Sie das Ergebnis m\u00f6glichst sinnvoll f\u00fcr den Auftragsgeber. Hinweis : In dem unten dargestellten Beispiel werden mit Hilfe von pandas Punkte mit (x/y) Werten in einem Koordinatensystem dargestellt. import pandas as pd data = pd . DataFrame ({ \"x\" : [ 11 , 37 , 98 , 82 ], \"y\" : [ 8 , 5 , 85 , 95 ]}) pltimage = data . plot . scatter ( x = \"x\" , y = \"y\" ) pltimage . set_xlabel ( \"Klassenarbeit\" ) pltimage . set_ylabel ( \"Durchschn. K1 bis K7\" ) pltimage . set_title ( \"\u00dcbung 2\" ) Reflexion Diskutieren Sie im Klassenverband die Aussagekraft der statischen Grunddaten und der erzeugten Diagramme. In wie weit k\u00f6nnen die Daten und die Grafiken genutzt werden, um ein Vorhersagemodell zu entwickeln, welches es erm\u00f6glicht die Leistungen eines neuen Sch\u00fclers in der Klasse vorherzusagen?","title":"Dateien als Datenquellen"},{"location":"Dateien_als_Datenquelle/#dateien-als-datenquellen","text":"","title":"Dateien als Datenquellen"},{"location":"Dateien_als_Datenquelle/#handlungssituation","text":"Eine der ersten Kunde der neu eingef\u00fchrten Abteilung \"Daten- und Prozessanalyse\" der ChangeIT GmbH ist eine gro\u00dfe berufsbildenden Schule. Diese Schule m\u00f6chte gerne die Leistungsdaten eines Jahrgangs ausgewertet haben. Diese Daten liegen sowohl als csv , xml und json vor.","title":"Handlungssituation"},{"location":"Dateien_als_Datenquelle/#daten-der-schulergruppe","text":"","title":"Daten der Sch\u00fclergruppe"},{"location":"Dateien_als_Datenquelle/#csv-darstellung","text":"Eine Datei Moodle.csv enth\u00e4lt die Daten in Form von CSV. ID-Nummer;Abteilung;Klassenarbeit;K1;K2;K3;K4;K5;K6;K7 1;J;100.00 %;90.00 %;100.00 %;92.86 %;100.00 %;100.00 %;100.00 %;100.00 % 2;J;80.62 %;70.00 %;100.00 %;64.29 %;72.22 %;77.27 %;83.33 %;93.58 % 3;J;95.07 %;95.00 %;100.00 %;100.00 %;100.00 %;100.00 %;100.00 %;100.00 %","title":"CSV Darstellung"},{"location":"Dateien_als_Datenquelle/#json-darstellung","text":"Eine Weitere Datei Moodle.json enth\u00e4lt die Daten in JSON Form. [ { \"ID-Nummer\" : 1 , \"Abteilung\" : \"J\" , \"Klassenarbeit\" : \"100.00 %\" , \"K1\" : \"90.00 %\" , \"K2\" : \"100.00 %\" , \"K3\" : \"92.86 %\" , \"K4\" : \"100.00 %\" , \"K5\" : \"100.00 %\" , \"K6\" : \"100.00 %\" , \"K7\" : \"100.00 %\" } ]","title":"JSON Darstellung"},{"location":"Dateien_als_Datenquelle/#xml-darstellung","text":"Eine weitere Datei Moodle.xml erh\u00e4lt die Daten in Form einer XML Datei. <root> <row> <ID-Nummer> 1 </ID-Nummer> <Abteilung> J </Abteilung> <Klassenarbeit> 100.00 % </Klassenarbeit> <K1> 90.00 % </K1> <K2> 100.00 % </K2> <K3> 92.86 % </K3> <K4> 100.00 % </K4> <K5> 100.00 % </K5> <K6> 100.00 % </K6> <K7> 100.00 % </K7> </row> </root>","title":"XML Darstellung"},{"location":"Dateien_als_Datenquelle/#attribute-des-datensatzes","text":"Die einzelnen Attribute des Datensatzes haben dabei folgende Bedeutung. ID-Nummer : Eindeutige ID eines Sch\u00fclers Abteilung : Jeweilige Klasse des Sch\u00fclers (J-N) Klassenarbeit : Ergebnis der Klassenarbeit K1 bis K7 : Ergebnisse in einzelnen Kapitel-Tests.","title":"Attribute des Datensatzes"},{"location":"Dateien_als_Datenquelle/#arbeitsprozess","text":"Im weiteren Verlauf der Kurses werden wir zun\u00e4chst immer wieder folgenden Arbeitsprozess anwenden. Wir werden zun\u00e4chst die Rohdaten aus einer Datenquelle einlesen. In unserem Fall ist das aktuell eine Datei. Anschlie\u00dfend werden wir die Daten anpassen, bereinigen oder vorauswerten. Abschlie\u00dfend werden wir das Ergebnis ansprechend f\u00fcr den Kunden visualisieren.","title":"Arbeitsprozess"},{"location":"Dateien_als_Datenquelle/#einlesen-der-dateien","text":"","title":"Einlesen der Dateien"},{"location":"Dateien_als_Datenquelle/#hinweise","text":"F\u00fcr die Analyse und Visualisierung der Daten verwenden wir das Python Paket pandas verwenden. Wenn Sie diese Pakete noch nicht installiert haben, so holen Sie dieses bitte nach. Ein Paket in Python installiert man \u00fcber den Python Paket Manger PIP wie folgt: pip install {Name des Paketes}","title":"Hinweise"},{"location":"Dateien_als_Datenquelle/#aufgabe-1-daten-einlesen","text":"Entscheiden Sie sich f\u00fcr ein Dateiformat welches Sie bearbeiten wollen? Schreiben Sie ein Python Programm welches die jeweilige Datei einlie\u00dft. Dabei sollten die Daten gleich in einem DataFrame (Datenformat f\u00fcr die Auswertung mit pandas ) eingelesen werden. Geben Sie nach dem Einlesen des Datensatzes das Dataframe im Jupyter Notebook aus. Die Ausgabe sollte der folgenden Abbildung entsprechen. Diskutieren Sie in ihrer Gruppe, ob die Daten bereits so wie Sie vorliegen verwendet werden k\u00f6nnen, oder ob noch Bereinigungen der Daten notwendig sind?","title":"Aufgabe 1: Daten einlesen"},{"location":"Dateien_als_Datenquelle/#bereinigen-der-dateien","text":"Leider sind die Daten f\u00fcr die weitere Bearbeitung nicht geeignet und m\u00fcssen entsprechend angepasst werden. Folgende Probleme wurden in den Daten festgestellt. Die Notenwerte liegen in den Daten als Zeichenketten vor, sinnvoller w\u00e4re es hier, wenn die Daten als float vorliegen w\u00fcrden. Wenn ein Sch\u00fcler einen Kapitel-Test nicht mitgeschrieben hat, so ist der Datensatz mit '-' gekennzeichnet. Sinnvoller w\u00e4re es, wenn hier der Wert 0.0 eingetragen w\u00e4re.","title":"Bereinigen der Dateien"},{"location":"Dateien_als_Datenquelle/#aufgabe-2-daten-umwandeln-bereinigen","text":"Wandeln Sie die Werte des DataFrames entsprechend der oben durchgef\u00fchrten \u00dcberlegungen um und geben Sie anschlie\u00dfend das DataFrame Objekt im Jupyter Notebook aus und kontrollieren Sie die Ausgabe. Hinweis : \u00dcber dftypes erhalten Sie Auskunft \u00fcber die Datentypen eines Dataframes print ( df . dtypes )","title":"Aufgabe 2: Daten umwandeln / bereinigen"},{"location":"Dateien_als_Datenquelle/#erste-statistische-grunddaten-ermitteln","text":"Wir wollen nun die Daten einer ersten statistischen Untersuchung unterziehen. Dazu sollen wichtige statistische Grunddaten ermittelt werden. Das arithmetische Mittel eines Datenmenge berechnet sich wie folgt: \\(\\bar{x}=\\frac{1}{N}*\\sum_{i=1}^{N}x_{i}\\) Die Varianz \\(var(x)\\) einer Datenmenge X berechnet sich wie folgt: \\(var(x)=\\frac{1}{N-1}*\\sum_{i=1}^{N}( x_{i} - \\bar{x})^{2}\\) Die Standardabweichung \\(\\sigma\\) ist dann die Quadratwurzel der Varianz. \\(\\sigma=\\sqrt{var(x))}\\) Dies Werte lassen sich auch mittels der pandas -Bibliothek ermitteln: mean(column) ermittelt den arithmetischen Mittelwert einer Spalte median(column) ermittelt das Mittel (mean) einer Spalte std(column) ermittelt die Standardabweichung einer Spalte","title":"Erste Statistische Grunddaten ermitteln"},{"location":"Dateien_als_Datenquelle/#aufgabe-3-statistische-grunddaten-ermitteln","text":"Ermitteln Sie mit Hilfe von pandas folgende Daten. Durchschnittswert (in %) der Klassenarbeit Standradabweichung der Klassenarbeit Durchschnittswert (in %) der Klassenarbeit der Gruppe \"J\" Notenspiegel der Klassenarbeit (also wie viele Sch\u00fcler haben ein \"seht gut\", wie viele Sch\u00fcler ein \"gut\" usw.) Schreiben Sie diese Werte f\u00fcr den Auftraggeber in ein Word Dokument! Diese Word Dokument soll dabei nir nur den Zahlenwert beinhalten, sondern eine kurze Beschreibung f\u00fcr den Auftraggeber zu diesem Wert enthalten. Hinweise : - Nutzen Sie zum ermitteln der Daten die Dokumentation zum pandas Dataframe . - Zum bestimmen der Noten soll der IHK Notenschl\u00fcssel herangezogen werden (siehe unten) ungen\u00fcgend mangelhaft ausreichend befriedigend gut sehr-gut 0 - < 30 30 - < 50 50 - < 66 66 - < 81 81 - < 92 92 - 100","title":"Aufgabe 3: statistische Grunddaten ermitteln"},{"location":"Dateien_als_Datenquelle/#daten-visualisieren","text":"Der Auftraggeber (die Schule) m\u00f6chte die Daten zur weiteren Verarbeitung in Dokumenten visualisiert haben. Zur Visualisierung von Daten nutzen wir das python Paket plot aus dem pandas Paket. F\u00fcr die folgenden Aufgaben stellt Ihnen der Leiter der Abteilung Datenanalyse folgenden Beispielcode zur Verf\u00fcgung.","title":"Daten visualisieren"},{"location":"Dateien_als_Datenquelle/#aufgabe-4-daten-visualisieren","text":"Der Auftraggeber w\u00fcrde gerne das Ergebnis der Klassenarbeit in geeigneter Form visualisiert haben. Entscheiden Sie sich f\u00fcr ein geeignete Darstellungsform und erstellen Sie mit Hilfe eines Python Programms die Grafik. Hinweis : In dem unten dargestellten Beispiel werden Werte in Form eines Tortendiagramms dargestellt. import pandas as pd import matplotlib.pyplot as plt data = pd . DataFrame ([ 1 , 2 , 3 , 4 , 5 , 6 ]) pieimage = data . plot . pie ( subplots = True ) ax = pieimage [ 0 ] ax . set_title ( \"Titel der Grafik\" ) ax . legend ([ \"A\" , \"B\" , \"C\" , \"D\" , \"E\" , \"F\" ]) plt . show () Hinweis : In dem unten dargestellten Beispiel werden Werte in Form eines Balkendiagramms dargestellt. import pandas as pd data = pd . DataFrame ([ 1 , 2 , 3 , 4 , 5 , 6 ]) barimage = data . plot . bar () barimage . set_title ( \"Titel Deiner Grafik\" )","title":"Aufgabe 4 Daten visualisieren"},{"location":"Dateien_als_Datenquelle/#aufgabe-5-daten-visualisieren","text":"Es stellt dich die Frage, ob Sch\u00fcler die im arithmetischen Mittel in den Kapitel-Tests eine gute Note schreiben auch in der Klassenarbeit eine gute Note schreiben. Versuchen Sie auf diese Fragestellung eine Antwort zu finden und visualisieren Sie das Ergebnis m\u00f6glichst sinnvoll f\u00fcr den Auftragsgeber. Hinweis : In dem unten dargestellten Beispiel werden mit Hilfe von pandas Punkte mit (x/y) Werten in einem Koordinatensystem dargestellt. import pandas as pd data = pd . DataFrame ({ \"x\" : [ 11 , 37 , 98 , 82 ], \"y\" : [ 8 , 5 , 85 , 95 ]}) pltimage = data . plot . scatter ( x = \"x\" , y = \"y\" ) pltimage . set_xlabel ( \"Klassenarbeit\" ) pltimage . set_ylabel ( \"Durchschn. K1 bis K7\" ) pltimage . set_title ( \"\u00dcbung 2\" )","title":"Aufgabe 5 Daten visualisieren"},{"location":"Dateien_als_Datenquelle/#reflexion","text":"Diskutieren Sie im Klassenverband die Aussagekraft der statischen Grunddaten und der erzeugten Diagramme. In wie weit k\u00f6nnen die Daten und die Grafiken genutzt werden, um ein Vorhersagemodell zu entwickeln, welches es erm\u00f6glicht die Leistungen eines neuen Sch\u00fclers in der Klasse vorherzusagen?","title":"Reflexion"},{"location":"Entscheidungsbaume/","text":"Entscheidungsb\u00e4ume Handlungssituation Ein gro\u00dfer Landmaschinenhersteller w\u00fcnscht sich eine gr\u00f6\u00dfere Kundenbindung und beauftragt die ChangeIT GmbH mit der Entwicklung einer App, die Landwirten Empfehlungen gibt, wann der Weizen zu ernten ist. Die App misst dazu \u00fcber einen via Bluetooth gekoppelten Feuchte-Sensor die Bodenfeuchte im Feld und kann \u00fcber eine API Abfrage die Regenwahrscheinlichkeit bestimmen. Erste Erfahrungswerte liegen bereits vor und werden vom Landmaschinenhersteller der ChangeIT in Form einer CSV Datei zur Verf\u00fcgung gestellt. Als Mitglied der Abteilung Daten- und Prozessanalyse erhalten Sie die Aufgabe ein geeignetes Vorhersagemodell zu entwickeln.[^1] [^1]: Vgl. Brandt, Y., Eickhoff-Schachtebeck, A. und Strecker, K. (2022): \u201eSchulbuch starkeSeiten Informatik Jahrgang 9/10 Differenzierende Ausgabe Niedersachsen\u201c, Klett-Verlag 2022 Die zur Verf\u00fcgung gestellten Daten Die Daten die uns der Auftraggeber zur Verf\u00fcgung stellt liegen in Form einer csv Datei vor. \"Feuchte\",\"Regenwahrscheinlichkeit\",\"Ergebnis\" \"17.94\",\"56.89\",\"Warten\" \"15.01\",\"80.88\",\"Ernten\" \"15.56\",\"75.99\",\"Warten\" \"18.25\",\"36.97\",\"Warten\" \"16.52\",\"90.11\",\"Warten\" \"15.26\",\"27.82\",\"Warten\" Die Spalten haben dabei folgende Bedeutung: Feuchte: Der Wert der Bodenfeuchte Regenwahrscheinlichkeit: Die Regenwahrscheinlichkeit in % Ergebnis: Die bisherigen Erfahrungswerte, wann es sich lohnt den Weizen zu Ernten oder besser noch zu Warten . Aufgabe 1: Visualisierung der Daten Zun\u00e4chst sollen die Daten in einem Diagramm visualisiert werden, um sich eine Vorstellung von den Daten zu machen. Schreiben Sie ein erste Python-Programm, welches m\u00f6glichst anschaulich den zur Verf\u00fcgung gestellten Datensatz ( ErnteBauern.csv ) visualisiert. Hinweise: Einlesen kann man eine CSV Datei in Python auf unterschiedlichste Weise. Zur sp\u00e4teren Datenanalyse ist jedoch die Bibliothek pandas das geeignete Mittel. Hier existiert bereits eine Methode read_csv die dieses erledigt. import pandas as pd # CSV-Datei laden data = pd . read_csv ( 'Datei.csv' ) Zum Darstellen von Datenmenge nutzen wir die Bibliothek matplotlib.pyplot . Diese muss zun\u00e4chst importiert werden. import matplotlib.pyplot as plt Angesprochen werden kann die Bibliothek hier \u00fcber den Namen plt . Enthalten ist z.B. eine Methode zum Darstellen von Punktemengen als X und Y werte ( scatter ). \u00dcber die Methode show() wird dann die Grafik angezeigt. datax = [ 4 , 6 , 7 , 9 ] datay = [ 2 , 4 , 7 , 3 ] plt . scatter ( datax , datay ) plt . show () \\newpage Analyse des Datensatzes Stellt man die zur Verf\u00fcgung gestellten Datensatz grafisch dar, so erh\u00e4lt man z.B. folgende Darstellung. Entwerfen Sie im Klassenverband erste Idee wie man ein Modell entwickeln k\u00f6nnte, welches Aussagen entwickelt wann es sich lohnt den Weizen zu ernten! Betrachtet man die Daten ein wenig genauer, so f\u00e4llt auf, dass ab einer Bodenfeuchte von >= 16 auf jeden Fall gewartet werden soll. Doch wie kann ein Algorithmus auf diesen Wert kommen? Die L\u00f6sung hierf\u00fcr bietet und der Wert der Entropie einer Datenmenge. Die Entropie einer Datenmenge ist ein Ma\u00df daf\u00fcr, wie viel \"Unordnung\" oder \"Unsicherheit\" in den Daten vorhanden ist. Eine h\u00f6here Entropie bedeutet, dass die Daten weniger geordnet und daher schwieriger vorherzusagen sind. Die Entropie wird normalerweise in Bits gemessen. Eine Entropie von 0 tritt nur auf, wenn alle Elemente in der Datenmenge identisch sind. In diesem Fall gibt es keine Unsicherheit, da das Auftreten jedes Elements vorhersehbar ist. Ein Beispiel w\u00e4re eine Liste von Einsen: Jedes Element ist eine 1, und daher gibt es keine Unsicherheit oder Entropie in den Daten. Wir Menschen sind also auf den Wert Feuchte >= 16 gekommen, indem wir eine Grenze gesucht haben, um eine Teilmenge zu erzeugen, die einen Entropie-Wert von 0 hat. Die Formel zur Berechnung der Wahrscheinlichkeit eines Ereignisses ist: \\[P(x_i) = \\frac{n_i}{N}\\] wobei \\(n_i\\) die Anzahl der Beispiele ist, in denen das Element \\(x_i\\) auftritt und \\(N\\) die Gesamtzahl der Beispiele im Array ist. Die Shannon-Entropie-Formel zur Berechnung der Entropie lautet wie folgt: \\[H(X) = -\\sum_{i=1}^n P(x_i) \\log_2(P(x_i))\\] wobei \\(P(x_i)\\) die Wahrscheinlichkeit des Ereignisses \\(x_i\\) ist und \\(\\log_2\\) der Logarithmus zur Basis 2 ist. Aufgabe 2 Gegeben ist folgender Datensatz: Kategorie A A B A A B Bestimmen Sie die Entropie des Datensatzes! L\u00f6sung \\[P_A = \\frac{4}{6}=0.6667\\] \\[P_B = \\frac{2}{6}=0.3333\\] \\[H = - (P_A \\log_2(P_A)+P_B \\log_2(P_B))\\] \\[H = - (0.6667+log_2(0.6667)+0.3333*log_2(0.3333))=0.918262\\] \\newpage Aufgabe 3 Wir k\u00f6nnen unseren Datens\u00e4tze nun in zwei Teildatens\u00e4tze einteilen m1 und m2 . import pandas as pd import matplotlib.pyplot as plt # CSV-Datei laden data = pd . read_csv ( 'ErnteBauern.csv' ) m1 = data [ data [ 'Feuchte' ] < 13 ] m2 = data [ data [ 'Feuchte' ] >= 13 ] Entwickeln Sie eine Pythonfunktion calcEntropie(mx):Entropie welche die Entropie eines \u00fcbergebenen Arrays mit den Kategorien Ernten und Warten ermittelt. Wenn wir mir unsere entwickelten Funktion calcEntropie(mx):Entropie die Datenmenge nun schrittweise teilen, erhalten wir die folgende Darstellung. Wie wir sehen erhalten wir bei einen Wert von \\(Feuchte = 16\\) eine Entropie in der Menge e2 von 0. Nun m\u00fcssen wir noch \u00fcberpr\u00fcfen ob es m\u00f6glich ist eine Menge im Effekt der Regenwahrscheinlichkeit zu finden, der gegen 0 geht. Wie wir sehen, ist der geringste Entropiewert der Wert von 0, wenn wir bei einer \\(Feuchte >= 16\\) die Mengen trennen. Durch diese Entscheidung erhalten wir den gr\u00f6\u00dften Informationsgewinn erreicht, dieser wird auch als gain bezeichnet. Wir haben also unsere erste Entscheidung f\u00fcr den Entscheidungsbaum getroffen. { width=400px } \\newpage Aufgabe 4 Entsprechend den zuvor durchgef\u00fchrten \u00dcberlegungen k\u00f6nnen wir die verbleibende Menge weiter einteilen: F\u00fchren Sie die weiteren \u00dcberlegungen mit der verbleibenden Menge ( \\(Feuchte < 16\\) ) durch und finden Sie die n\u00e4chsten zwei Knoten unseres Entscheidungsbaumes, indem Sie die Funktion calcEntropie(m):Entropie verwenden. Erg\u00e4nzen Sie mit den Ergebnissen den unten abgebildeten Entscheidungsbaum! { width=500px } Diskutieren Sie wie Sie weiter vorgehen sollten ! \\newpage Es bietet sich nicht an den letzten Datenpunkt von Warten bei Feuchte=13 / Regenwahrscheinlichkeit=45 mit in den Entscheidungsbaum aufzunehmen. Bei dem Datenpunkt scheint es sich im einen Ausrei\u00dfer zu handeln. W\u00fcrde der Datenpunkt mit in den Entscheidungsbaum aufgenommen, so w\u00fcrde dieses den Entscheidungsbaum unn\u00f6tig verkomplizieren. Man spricht in diesem Fall auch von der \u00dcberanpassung eines Modells. Die \u00dcberanpassung (auch bekannt als Overfitting) bezeichnet ein Ph\u00e4nomen, bei dem ein Modell so eng auf die Trainingsdaten passt, dass es nur in der Lage ist, diese spezifischen Daten gut vorherzusagen und nicht in der Lage ist, neue Daten oder eine Testmenge gut zu verallgemeinern. Dies kann insbesondere bei komplexeren Entscheidungsbaum-Modellen auftreten, bei denen das Modell so viele Schichten hat, dass es fast jede m\u00f6gliche Kombination von Variablen innerhalb der Trainingsdaten abdeckt. Dies f\u00fchrt dazu, dass das Modell ein \"Ged\u00e4chtnis\" f\u00fcr die Trainingsdaten entwickelt und sich an jede einzelne Beobachtung erinnert, anstatt allgemeine Muster zu lernen, die f\u00fcr neue Datens\u00e4tze gelten k\u00f6nnen. Ein overfitted Entscheidungsbaum kann sich negativ auf die Leistung des Modells auswirken, indem es zu schlechten Vorhersagen f\u00fcr neue Daten f\u00fchrt. Um die \u00dcberanpassung von Entscheidungsb\u00e4umen zu reduzieren, k\u00f6nnen verschiedene Methoden angewandt werden, wie z.B. Regelungsverfahren wie Pruning, Setzen von Maximaltiefe f\u00fcr den Baum, sowie Verwendung von Klassifikationsmodellen mit weniger Features. Aufgabe 5 Mittels des Pythonpaketes sklearn lassen sich Entscheidungsb\u00e4ume erstellen. Der folgende Codeauszug nutzt dieses Paket zum erstellen eines Entscheidungsbaumes: from sklearn import tree y = data [ 'Ergebnis' ] x = data . drop ([ 'Ergebnis' ], axis = 1 ) clf = tree . DecisionTreeClassifier ( criterion = 'entropy' ) clf = clf . fit ( x , y ) Untersuchen Sie mit Hilfe der Methode predict den entwickelten Entscheidungsbaum! Erzeugen Sie daf\u00fcr 4 typische Werte f\u00fcr den Datensatz! \\newpage Aufgabe 6 Python erm\u00f6glicht es Ihnen auch den Entscheidungsbaum zu visualisieren. Dazu dient das Paket graphviz , welches nat\u00fcrlich zuvor installiert werden muss. Der folgende Code visualisiert den entstandenen Entscheidungsbaum im JupyterNotebook. from sklearn.tree import export_graphviz from IPython.display import display import graphviz # exportiere Baum in DOT-Format dot_data = export_graphviz ( clf , out_file = None , feature_names = x . columns . values . tolist (), class_names = [ 'Ernten' , 'Warten' ], filled = True , rounded = True , special_characters = True ) # konvertiere DOT-Format zu einem Graph-Objekt graph = graphviz . Source ( dot_data ) display ( graph ) Welche Aussage lassen sich aus der Grafik ableiten? \\newpage Aufgabe 7 Zur Validierung unseres Modells stellt uns der Kunde einen weiteren Datensatz zur Verf\u00fcgung ( ErnteBauern2 ). \u00dcberpr\u00fcfen Sie mit Hilfe dieses Datensatzes die Qualit\u00e4t des Entscheidungsbaumes: Hinweis : Nutzen Sie hierzu das Modul accuracy_score aus dem Paket sklearn.metrics ! import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # Laden Sie den Testdatensatz test_data = pd . read_csv ( 'Data/ErnteBauern2.csv' ) # Trennen Sie die Zielvariable vom Rest der Daten X_test = test_data . drop ( 'Ergebnis' , axis = 1 ) y_test = test_data [ 'Ergebnis' ] # F\u00fchren Sie Vorhersagen auf dem Testdatensatz durch # (wobei clf unser trainierter Entscheidungsbaum aus Aufgabe 5 ist) y_pred = clf . predict ( X_test ) # Berechnen Sie die Vorhersagegenauigkeit auf dem Testdatensatz accuracy = accuracy_score ( y_test , y_pred ) print ( \"Vorhersagegenauigkeit auf dem Testdatensatz: {:.2f} %\" . format ( accuracy * 100 )) \\newpage Aufgabe 8 W\u00e4hlen Sie einen geeigneten Datensatz aus und entwerfen Sie eine Entscheidungsbaum Modell und pr\u00e4sentieren Sie im Anschluss daran der Klasse ihren Datensatz, ihr Vorgehen und das entstandene Modell. Folgende Datens\u00e4tze k\u00f6nnen Sie z.B. verwenden! Titanic-Datensatz : Der Titanic-Datensatz enth\u00e4lt Informationen \u00fcber Passagiere an Bord des Schiffes Titanic, einschlie\u00dflich Merkmalen wie Alter, Geschlecht, Klasse und \u00dcberlebensstatus. Dieser Datensatz ist gut geeignet f\u00fcr bin\u00e4re Klassifizierungsaufgaben und kann auch zur Vorhersage des \u00dcberlebens von Passagieren auf anderen Schiffsreisen verwendet werden. https://www.kaggle.com/c/titanic/data Bank Marketing-Datensatz : Dieser Datensatz enth\u00e4lt Informationen zu Kunden einer portugiesischen Bank und ob sie Ja oder Nein f\u00fcr ein Termingeld-Abonnement abgeschlossen haben. Es enth\u00e4lt eine Vielzahl von Kundenmerkmalen wie Alter, Beruf, Familienstand usw., die verwendet werden k\u00f6nnen, um vorherzusagen, ob ein Kunde ein Abonnement abschlie\u00dfen wird oder nicht. https://archive.ics.uci.edu/ml/datasets/Bank+Marketing Breast Cancer Wisconsin (diagnostic) Dataset : Dieser Datensatz enth\u00e4lt Details zu den Zellkernmerkmalen von malignen und benignen Brustgewebeproben sowie einer Diagnose, ob eine Probe maligne oder benign ist. Der Datensatz ist gut geeignet f\u00fcr bin\u00e4re Klassifizierungsaufgaben zum Erkennen von Brustkrebs. https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic) Die Pr\u00e4sentation sollte folgendes beinhalten: Vorstellen (Exploration) des Datensatzes mit geeigneter Visualisierung der Daten. Darstellung des Entscheidungsbaumes Validierung des Entscheidungsbaumes (Aussagen zur Genauigkeit des Entscheidungsbaumes) \\newpage Fragen zum Verst\u00e4ndnis Was ist der Hauptzweck von Entscheidungsb\u00e4umen? [ ] Datenspeicherung [ ] Berechnung von Durchschnittswerten [ ] Klassifizierung und Vorhersage [ ] Datenvisualisierung Welches Ma\u00df wird verwendet, um die \"Unordnung\" oder \"Unsicherheit\" in einer Datenmenge zu beschreiben? [ ] Kovarianz [ ] Entropie [ ] Varianz [ ] Korrelation Wie wird die Entropie einer Datenmenge berechnet? [ ] \\(P(x_i) = \\frac{n_i}{N}\\) [ ] \\(H(X) = \\sum_{i=1}^n P(x_i) \\log_2(P(x_i))\\) [ ] \\(H(X) = -\\sum_{i=1}^n P(x_i) \\log_2(P(x_i))\\) [ ] \\(H(X) = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2\\) Welches Python-Paket wird verwendet, um Entscheidungsb\u00e4ume zu erstellen? [ ] numpy [ ] matplotlib [ ] pandas [ ] sklearn Wie kann man die Vorhersagegenauigkeit eines Entscheidungsbaums auf einem Testdatensatz berechnen? [ ] Indem man die Entropie der Vorhersagen berechnet [ ] Mit der Methode accuracy_score aus dem Paket sklearn.metrics [ ] Durch Vergleich der mittleren quadratischen Fehler [ ] Durch Berechnung der Korrelation zwischen den tats\u00e4chlichen und den vorhergesagten Werten Welches der folgenden Aussagen beschreibt die \u00dcberanpassung (Overfitting) von Entscheidungsb\u00e4umen am besten? [ ] Ein \u00fcberangepasster Entscheidungsbaum hat eine zu geringe Anzahl an Bl\u00e4ttern. [ ] Die Genauigkeit eines \u00fcberangepassten Entscheidungsbaums sinkt, wenn neue Daten hinzugef\u00fcgt werden. [ ] Ein \u00fcberangepasster Entscheidungsbaum passt sich zu stark an die Trainingsdaten an und kann dadurch schlechtere Vorhersagen auf neuen Daten liefern. [ ] Ein \u00fcberangepasster Entscheidungsbaum hat eine hohe Generalisierungsf\u00e4higkeit und ist daher besser auf neuen Daten anwendbar.","title":"Entscheidungsb\u00e4ume"},{"location":"Entscheidungsbaume/#entscheidungsbaume","text":"","title":"Entscheidungsb\u00e4ume"},{"location":"Entscheidungsbaume/#handlungssituation","text":"Ein gro\u00dfer Landmaschinenhersteller w\u00fcnscht sich eine gr\u00f6\u00dfere Kundenbindung und beauftragt die ChangeIT GmbH mit der Entwicklung einer App, die Landwirten Empfehlungen gibt, wann der Weizen zu ernten ist. Die App misst dazu \u00fcber einen via Bluetooth gekoppelten Feuchte-Sensor die Bodenfeuchte im Feld und kann \u00fcber eine API Abfrage die Regenwahrscheinlichkeit bestimmen. Erste Erfahrungswerte liegen bereits vor und werden vom Landmaschinenhersteller der ChangeIT in Form einer CSV Datei zur Verf\u00fcgung gestellt. Als Mitglied der Abteilung Daten- und Prozessanalyse erhalten Sie die Aufgabe ein geeignetes Vorhersagemodell zu entwickeln.[^1] [^1]: Vgl. Brandt, Y., Eickhoff-Schachtebeck, A. und Strecker, K. (2022): \u201eSchulbuch starkeSeiten Informatik Jahrgang 9/10 Differenzierende Ausgabe Niedersachsen\u201c, Klett-Verlag 2022","title":"Handlungssituation"},{"location":"Entscheidungsbaume/#die-zur-verfugung-gestellten-daten","text":"Die Daten die uns der Auftraggeber zur Verf\u00fcgung stellt liegen in Form einer csv Datei vor. \"Feuchte\",\"Regenwahrscheinlichkeit\",\"Ergebnis\" \"17.94\",\"56.89\",\"Warten\" \"15.01\",\"80.88\",\"Ernten\" \"15.56\",\"75.99\",\"Warten\" \"18.25\",\"36.97\",\"Warten\" \"16.52\",\"90.11\",\"Warten\" \"15.26\",\"27.82\",\"Warten\" Die Spalten haben dabei folgende Bedeutung: Feuchte: Der Wert der Bodenfeuchte Regenwahrscheinlichkeit: Die Regenwahrscheinlichkeit in % Ergebnis: Die bisherigen Erfahrungswerte, wann es sich lohnt den Weizen zu Ernten oder besser noch zu Warten .","title":"Die zur Verf\u00fcgung gestellten Daten"},{"location":"Entscheidungsbaume/#aufgabe-1-visualisierung-der-daten","text":"Zun\u00e4chst sollen die Daten in einem Diagramm visualisiert werden, um sich eine Vorstellung von den Daten zu machen. Schreiben Sie ein erste Python-Programm, welches m\u00f6glichst anschaulich den zur Verf\u00fcgung gestellten Datensatz ( ErnteBauern.csv ) visualisiert. Hinweise: Einlesen kann man eine CSV Datei in Python auf unterschiedlichste Weise. Zur sp\u00e4teren Datenanalyse ist jedoch die Bibliothek pandas das geeignete Mittel. Hier existiert bereits eine Methode read_csv die dieses erledigt. import pandas as pd # CSV-Datei laden data = pd . read_csv ( 'Datei.csv' ) Zum Darstellen von Datenmenge nutzen wir die Bibliothek matplotlib.pyplot . Diese muss zun\u00e4chst importiert werden. import matplotlib.pyplot as plt Angesprochen werden kann die Bibliothek hier \u00fcber den Namen plt . Enthalten ist z.B. eine Methode zum Darstellen von Punktemengen als X und Y werte ( scatter ). \u00dcber die Methode show() wird dann die Grafik angezeigt. datax = [ 4 , 6 , 7 , 9 ] datay = [ 2 , 4 , 7 , 3 ] plt . scatter ( datax , datay ) plt . show () \\newpage","title":"Aufgabe 1: Visualisierung der Daten"},{"location":"Entscheidungsbaume/#analyse-des-datensatzes","text":"Stellt man die zur Verf\u00fcgung gestellten Datensatz grafisch dar, so erh\u00e4lt man z.B. folgende Darstellung. Entwerfen Sie im Klassenverband erste Idee wie man ein Modell entwickeln k\u00f6nnte, welches Aussagen entwickelt wann es sich lohnt den Weizen zu ernten! Betrachtet man die Daten ein wenig genauer, so f\u00e4llt auf, dass ab einer Bodenfeuchte von >= 16 auf jeden Fall gewartet werden soll. Doch wie kann ein Algorithmus auf diesen Wert kommen? Die L\u00f6sung hierf\u00fcr bietet und der Wert der Entropie einer Datenmenge. Die Entropie einer Datenmenge ist ein Ma\u00df daf\u00fcr, wie viel \"Unordnung\" oder \"Unsicherheit\" in den Daten vorhanden ist. Eine h\u00f6here Entropie bedeutet, dass die Daten weniger geordnet und daher schwieriger vorherzusagen sind. Die Entropie wird normalerweise in Bits gemessen. Eine Entropie von 0 tritt nur auf, wenn alle Elemente in der Datenmenge identisch sind. In diesem Fall gibt es keine Unsicherheit, da das Auftreten jedes Elements vorhersehbar ist. Ein Beispiel w\u00e4re eine Liste von Einsen: Jedes Element ist eine 1, und daher gibt es keine Unsicherheit oder Entropie in den Daten. Wir Menschen sind also auf den Wert Feuchte >= 16 gekommen, indem wir eine Grenze gesucht haben, um eine Teilmenge zu erzeugen, die einen Entropie-Wert von 0 hat. Die Formel zur Berechnung der Wahrscheinlichkeit eines Ereignisses ist: \\[P(x_i) = \\frac{n_i}{N}\\] wobei \\(n_i\\) die Anzahl der Beispiele ist, in denen das Element \\(x_i\\) auftritt und \\(N\\) die Gesamtzahl der Beispiele im Array ist. Die Shannon-Entropie-Formel zur Berechnung der Entropie lautet wie folgt: \\[H(X) = -\\sum_{i=1}^n P(x_i) \\log_2(P(x_i))\\] wobei \\(P(x_i)\\) die Wahrscheinlichkeit des Ereignisses \\(x_i\\) ist und \\(\\log_2\\) der Logarithmus zur Basis 2 ist.","title":"Analyse des Datensatzes"},{"location":"Entscheidungsbaume/#aufgabe-2","text":"Gegeben ist folgender Datensatz: Kategorie A A B A A B Bestimmen Sie die Entropie des Datensatzes!","title":"Aufgabe 2"},{"location":"Entscheidungsbaume/#losung","text":"\\[P_A = \\frac{4}{6}=0.6667\\] \\[P_B = \\frac{2}{6}=0.3333\\] \\[H = - (P_A \\log_2(P_A)+P_B \\log_2(P_B))\\] \\[H = - (0.6667+log_2(0.6667)+0.3333*log_2(0.3333))=0.918262\\] \\newpage","title":"L\u00f6sung"},{"location":"Entscheidungsbaume/#aufgabe-3","text":"Wir k\u00f6nnen unseren Datens\u00e4tze nun in zwei Teildatens\u00e4tze einteilen m1 und m2 . import pandas as pd import matplotlib.pyplot as plt # CSV-Datei laden data = pd . read_csv ( 'ErnteBauern.csv' ) m1 = data [ data [ 'Feuchte' ] < 13 ] m2 = data [ data [ 'Feuchte' ] >= 13 ] Entwickeln Sie eine Pythonfunktion calcEntropie(mx):Entropie welche die Entropie eines \u00fcbergebenen Arrays mit den Kategorien Ernten und Warten ermittelt. Wenn wir mir unsere entwickelten Funktion calcEntropie(mx):Entropie die Datenmenge nun schrittweise teilen, erhalten wir die folgende Darstellung. Wie wir sehen erhalten wir bei einen Wert von \\(Feuchte = 16\\) eine Entropie in der Menge e2 von 0. Nun m\u00fcssen wir noch \u00fcberpr\u00fcfen ob es m\u00f6glich ist eine Menge im Effekt der Regenwahrscheinlichkeit zu finden, der gegen 0 geht. Wie wir sehen, ist der geringste Entropiewert der Wert von 0, wenn wir bei einer \\(Feuchte >= 16\\) die Mengen trennen. Durch diese Entscheidung erhalten wir den gr\u00f6\u00dften Informationsgewinn erreicht, dieser wird auch als gain bezeichnet. Wir haben also unsere erste Entscheidung f\u00fcr den Entscheidungsbaum getroffen. { width=400px } \\newpage","title":"Aufgabe 3"},{"location":"Entscheidungsbaume/#aufgabe-4","text":"Entsprechend den zuvor durchgef\u00fchrten \u00dcberlegungen k\u00f6nnen wir die verbleibende Menge weiter einteilen: F\u00fchren Sie die weiteren \u00dcberlegungen mit der verbleibenden Menge ( \\(Feuchte < 16\\) ) durch und finden Sie die n\u00e4chsten zwei Knoten unseres Entscheidungsbaumes, indem Sie die Funktion calcEntropie(m):Entropie verwenden. Erg\u00e4nzen Sie mit den Ergebnissen den unten abgebildeten Entscheidungsbaum! { width=500px } Diskutieren Sie wie Sie weiter vorgehen sollten ! \\newpage Es bietet sich nicht an den letzten Datenpunkt von Warten bei Feuchte=13 / Regenwahrscheinlichkeit=45 mit in den Entscheidungsbaum aufzunehmen. Bei dem Datenpunkt scheint es sich im einen Ausrei\u00dfer zu handeln. W\u00fcrde der Datenpunkt mit in den Entscheidungsbaum aufgenommen, so w\u00fcrde dieses den Entscheidungsbaum unn\u00f6tig verkomplizieren. Man spricht in diesem Fall auch von der \u00dcberanpassung eines Modells. Die \u00dcberanpassung (auch bekannt als Overfitting) bezeichnet ein Ph\u00e4nomen, bei dem ein Modell so eng auf die Trainingsdaten passt, dass es nur in der Lage ist, diese spezifischen Daten gut vorherzusagen und nicht in der Lage ist, neue Daten oder eine Testmenge gut zu verallgemeinern. Dies kann insbesondere bei komplexeren Entscheidungsbaum-Modellen auftreten, bei denen das Modell so viele Schichten hat, dass es fast jede m\u00f6gliche Kombination von Variablen innerhalb der Trainingsdaten abdeckt. Dies f\u00fchrt dazu, dass das Modell ein \"Ged\u00e4chtnis\" f\u00fcr die Trainingsdaten entwickelt und sich an jede einzelne Beobachtung erinnert, anstatt allgemeine Muster zu lernen, die f\u00fcr neue Datens\u00e4tze gelten k\u00f6nnen. Ein overfitted Entscheidungsbaum kann sich negativ auf die Leistung des Modells auswirken, indem es zu schlechten Vorhersagen f\u00fcr neue Daten f\u00fchrt. Um die \u00dcberanpassung von Entscheidungsb\u00e4umen zu reduzieren, k\u00f6nnen verschiedene Methoden angewandt werden, wie z.B. Regelungsverfahren wie Pruning, Setzen von Maximaltiefe f\u00fcr den Baum, sowie Verwendung von Klassifikationsmodellen mit weniger Features.","title":"Aufgabe 4"},{"location":"Entscheidungsbaume/#aufgabe-5","text":"Mittels des Pythonpaketes sklearn lassen sich Entscheidungsb\u00e4ume erstellen. Der folgende Codeauszug nutzt dieses Paket zum erstellen eines Entscheidungsbaumes: from sklearn import tree y = data [ 'Ergebnis' ] x = data . drop ([ 'Ergebnis' ], axis = 1 ) clf = tree . DecisionTreeClassifier ( criterion = 'entropy' ) clf = clf . fit ( x , y ) Untersuchen Sie mit Hilfe der Methode predict den entwickelten Entscheidungsbaum! Erzeugen Sie daf\u00fcr 4 typische Werte f\u00fcr den Datensatz! \\newpage","title":"Aufgabe 5"},{"location":"Entscheidungsbaume/#aufgabe-6","text":"Python erm\u00f6glicht es Ihnen auch den Entscheidungsbaum zu visualisieren. Dazu dient das Paket graphviz , welches nat\u00fcrlich zuvor installiert werden muss. Der folgende Code visualisiert den entstandenen Entscheidungsbaum im JupyterNotebook. from sklearn.tree import export_graphviz from IPython.display import display import graphviz # exportiere Baum in DOT-Format dot_data = export_graphviz ( clf , out_file = None , feature_names = x . columns . values . tolist (), class_names = [ 'Ernten' , 'Warten' ], filled = True , rounded = True , special_characters = True ) # konvertiere DOT-Format zu einem Graph-Objekt graph = graphviz . Source ( dot_data ) display ( graph ) Welche Aussage lassen sich aus der Grafik ableiten? \\newpage","title":"Aufgabe 6"},{"location":"Entscheidungsbaume/#aufgabe-7","text":"Zur Validierung unseres Modells stellt uns der Kunde einen weiteren Datensatz zur Verf\u00fcgung ( ErnteBauern2 ). \u00dcberpr\u00fcfen Sie mit Hilfe dieses Datensatzes die Qualit\u00e4t des Entscheidungsbaumes: Hinweis : Nutzen Sie hierzu das Modul accuracy_score aus dem Paket sklearn.metrics ! import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # Laden Sie den Testdatensatz test_data = pd . read_csv ( 'Data/ErnteBauern2.csv' ) # Trennen Sie die Zielvariable vom Rest der Daten X_test = test_data . drop ( 'Ergebnis' , axis = 1 ) y_test = test_data [ 'Ergebnis' ] # F\u00fchren Sie Vorhersagen auf dem Testdatensatz durch # (wobei clf unser trainierter Entscheidungsbaum aus Aufgabe 5 ist) y_pred = clf . predict ( X_test ) # Berechnen Sie die Vorhersagegenauigkeit auf dem Testdatensatz accuracy = accuracy_score ( y_test , y_pred ) print ( \"Vorhersagegenauigkeit auf dem Testdatensatz: {:.2f} %\" . format ( accuracy * 100 )) \\newpage","title":"Aufgabe 7"},{"location":"Entscheidungsbaume/#aufgabe-8","text":"W\u00e4hlen Sie einen geeigneten Datensatz aus und entwerfen Sie eine Entscheidungsbaum Modell und pr\u00e4sentieren Sie im Anschluss daran der Klasse ihren Datensatz, ihr Vorgehen und das entstandene Modell. Folgende Datens\u00e4tze k\u00f6nnen Sie z.B. verwenden! Titanic-Datensatz : Der Titanic-Datensatz enth\u00e4lt Informationen \u00fcber Passagiere an Bord des Schiffes Titanic, einschlie\u00dflich Merkmalen wie Alter, Geschlecht, Klasse und \u00dcberlebensstatus. Dieser Datensatz ist gut geeignet f\u00fcr bin\u00e4re Klassifizierungsaufgaben und kann auch zur Vorhersage des \u00dcberlebens von Passagieren auf anderen Schiffsreisen verwendet werden. https://www.kaggle.com/c/titanic/data Bank Marketing-Datensatz : Dieser Datensatz enth\u00e4lt Informationen zu Kunden einer portugiesischen Bank und ob sie Ja oder Nein f\u00fcr ein Termingeld-Abonnement abgeschlossen haben. Es enth\u00e4lt eine Vielzahl von Kundenmerkmalen wie Alter, Beruf, Familienstand usw., die verwendet werden k\u00f6nnen, um vorherzusagen, ob ein Kunde ein Abonnement abschlie\u00dfen wird oder nicht. https://archive.ics.uci.edu/ml/datasets/Bank+Marketing Breast Cancer Wisconsin (diagnostic) Dataset : Dieser Datensatz enth\u00e4lt Details zu den Zellkernmerkmalen von malignen und benignen Brustgewebeproben sowie einer Diagnose, ob eine Probe maligne oder benign ist. Der Datensatz ist gut geeignet f\u00fcr bin\u00e4re Klassifizierungsaufgaben zum Erkennen von Brustkrebs. https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)","title":"Aufgabe 8"},{"location":"Entscheidungsbaume/#die-prasentation-sollte-folgendes-beinhalten","text":"Vorstellen (Exploration) des Datensatzes mit geeigneter Visualisierung der Daten. Darstellung des Entscheidungsbaumes Validierung des Entscheidungsbaumes (Aussagen zur Genauigkeit des Entscheidungsbaumes) \\newpage","title":"Die Pr\u00e4sentation sollte folgendes beinhalten:"},{"location":"Entscheidungsbaume/#fragen-zum-verstandnis","text":"Was ist der Hauptzweck von Entscheidungsb\u00e4umen? [ ] Datenspeicherung [ ] Berechnung von Durchschnittswerten [ ] Klassifizierung und Vorhersage [ ] Datenvisualisierung Welches Ma\u00df wird verwendet, um die \"Unordnung\" oder \"Unsicherheit\" in einer Datenmenge zu beschreiben? [ ] Kovarianz [ ] Entropie [ ] Varianz [ ] Korrelation Wie wird die Entropie einer Datenmenge berechnet? [ ] \\(P(x_i) = \\frac{n_i}{N}\\) [ ] \\(H(X) = \\sum_{i=1}^n P(x_i) \\log_2(P(x_i))\\) [ ] \\(H(X) = -\\sum_{i=1}^n P(x_i) \\log_2(P(x_i))\\) [ ] \\(H(X) = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2\\) Welches Python-Paket wird verwendet, um Entscheidungsb\u00e4ume zu erstellen? [ ] numpy [ ] matplotlib [ ] pandas [ ] sklearn Wie kann man die Vorhersagegenauigkeit eines Entscheidungsbaums auf einem Testdatensatz berechnen? [ ] Indem man die Entropie der Vorhersagen berechnet [ ] Mit der Methode accuracy_score aus dem Paket sklearn.metrics [ ] Durch Vergleich der mittleren quadratischen Fehler [ ] Durch Berechnung der Korrelation zwischen den tats\u00e4chlichen und den vorhergesagten Werten Welches der folgenden Aussagen beschreibt die \u00dcberanpassung (Overfitting) von Entscheidungsb\u00e4umen am besten? [ ] Ein \u00fcberangepasster Entscheidungsbaum hat eine zu geringe Anzahl an Bl\u00e4ttern. [ ] Die Genauigkeit eines \u00fcberangepassten Entscheidungsbaums sinkt, wenn neue Daten hinzugef\u00fcgt werden. [ ] Ein \u00fcberangepasster Entscheidungsbaum passt sich zu stark an die Trainingsdaten an und kann dadurch schlechtere Vorhersagen auf neuen Daten liefern. [ ] Ein \u00fcberangepasster Entscheidungsbaum hat eine hohe Generalisierungsf\u00e4higkeit und ist daher besser auf neuen Daten anwendbar.","title":"Fragen zum Verst\u00e4ndnis"},{"location":"Info_fuer_Lehrende/","text":"Informationen f\u00fcr Lehrende Willkommen als Lehrender zu meinem LF10c Moodle Kurs. Nahezu alle Inhalte dieses Kurses werden aus einem github Repository nachgeladen ( https://github.com/jtuttas/datenanalyse ). Daher kann es sein, dass sich einige Inhalte im laufe der Zeit \u00e4ndern oder angepasst werden. In diesem Repository finden Sie ebenso die Beschreibung der Lernsituationen als MK-Docs gerenderte HTML Seiten. Strukturelle \u00c4nderungen am Kurs sind nur im Moodle-System m\u00f6glich. Sollte es gr\u00f6\u00dfere \u00c4nderungen an dem Kurs geben so werden Sie an dieser Stelle im Changelog (siehe unten) aufgef\u00fchrt. Kompetenzformulierung gem\u00e4\u00df Rahmenlehrplan \" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Lernsituationen als PDF Dieser Kurs umfasst das LF10c (80 Std) der Ausbildung zum Fachinformatiker f\u00fcr Daten- und Prozessanalyse. Folgende Lernsituationen sind enthalten: LS1 KI im Alltag LS2 Datenexploration und -Visualisierung LS3 Entscheidungsb\u00e4ume LS4 K-mean Clustering LS5 Regressionsanalyse LS6 K-nearest Neighbor (KNN) LS7 Neuronale Netze LS8 Cloud KI Systeme nutzen LS9 Verst\u00e4rkendes Lernen Change log V1.0: Erster Release im Schuljahr 23/24","title":"Informationen f\u00fcr Lehrende"},{"location":"Info_fuer_Lehrende/#informationen-fur-lehrende","text":"Willkommen als Lehrender zu meinem LF10c Moodle Kurs. Nahezu alle Inhalte dieses Kurses werden aus einem github Repository nachgeladen ( https://github.com/jtuttas/datenanalyse ). Daher kann es sein, dass sich einige Inhalte im laufe der Zeit \u00e4ndern oder angepasst werden. In diesem Repository finden Sie ebenso die Beschreibung der Lernsituationen als MK-Docs gerenderte HTML Seiten. Strukturelle \u00c4nderungen am Kurs sind nur im Moodle-System m\u00f6glich. Sollte es gr\u00f6\u00dfere \u00c4nderungen an dem Kurs geben so werden Sie an dieser Stelle im Changelog (siehe unten) aufgef\u00fchrt.","title":"Informationen f\u00fcr Lehrende"},{"location":"Info_fuer_Lehrende/#kompetenzformulierung-gema-rahmenlehrplan","text":"\" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte.","title":"Kompetenzformulierung gem\u00e4\u00df Rahmenlehrplan"},{"location":"Info_fuer_Lehrende/#lernsituationen-als-pdf","text":"Dieser Kurs umfasst das LF10c (80 Std) der Ausbildung zum Fachinformatiker f\u00fcr Daten- und Prozessanalyse. Folgende Lernsituationen sind enthalten: LS1 KI im Alltag LS2 Datenexploration und -Visualisierung LS3 Entscheidungsb\u00e4ume LS4 K-mean Clustering LS5 Regressionsanalyse LS6 K-nearest Neighbor (KNN) LS7 Neuronale Netze LS8 Cloud KI Systeme nutzen LS9 Verst\u00e4rkendes Lernen","title":"Lernsituationen als PDF"},{"location":"Info_fuer_Lehrende/#change-log","text":"V1.0: Erster Release im Schuljahr 23/24","title":"Change log"},{"location":"KI_im_Alltag/","text":"KI im Alltag Handlungssituation Die Change IT GmbH pr\u00fcft ob es sinnvoll w\u00e4re eine eigne Abteilung zur Daten- und Prozessanalyse mit dem Schwerpunkt maschinelles Lernen und Deep Learning zu gr\u00fcnden. Sie erhalten den Auftrag die Begriffe KI, Maschine Learning, Deep Learning einzuordnen, Anwendungsbeispiele f\u00fcr diese Technologien zu finden und ggf. moralische Grenzen aufzuzeigen. t\u00e4glicher Einsatz von KI Systemen K\u00fcnstliche Intelligenz begegnet uns inzwischen in vielf\u00e4ltigen Formen im Alltag. Sammeln Sie in einer gemeinsamen Brainstorming Phase Situationen / Anl\u00e4sse, bei denen Sie mit k\u00fcnstlicher Intelligenz zu tun haben und versuchen Sie diese Situation mit nur einem Wort zu beschreiben. Einsatzm\u00f6glichkeiten und moralische Grenzen von k\u00fcnstlicher Intelligenz Lesen Sie den Artikel \"L\u00e4uft wie KI-geschmiert\" von Andrea Trinkwalder aus der ct 17/2022; S20 durch und beantworten Sie die folgenden Fragen. Was ist der Unterschied zwischen k\u00fcnstlicher Intelligenz und Machine Learning? a) K\u00fcnstliche Intelligenz ist ein Teilbereich von Machine Learning. b) Machine Learning ist ein Teilbereich von k\u00fcnstlicher Intelligenz. c) Es gibt keinen Unterschied zwischen den beiden Begriffen. Wie k\u00f6nnen Unternehmen sicherstellen, dass ihre Mitarbeiter die F\u00e4higkeiten haben, um mit k\u00fcnstlicher Intelligenz zu arbeiten? a) Durch die Einstellung von neuen Mitarbeitern mit KI-Kenntnissen. b) Durch die Schulung und Weiterbildung bestehender Mitarbeiter. c) Durch die Auslagerung von KI-Aufgaben an externe Dienstleister. Was sind einige der Herausforderungen bei der Verwendung von k\u00fcnstlicher Intelligenz in der Produktion? a) Die Kosten f\u00fcr die Implementierung von KI-Systemen. b) Die Notwendigkeit, menschliche Arbeitskr\u00e4fte zu ersetzen. c) Die Schwierigkeit, komplexe Produktionsprozesse zu automatisieren. Wie kann k\u00fcnstliche Intelligenz in der Medizin eingesetzt werden? a) Zur Diagnose von Krankheiten auf Basis von Patientendaten. b) Zur Entwicklung neuer Medikamente und Therapien. c) Zur \u00dcberwachung von Patienten w\u00e4hrend medizinischer Eingriffe. Was sind einige der ethischen Bedenken im Zusammenhang mit der Verwendung von k\u00fcnstlicher Intelligenz? a) Die M\u00f6glichkeit, dass KI-Systeme menschliche Arbeitskr\u00e4fte ersetzen k\u00f6nnten. b) Die M\u00f6glichkeit, dass KI-Systeme Entscheidungen treffen, die nicht im Einklang mit menschlichen Werten und Normen stehen. c) Die M\u00f6glichkeit, dass KI-Systeme zu Abh\u00e4ngigkeit und Passivit\u00e4t f\u00fchren k\u00f6nnten. Kategorien des Machine Leanings (ML) Es gibt vielf\u00e4ltige Arten / Kategorien von Probleme die mit Hilfe von KI gel\u00f6st werden k\u00f6nnen, die h\u00e4ufigsten Kategorien sind: Klassifikation : Hierbei geht es darum, Objekte oder Daten in vordefinierte Kategorien zu klassifizieren. Regression : Hierbei geht es um die Vorhersage von numerischen Werten auf der Grundlage von Daten. Clustering : Hierbei geht es darum, \u00e4hnliche Objekte oder Daten in Gruppen oder Cluster zu gruppieren. Aufgabe: Gruppieren Sie die zuvor erarbeiteten Einsatzgebiete von K\u00fcnstlicher Intelligenz (und ggf. noch weitere) in diese Kategorien. Pr\u00e4sentieren Sie ihre Zusammenstellung im Anschluss der Klasse. Reflexion Sie erhalten folgende Email von Dr. Harald W\u00f6hler, dem Gesch\u00e4ftsf\u00fchrer der ChangeIT GmbH! Liebe Mitarbeiterinnen und Mitarbeiter der Abteilung Daten- und Prozessanalyse, ich wende mich heute an Sie mit einem neuen Projektvorschlag. Ich bin auf die Idee gekommen, eine K\u00fcnstliche Intelligenz zu entwickeln, die Bewerbungsschreiben im Vorfeld bewerten und aussortieren kann. Diese KI soll uns dabei helfen, die Bewerbungsprozesse zu optimieren und die effektive Arbeit der Personalabteilung zu unterst\u00fctzen. Ich bin davon \u00fcberzeugt, dass wir mit unserem Know-how und unserer Erfahrung in der Daten- und Prozessanalyse die ideale Abteilung sind, um ein solches Projekt erfolgreich umzusetzen. Die KI soll in der Lage sein, Bewerbungsschreiben auf verschiedene Kriterien hin zu analysieren, wie zum Beispiel die \u00dcbereinstimmung mit den Anforderungen der ausgeschriebenen Stelle oder die Pr\u00e4sentation der eigenen Erfahrungen und Qualifikationen. Ich m\u00f6chte Sie alle dazu ermutigen, an diesem Projekt mitzuarbeiten und Ihre Ideen einzubringen. Gemeinsam k\u00f6nnen wir eine KI entwickeln, die nicht nur unseren eigenen Arbeitsprozess optimiert, sondern auch anderen Unternehmen als L\u00f6sung angeboten werden kann. Ich w\u00fcrde mich sehr freuen, wenn Sie sich an diesem Projekt beteiligen und uns Ihre Ideen und Anregungen mitteilen w\u00fcrden. Wenn Sie Fragen haben oder weitere Informationen ben\u00f6tigen, stehe ich Ihnen gerne zur Verf\u00fcgung. Mit freundlichen Gr\u00fc\u00dfen, Dr. Harald W\u00f6hler Aufgabe : Verfassen Sie zu dieser Email eine Antwort.","title":"KI im Alltag"},{"location":"KI_im_Alltag/#ki-im-alltag","text":"","title":"KI im Alltag"},{"location":"KI_im_Alltag/#handlungssituation","text":"Die Change IT GmbH pr\u00fcft ob es sinnvoll w\u00e4re eine eigne Abteilung zur Daten- und Prozessanalyse mit dem Schwerpunkt maschinelles Lernen und Deep Learning zu gr\u00fcnden. Sie erhalten den Auftrag die Begriffe KI, Maschine Learning, Deep Learning einzuordnen, Anwendungsbeispiele f\u00fcr diese Technologien zu finden und ggf. moralische Grenzen aufzuzeigen.","title":"Handlungssituation"},{"location":"KI_im_Alltag/#taglicher-einsatz-von-ki-systemen","text":"K\u00fcnstliche Intelligenz begegnet uns inzwischen in vielf\u00e4ltigen Formen im Alltag. Sammeln Sie in einer gemeinsamen Brainstorming Phase Situationen / Anl\u00e4sse, bei denen Sie mit k\u00fcnstlicher Intelligenz zu tun haben und versuchen Sie diese Situation mit nur einem Wort zu beschreiben.","title":"t\u00e4glicher Einsatz von KI Systemen"},{"location":"KI_im_Alltag/#einsatzmoglichkeiten-und-moralische-grenzen-von-kunstlicher-intelligenz","text":"Lesen Sie den Artikel \"L\u00e4uft wie KI-geschmiert\" von Andrea Trinkwalder aus der ct 17/2022; S20 durch und beantworten Sie die folgenden Fragen. Was ist der Unterschied zwischen k\u00fcnstlicher Intelligenz und Machine Learning? a) K\u00fcnstliche Intelligenz ist ein Teilbereich von Machine Learning. b) Machine Learning ist ein Teilbereich von k\u00fcnstlicher Intelligenz. c) Es gibt keinen Unterschied zwischen den beiden Begriffen. Wie k\u00f6nnen Unternehmen sicherstellen, dass ihre Mitarbeiter die F\u00e4higkeiten haben, um mit k\u00fcnstlicher Intelligenz zu arbeiten? a) Durch die Einstellung von neuen Mitarbeitern mit KI-Kenntnissen. b) Durch die Schulung und Weiterbildung bestehender Mitarbeiter. c) Durch die Auslagerung von KI-Aufgaben an externe Dienstleister. Was sind einige der Herausforderungen bei der Verwendung von k\u00fcnstlicher Intelligenz in der Produktion? a) Die Kosten f\u00fcr die Implementierung von KI-Systemen. b) Die Notwendigkeit, menschliche Arbeitskr\u00e4fte zu ersetzen. c) Die Schwierigkeit, komplexe Produktionsprozesse zu automatisieren. Wie kann k\u00fcnstliche Intelligenz in der Medizin eingesetzt werden? a) Zur Diagnose von Krankheiten auf Basis von Patientendaten. b) Zur Entwicklung neuer Medikamente und Therapien. c) Zur \u00dcberwachung von Patienten w\u00e4hrend medizinischer Eingriffe. Was sind einige der ethischen Bedenken im Zusammenhang mit der Verwendung von k\u00fcnstlicher Intelligenz? a) Die M\u00f6glichkeit, dass KI-Systeme menschliche Arbeitskr\u00e4fte ersetzen k\u00f6nnten. b) Die M\u00f6glichkeit, dass KI-Systeme Entscheidungen treffen, die nicht im Einklang mit menschlichen Werten und Normen stehen. c) Die M\u00f6glichkeit, dass KI-Systeme zu Abh\u00e4ngigkeit und Passivit\u00e4t f\u00fchren k\u00f6nnten.","title":"Einsatzm\u00f6glichkeiten und moralische Grenzen von k\u00fcnstlicher Intelligenz"},{"location":"KI_im_Alltag/#kategorien-des-machine-leanings-ml","text":"Es gibt vielf\u00e4ltige Arten / Kategorien von Probleme die mit Hilfe von KI gel\u00f6st werden k\u00f6nnen, die h\u00e4ufigsten Kategorien sind: Klassifikation : Hierbei geht es darum, Objekte oder Daten in vordefinierte Kategorien zu klassifizieren. Regression : Hierbei geht es um die Vorhersage von numerischen Werten auf der Grundlage von Daten. Clustering : Hierbei geht es darum, \u00e4hnliche Objekte oder Daten in Gruppen oder Cluster zu gruppieren. Aufgabe: Gruppieren Sie die zuvor erarbeiteten Einsatzgebiete von K\u00fcnstlicher Intelligenz (und ggf. noch weitere) in diese Kategorien. Pr\u00e4sentieren Sie ihre Zusammenstellung im Anschluss der Klasse.","title":"Kategorien des Machine Leanings (ML)"},{"location":"KI_im_Alltag/#reflexion","text":"Sie erhalten folgende Email von Dr. Harald W\u00f6hler, dem Gesch\u00e4ftsf\u00fchrer der ChangeIT GmbH! Liebe Mitarbeiterinnen und Mitarbeiter der Abteilung Daten- und Prozessanalyse, ich wende mich heute an Sie mit einem neuen Projektvorschlag. Ich bin auf die Idee gekommen, eine K\u00fcnstliche Intelligenz zu entwickeln, die Bewerbungsschreiben im Vorfeld bewerten und aussortieren kann. Diese KI soll uns dabei helfen, die Bewerbungsprozesse zu optimieren und die effektive Arbeit der Personalabteilung zu unterst\u00fctzen. Ich bin davon \u00fcberzeugt, dass wir mit unserem Know-how und unserer Erfahrung in der Daten- und Prozessanalyse die ideale Abteilung sind, um ein solches Projekt erfolgreich umzusetzen. Die KI soll in der Lage sein, Bewerbungsschreiben auf verschiedene Kriterien hin zu analysieren, wie zum Beispiel die \u00dcbereinstimmung mit den Anforderungen der ausgeschriebenen Stelle oder die Pr\u00e4sentation der eigenen Erfahrungen und Qualifikationen. Ich m\u00f6chte Sie alle dazu ermutigen, an diesem Projekt mitzuarbeiten und Ihre Ideen einzubringen. Gemeinsam k\u00f6nnen wir eine KI entwickeln, die nicht nur unseren eigenen Arbeitsprozess optimiert, sondern auch anderen Unternehmen als L\u00f6sung angeboten werden kann. Ich w\u00fcrde mich sehr freuen, wenn Sie sich an diesem Projekt beteiligen und uns Ihre Ideen und Anregungen mitteilen w\u00fcrden. Wenn Sie Fragen haben oder weitere Informationen ben\u00f6tigen, stehe ich Ihnen gerne zur Verf\u00fcgung. Mit freundlichen Gr\u00fc\u00dfen, Dr. Harald W\u00f6hler Aufgabe : Verfassen Sie zu dieser Email eine Antwort.","title":"Reflexion"},{"location":"LS1/","text":"FIDP - Lernfeld 10c LS 10.1: Maschine Learning / Deep Learning Systeme im Alltag Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3 Kompetenzformulierung \" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.1: Maschine Learning Systeme im Alltag 2 Unterrichtsstunden Handlungssituation Die Change IT GmbH pr\u00fcft ob es sinnvoll w\u00e4re eine eigne Abteilung zur Daten- und Prozessanalyse mit dem Schwerpunkt maschinelles Lernen und Deep Learning zu gr\u00fcnden. Sie erhalten den Auftrag die Begriffe KI, Maschine Learning, Deep Learning zu beschreiben, Anwendungsbeispiele f\u00fcr diese Technologien zu finden und ggf. moralische Grenzen aufzuzeigen. Handlungsergebnis Pr\u00e4sentation zu KI und deren wirtschaftlichen Nutzen und moralische Aspekte. Vorausgesetzte F\u00e4higkeiten und Kenntnisse Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Nutzungsszenarien von KI am Alltag nennen Begriffe wie ML,Deep Learning und KI einordnen Moralische Probleme identifizieren Die Sch\u00fclerinnen und Sch\u00fcler benennen in einer Brainstormingphase allt\u00e4gliche Nutzungsszenarien f\u00fcr KI Sie informieren sich durch einen Artikel aus der ct \u00fcber weitere Einsatzszenarien und identifizieren moralische Grenzen der Technologie Einzelarbeit Plenum Wortwolke Planen / Entscheiden Kategorien von KI Problemen nennen Die Sch\u00fcler informieren sich \u00fcber Kategorien von KI Problemen Einzelarbeit Durchf\u00fchren Anwendungsszenarien von KI Kategorisieren Die Sch\u00fclerinnen und Sch\u00fcler ordnen die zuvor identifizierten Einsatzszenarien den Kategorien zu Gruppenarbeit Board Kontrollieren / Bewerten Die eigene Meinung \u00e4u\u00dfern und kritisch hinterfragen Die Sch\u00fclerinnen und Sch\u00fcler Pr\u00e4sentieren und Diskutieren ihre Einteilung Plenum Reflektieren Grenzen der KI Nutzung erkennen Die Sch\u00fclerinnen und Sch\u00fcler verfassen eine EMail Antwort Einzelarbeit Plenum Arbeitsmaterialien / Links Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository Schulische Entscheidungen Das zusammentragen der Ergebnisse erfolgt in einem elektronischen Board. Leistungsnachweise Bewertung des erstellten elektronischen Boards M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern","title":"LS10c.1"},{"location":"LS1/#fidp-lernfeld-10c","text":"","title":"FIDP - Lernfeld 10c"},{"location":"LS1/#ls-101-maschine-learning-deep-learning-systeme-im-alltag","text":"Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3","title":"LS 10.1: Maschine Learning / Deep Learning Systeme im Alltag"},{"location":"LS1/#kompetenzformulierung","text":"\" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.1: Maschine Learning Systeme im Alltag 2 Unterrichtsstunden","title":"Kompetenzformulierung"},{"location":"LS1/#handlungssituation","text":"Die Change IT GmbH pr\u00fcft ob es sinnvoll w\u00e4re eine eigne Abteilung zur Daten- und Prozessanalyse mit dem Schwerpunkt maschinelles Lernen und Deep Learning zu gr\u00fcnden. Sie erhalten den Auftrag die Begriffe KI, Maschine Learning, Deep Learning zu beschreiben, Anwendungsbeispiele f\u00fcr diese Technologien zu finden und ggf. moralische Grenzen aufzuzeigen.","title":"Handlungssituation"},{"location":"LS1/#handlungsergebnis","text":"Pr\u00e4sentation zu KI und deren wirtschaftlichen Nutzen und moralische Aspekte.","title":"Handlungsergebnis"},{"location":"LS1/#vorausgesetzte-fahigkeiten-und-kenntnisse","text":"Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Nutzungsszenarien von KI am Alltag nennen Begriffe wie ML,Deep Learning und KI einordnen Moralische Probleme identifizieren Die Sch\u00fclerinnen und Sch\u00fcler benennen in einer Brainstormingphase allt\u00e4gliche Nutzungsszenarien f\u00fcr KI Sie informieren sich durch einen Artikel aus der ct \u00fcber weitere Einsatzszenarien und identifizieren moralische Grenzen der Technologie Einzelarbeit Plenum Wortwolke Planen / Entscheiden Kategorien von KI Problemen nennen Die Sch\u00fcler informieren sich \u00fcber Kategorien von KI Problemen Einzelarbeit Durchf\u00fchren Anwendungsszenarien von KI Kategorisieren Die Sch\u00fclerinnen und Sch\u00fcler ordnen die zuvor identifizierten Einsatzszenarien den Kategorien zu Gruppenarbeit Board Kontrollieren / Bewerten Die eigene Meinung \u00e4u\u00dfern und kritisch hinterfragen Die Sch\u00fclerinnen und Sch\u00fcler Pr\u00e4sentieren und Diskutieren ihre Einteilung Plenum Reflektieren Grenzen der KI Nutzung erkennen Die Sch\u00fclerinnen und Sch\u00fcler verfassen eine EMail Antwort Einzelarbeit Plenum","title":"Vorausgesetzte F\u00e4higkeiten und Kenntnisse"},{"location":"LS1/#arbeitsmaterialien-links","text":"Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository","title":"Arbeitsmaterialien / Links"},{"location":"LS1/#schulische-entscheidungen","text":"Das zusammentragen der Ergebnisse erfolgt in einem elektronischen Board.","title":"Schulische Entscheidungen"},{"location":"LS1/#leistungsnachweise","text":"Bewertung des erstellten elektronischen Boards","title":"Leistungsnachweise"},{"location":"LS1/#mogliche-verknupfungen-zu-anderen-lernfeldern-fachern","text":"","title":"M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern"},{"location":"LS2/","text":"FIDP - Lernfeld 10c LS 10.2: Datenexploration und Visualisierung Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3 Kompetenzformulierung \" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.2: Datenexploration und Visualisierung 7 Unterrichtsstunden Handlungssituation Eine der ersten Kunde der neu eingef\u00fchrten Abteilung \"Daten- und Prozessanalyse\" der ChangeIT GmbH ist eine gro\u00dfe berufsbildenden Schule. Diese Schule m\u00f6chte gerne die Leistungsdaten eines Jahrgangs ausgewertet haben. Diese Daten liegen sowohl als csv , xml und json vor. Handlungsergebnis Statistische Grunddaten des Datensatzes Visualisierung einiger Zusammenh\u00e4nge Vorausgesetzte F\u00e4higkeiten und Kenntnisse Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren - Daten und ihre Darstellung in unterschiedlichen Formaten - Analysieren der zur Verf\u00fcgung gestellten Daten - Informieren sich \u00fcber die Berechnung statistischer Grunddaten - Einzelarbeit - Plenum Planen / Entscheiden - Arbeiten mit Python Paketen - Auswahl eines Datensatzes -Einlesen des Datensatzes mit einem Python Programm - Einzelarbeit Durchf\u00fchren - Umwandeln und Zusammenfassen von Daten - grafische Darstellung von Datenmengen - Bereinigen der Daten - Vorbereiten der Daten f\u00fcr die statistische Auswertung - Erheben der statistischen Grunddaten - Visualisieren der Daten - Einzelarbiet Kontrollieren / Bewerten - Eignung unterschiedlicher grafischer Darstellungsformen - Bewerten des Ergebnisses hinsichtlich der ursp\u00fcnglichen Fragestellung - Einzelarbeit - Plenum Reflektieren - Kritikf\u00e4higkeit - Aussagekraft der statistischen Grunddaten - Plenum Arbeitsmaterialien / Links Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository Schulische Entscheidungen Die Ermittlung der statistischen Grunddaten und das erzeugen der Diagramm erfolgt mittels der Programmiersprache Python und der Entwicklungsumgebung VS Code. Leistungsnachweise M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern","title":"LS10c.2"},{"location":"LS2/#fidp-lernfeld-10c","text":"","title":"FIDP - Lernfeld 10c"},{"location":"LS2/#ls-102-datenexploration-und-visualisierung","text":"Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3","title":"LS 10.2: Datenexploration und Visualisierung"},{"location":"LS2/#kompetenzformulierung","text":"\" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.2: Datenexploration und Visualisierung 7 Unterrichtsstunden","title":"Kompetenzformulierung"},{"location":"LS2/#handlungssituation","text":"Eine der ersten Kunde der neu eingef\u00fchrten Abteilung \"Daten- und Prozessanalyse\" der ChangeIT GmbH ist eine gro\u00dfe berufsbildenden Schule. Diese Schule m\u00f6chte gerne die Leistungsdaten eines Jahrgangs ausgewertet haben. Diese Daten liegen sowohl als csv , xml und json vor.","title":"Handlungssituation"},{"location":"LS2/#handlungsergebnis","text":"Statistische Grunddaten des Datensatzes Visualisierung einiger Zusammenh\u00e4nge","title":"Handlungsergebnis"},{"location":"LS2/#vorausgesetzte-fahigkeiten-und-kenntnisse","text":"Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren - Daten und ihre Darstellung in unterschiedlichen Formaten - Analysieren der zur Verf\u00fcgung gestellten Daten - Informieren sich \u00fcber die Berechnung statistischer Grunddaten - Einzelarbeit - Plenum Planen / Entscheiden - Arbeiten mit Python Paketen - Auswahl eines Datensatzes -Einlesen des Datensatzes mit einem Python Programm - Einzelarbeit Durchf\u00fchren - Umwandeln und Zusammenfassen von Daten - grafische Darstellung von Datenmengen - Bereinigen der Daten - Vorbereiten der Daten f\u00fcr die statistische Auswertung - Erheben der statistischen Grunddaten - Visualisieren der Daten - Einzelarbiet Kontrollieren / Bewerten - Eignung unterschiedlicher grafischer Darstellungsformen - Bewerten des Ergebnisses hinsichtlich der ursp\u00fcnglichen Fragestellung - Einzelarbeit - Plenum Reflektieren - Kritikf\u00e4higkeit - Aussagekraft der statistischen Grunddaten - Plenum","title":"Vorausgesetzte F\u00e4higkeiten und Kenntnisse"},{"location":"LS2/#arbeitsmaterialien-links","text":"Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository","title":"Arbeitsmaterialien / Links"},{"location":"LS2/#schulische-entscheidungen","text":"Die Ermittlung der statistischen Grunddaten und das erzeugen der Diagramm erfolgt mittels der Programmiersprache Python und der Entwicklungsumgebung VS Code.","title":"Schulische Entscheidungen"},{"location":"LS2/#leistungsnachweise","text":"","title":"Leistungsnachweise"},{"location":"LS2/#mogliche-verknupfungen-zu-anderen-lernfeldern-fachern","text":"","title":"M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern"},{"location":"LS3/","text":"FIDP - Lernfeld 10c LS 10.3: ML auf der Grundlage von Entscheidungsb\u00e4umen modellieren Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3 Kompetenzformulierung \" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.3: ML auf der Grundlage von Entscheidungsb\u00e4umen modellieren 10 Unterrichtsstunden Handlungssituation Ein gro\u00dfer Landmaschinenhersteller w\u00fcnscht sich eine gr\u00f6\u00dfere Kundenbindung und beauftragt die ChangeIT GmbH mit der Entwicklung einer App, die Landwirten Empfehlungen gibt, wann der Weizen zu ernten ist. Die App misst dazu \u00fcber einen via Bluetooth gekoppelten Feuchte-Sensor die Bodenfeuchte im Feld und kann \u00fcber eine API Abfrage die Regenwahrscheinlichkeit bestimmen. Erste Erfahrungswerte liegen bereits vor und werden vom Landmaschinenhersteller der ChangeIT in Form einer CSV Datei zur Verf\u00fcgung gestellt. Als Mitglied der Abteilung Daten- und Prozessanalyse erhalten Sie die Aufgabe ein geeignetes Vorhersagemodell zu entwickeln. Handlungsergebnis Ein Modell auf der Grundlage eines Entscheidungsbaumes, um f\u00fcr eine App Bauern die Empfehlung zu geben, ob Weizen zu ernten ist oder doch besser noch gewartet werden soll! Vorausgesetzte F\u00e4higkeiten und Kenntnisse Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Daten Visualisieren Muster in Datenstrukturen erkennen Entropie einer Datenmenge bestimmen Die Sch\u00fclerinnen und Sch\u00fcler stellen den Datensatz grafisch dar Sie versuchen Muster f\u00fcr Entwicklung eines Modells zu erkennen Plenum Einzelarbeit Planen / Entscheiden Auf der Grundlage der Entropie Teildatenmengen einteilen Die Sch\u00fclerinnen und Sch\u00fclern entwickeln eine Funktion zur Bestimmung der Entropie und wenden die an einer Teildatenmenge an Einzelarbeit Plenum Durchf\u00fchren Einen Entscheidungsbaum entwickeln Unter Verwendung der Python Bibliothek sklearn implementieren die Sch\u00fclerinnen und Sch\u00fcler ein Vorhersagemodell Einzelarbeit Kontrollieren / Bewerten Bewerten eines Vorhersagemodells Die Sch\u00fclerinnen und Sch\u00fcler bewerten das erzeugte Vorhersagemodell Einzelarbeit Plenum Reflektieren Selbst\u00e4ndige Probleml\u00f6sung Die Sch\u00fclerinnen und Sch\u00fcler w\u00e4hlen einen weiteren geeigneten Datensatz aus und entwickeln f\u00fcr diesen ein Vorhersagemodell und stellen das entwickelte Modell vor Gruppenarbeit Plenum Arbeitsmaterialien / Links Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository Schulische Entscheidungen Die Programmiersprache Python wird verwendet Als Entwicklungsumgebung wird VS Code / Jupyter Notebook benutzt Leistungsnachweise Die Pr\u00e4sentation der einzelnen Vorhersagemodelle kann zur Leistungsbewertung herangezogen werden! M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern","title":"LS10c.3"},{"location":"LS3/#fidp-lernfeld-10c","text":"","title":"FIDP - Lernfeld 10c"},{"location":"LS3/#ls-103-ml-auf-der-grundlage-von-entscheidungsbaumen-modellieren","text":"Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3","title":"LS 10.3: ML auf der Grundlage von Entscheidungsb\u00e4umen modellieren"},{"location":"LS3/#kompetenzformulierung","text":"\" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.3: ML auf der Grundlage von Entscheidungsb\u00e4umen modellieren 10 Unterrichtsstunden","title":"Kompetenzformulierung"},{"location":"LS3/#handlungssituation","text":"Ein gro\u00dfer Landmaschinenhersteller w\u00fcnscht sich eine gr\u00f6\u00dfere Kundenbindung und beauftragt die ChangeIT GmbH mit der Entwicklung einer App, die Landwirten Empfehlungen gibt, wann der Weizen zu ernten ist. Die App misst dazu \u00fcber einen via Bluetooth gekoppelten Feuchte-Sensor die Bodenfeuchte im Feld und kann \u00fcber eine API Abfrage die Regenwahrscheinlichkeit bestimmen. Erste Erfahrungswerte liegen bereits vor und werden vom Landmaschinenhersteller der ChangeIT in Form einer CSV Datei zur Verf\u00fcgung gestellt. Als Mitglied der Abteilung Daten- und Prozessanalyse erhalten Sie die Aufgabe ein geeignetes Vorhersagemodell zu entwickeln.","title":"Handlungssituation"},{"location":"LS3/#handlungsergebnis","text":"Ein Modell auf der Grundlage eines Entscheidungsbaumes, um f\u00fcr eine App Bauern die Empfehlung zu geben, ob Weizen zu ernten ist oder doch besser noch gewartet werden soll!","title":"Handlungsergebnis"},{"location":"LS3/#vorausgesetzte-fahigkeiten-und-kenntnisse","text":"Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Daten Visualisieren Muster in Datenstrukturen erkennen Entropie einer Datenmenge bestimmen Die Sch\u00fclerinnen und Sch\u00fcler stellen den Datensatz grafisch dar Sie versuchen Muster f\u00fcr Entwicklung eines Modells zu erkennen Plenum Einzelarbeit Planen / Entscheiden Auf der Grundlage der Entropie Teildatenmengen einteilen Die Sch\u00fclerinnen und Sch\u00fclern entwickeln eine Funktion zur Bestimmung der Entropie und wenden die an einer Teildatenmenge an Einzelarbeit Plenum Durchf\u00fchren Einen Entscheidungsbaum entwickeln Unter Verwendung der Python Bibliothek sklearn implementieren die Sch\u00fclerinnen und Sch\u00fcler ein Vorhersagemodell Einzelarbeit Kontrollieren / Bewerten Bewerten eines Vorhersagemodells Die Sch\u00fclerinnen und Sch\u00fcler bewerten das erzeugte Vorhersagemodell Einzelarbeit Plenum Reflektieren Selbst\u00e4ndige Probleml\u00f6sung Die Sch\u00fclerinnen und Sch\u00fcler w\u00e4hlen einen weiteren geeigneten Datensatz aus und entwickeln f\u00fcr diesen ein Vorhersagemodell und stellen das entwickelte Modell vor Gruppenarbeit Plenum","title":"Vorausgesetzte F\u00e4higkeiten und Kenntnisse"},{"location":"LS3/#arbeitsmaterialien-links","text":"Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository","title":"Arbeitsmaterialien / Links"},{"location":"LS3/#schulische-entscheidungen","text":"Die Programmiersprache Python wird verwendet Als Entwicklungsumgebung wird VS Code / Jupyter Notebook benutzt","title":"Schulische Entscheidungen"},{"location":"LS3/#leistungsnachweise","text":"Die Pr\u00e4sentation der einzelnen Vorhersagemodelle kann zur Leistungsbewertung herangezogen werden!","title":"Leistungsnachweise"},{"location":"LS3/#mogliche-verknupfungen-zu-anderen-lernfeldern-fachern","text":"","title":"M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern"},{"location":"LS4/","text":"FIDP - Lernfeld 10c LS 10.4: K-Mean Clustering Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3 Kompetenzformulierung \" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.4: k-mean Clustering 8 Unterrichtsstunden Handlungssituation Ein Regionaler Stromanbieter will in seinem Versorgungsgebiet Lades\u00e4ulen f\u00fcr Elektroautos anbieten. Dabei sollen m\u00f6glichst alle Kunden einen kurzen Weg zur angebotenen Lades\u00e4ule habe. Die Abteilung Daten- und Prozessanalyse der ChangeIT GmbH wird damit beauftragt diese Standorte zu ermitteln. Der Kunde stellt dazu einen Plan zur Verf\u00fcgung, der die Wohnorte der Kunden, die ein Elektroauto besitzen enth\u00e4lt. Handlungsergebnis Eine Modell welches auf der Grundlage des K-Mean Clustering den besten Standort f\u00fcr Lades\u00e4ulen ermittelt. Vorausgesetzte F\u00e4higkeiten und Kenntnisse Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Datenexploration und Visualisierung Die Sch\u00fclerinnen und Sch\u00fcler lesen die zur Verf\u00fcgung gestellten Daten ein und stellen sie grafisch dar Einzelarbeit Plenum Planen / Entscheiden Erstes Clustering anwenden Die Sch\u00fclerinnen und Sch\u00fcler w\u00e4hlen einen geeigneten Punkt f. die Platzierung von Ladens\u00e4ulen aus und ermitteln die zugeh\u00f6rigen Datenpunkte mittel der Euklidische Distanz Einzelarbeit Plenum Durchf\u00fchren Das k-mean Clustering zur Probleml\u00f6sung anwenden Die Sch\u00fclerinnen und Sch\u00fcler f\u00fchren iterativ den Algorithmus des k-mean Clusterings aus un bewerten die Ergebnisse Einzelarbeit Kontrollieren / Bewerten Bibliotheken f. das k-mean Clustering anwenden Die Sch\u00fclerinnen und Sch\u00fclern verwenden die Bibliothek KMeans aus dem Paket sklearn.cluster und stellen die Ergebnisse der Bibliothek grafisch dar und bewerten diese Plenum Einzelarbeit Reflektieren Transfer auf weitere Einsatzm\u00f6glichkeiten Die Sch\u00fclerinnen und Sch\u00fcler erstellen eine Wortwolke zu weiteren Anwendungsf\u00e4llen f\u00fcr das k-mean Clustering Plenum Arbeitsmaterialien / Links Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository Schulische Entscheidungen Als Programmiersprache wird Python verwendet als Entwicklungsumgebung wird vs code und Jupyter Notebook verwendet Leistungsnachweise Ggf. Pr\u00e4sentationen zu weiteren Einsatzm\u00f6glichkeiten des k-mean Clusterings. M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern","title":"LS10c.4"},{"location":"LS4/#fidp-lernfeld-10c","text":"","title":"FIDP - Lernfeld 10c"},{"location":"LS4/#ls-104-k-mean-clustering","text":"Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3","title":"LS 10.4: K-Mean Clustering"},{"location":"LS4/#kompetenzformulierung","text":"\" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.4: k-mean Clustering 8 Unterrichtsstunden","title":"Kompetenzformulierung"},{"location":"LS4/#handlungssituation","text":"Ein Regionaler Stromanbieter will in seinem Versorgungsgebiet Lades\u00e4ulen f\u00fcr Elektroautos anbieten. Dabei sollen m\u00f6glichst alle Kunden einen kurzen Weg zur angebotenen Lades\u00e4ule habe. Die Abteilung Daten- und Prozessanalyse der ChangeIT GmbH wird damit beauftragt diese Standorte zu ermitteln. Der Kunde stellt dazu einen Plan zur Verf\u00fcgung, der die Wohnorte der Kunden, die ein Elektroauto besitzen enth\u00e4lt.","title":"Handlungssituation"},{"location":"LS4/#handlungsergebnis","text":"Eine Modell welches auf der Grundlage des K-Mean Clustering den besten Standort f\u00fcr Lades\u00e4ulen ermittelt.","title":"Handlungsergebnis"},{"location":"LS4/#vorausgesetzte-fahigkeiten-und-kenntnisse","text":"Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Datenexploration und Visualisierung Die Sch\u00fclerinnen und Sch\u00fcler lesen die zur Verf\u00fcgung gestellten Daten ein und stellen sie grafisch dar Einzelarbeit Plenum Planen / Entscheiden Erstes Clustering anwenden Die Sch\u00fclerinnen und Sch\u00fcler w\u00e4hlen einen geeigneten Punkt f. die Platzierung von Ladens\u00e4ulen aus und ermitteln die zugeh\u00f6rigen Datenpunkte mittel der Euklidische Distanz Einzelarbeit Plenum Durchf\u00fchren Das k-mean Clustering zur Probleml\u00f6sung anwenden Die Sch\u00fclerinnen und Sch\u00fcler f\u00fchren iterativ den Algorithmus des k-mean Clusterings aus un bewerten die Ergebnisse Einzelarbeit Kontrollieren / Bewerten Bibliotheken f. das k-mean Clustering anwenden Die Sch\u00fclerinnen und Sch\u00fclern verwenden die Bibliothek KMeans aus dem Paket sklearn.cluster und stellen die Ergebnisse der Bibliothek grafisch dar und bewerten diese Plenum Einzelarbeit Reflektieren Transfer auf weitere Einsatzm\u00f6glichkeiten Die Sch\u00fclerinnen und Sch\u00fcler erstellen eine Wortwolke zu weiteren Anwendungsf\u00e4llen f\u00fcr das k-mean Clustering Plenum","title":"Vorausgesetzte F\u00e4higkeiten und Kenntnisse"},{"location":"LS4/#arbeitsmaterialien-links","text":"Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository","title":"Arbeitsmaterialien / Links"},{"location":"LS4/#schulische-entscheidungen","text":"Als Programmiersprache wird Python verwendet als Entwicklungsumgebung wird vs code und Jupyter Notebook verwendet","title":"Schulische Entscheidungen"},{"location":"LS4/#leistungsnachweise","text":"Ggf. Pr\u00e4sentationen zu weiteren Einsatzm\u00f6glichkeiten des k-mean Clusterings.","title":"Leistungsnachweise"},{"location":"LS4/#mogliche-verknupfungen-zu-anderen-lernfeldern-fachern","text":"","title":"M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern"},{"location":"LS5/","text":"FIDP - Lernfeld 10c LS 10.5: Regressionsanalyse Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3 Kompetenzformulierung \" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.5: Regressionsanalyse 10 Unterrichtsstunden Handlungssituation Ein gro\u00dfes internationales Immobilienb\u00fcro beauftragt die Abteilung Daten- und Prozessanalyse der ChangeIT GmbH mit der Entwicklung eines Modells zur Vorhersage von Immobilienpreise. Sie wirken ma\u00dfgeblich an der Entwicklung des Vorhersagemodells mit und Beurteilen die Qualit\u00e4t des Modells. Handlungsergebnis Vorhersagemodell f\u00fcr Immobilienpreise. Vorausgesetzte F\u00e4higkeiten und Kenntnisse Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Fachkompetenz : - Verst\u00e4ndnis des Konzepts der linearen Regression - Vorstellung des Boston Housing Datasets als Beispiel - Erl\u00e4uterung der mathematischen Formel f\u00fcr die lineare Regression Personale Kompetenz : - Interesse an Datenanalyse und -interpretation - Offenheit f\u00fcr neue Methoden und Herangehensweisen - Einf\u00fchrung in das Konzept der linearen Regression und seine praktische Anwendung - Vorstellung des Boston Housing Datasets - Visualisierung ausgew\u00e4hlter Features des Datensatzes - Erl\u00e4uterung der mathematischen Formel f\u00fcr die lineare Regression - \u00dcben der linearen Regression an einem Einfache Beispiel mit 5-6 Datens\u00e4tzen - Vorstellung von Python-Bibliotheken wie NumPy, Pandas, Matplotlib, Scikit - Lehrervortrag - Gruppenarbeit - Diskussion im Plenum Planen / Entscheiden Fachkompetenz : - Auswahl geeigneter Features Personale Kompetenz : - F\u00e4higkeit zur Zusammenarbeit in Gruppen - Entscheidungsf\u00e4higkeit - Auswahl geeigneter Datens\u00e4tze / Features - Diskussion im Plenum Durchf\u00fchren Fachkompetenz : - Anwendung der linearen Regression in Python - Interpretation der Ergebnisse Personale Kompetenz : - F\u00e4higkeit zur Zusammenarbeit in Gruppen - Kreativit\u00e4t - Anwendung der linearen Regression in Python auf die ausgew\u00e4hlten Datens\u00e4tze / Features - Interpretation der Ergebnisse - Erstellung von eigenen Modellen - Gruppenarbeit - Diskussion im Plenum Kontrollieren / Bewerten Fachkompetenz : - Bewertung der Ergebnisse anhand von Kriterien Personale Kompetenz : - F\u00e4higkeit zur Selbstreflexion - Feedbackf\u00e4higkeit - Pr\u00e4sentation der Ergebnisse durch jede Gruppe - Bewertung der Ergebnisse anhand von Kriterien wie Genauigkeit, Kreativit\u00e4t und Interpretation der Ergebnisse - \u00dcben des Gelernten in Gruppen am Beispiel weiterer Datens\u00e4tze - Gruppenpr\u00e4sentation - Bewertung im Plenum -Gruppenpr\u00e4sentation Reflektieren Fachkompetenz : - Reflexion \u00fcber das Gelernte Personale Kompetenz : - F\u00e4higkeit zur Selbstreflexion - Offenheit f\u00fcr Feedback - Verfassen einer Antwort auf eine Email vom Geschaftsf\u00fchrer - Feedback von Lehrer oder Lehrerin - Feedbackrunde Arbeitsmaterialien / Links Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen oder als pdf GitHub Repository Weitere Datens\u00e4tze k\u00f6nnen \u00fcber die Seite https://www.kaggle.com/ Schulische Entscheidungen Zur Entwicklung des Vorhersagemodells wird die Programmiersprache Python verwendet und das Jupyter Notebook genutzt. Leistungsnachweise Die zu haltenden Pr\u00e4sentationen k\u00f6nnen zur Leistungskontrolle verwendet werden! M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern","title":"LS10c.5"},{"location":"LS5/#fidp-lernfeld-10c","text":"","title":"FIDP - Lernfeld 10c"},{"location":"LS5/#ls-105-regressionsanalyse","text":"Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3","title":"LS 10.5: Regressionsanalyse"},{"location":"LS5/#kompetenzformulierung","text":"\" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.5: Regressionsanalyse 10 Unterrichtsstunden","title":"Kompetenzformulierung"},{"location":"LS5/#handlungssituation","text":"Ein gro\u00dfes internationales Immobilienb\u00fcro beauftragt die Abteilung Daten- und Prozessanalyse der ChangeIT GmbH mit der Entwicklung eines Modells zur Vorhersage von Immobilienpreise. Sie wirken ma\u00dfgeblich an der Entwicklung des Vorhersagemodells mit und Beurteilen die Qualit\u00e4t des Modells.","title":"Handlungssituation"},{"location":"LS5/#handlungsergebnis","text":"Vorhersagemodell f\u00fcr Immobilienpreise.","title":"Handlungsergebnis"},{"location":"LS5/#vorausgesetzte-fahigkeiten-und-kenntnisse","text":"Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Fachkompetenz : - Verst\u00e4ndnis des Konzepts der linearen Regression - Vorstellung des Boston Housing Datasets als Beispiel - Erl\u00e4uterung der mathematischen Formel f\u00fcr die lineare Regression Personale Kompetenz : - Interesse an Datenanalyse und -interpretation - Offenheit f\u00fcr neue Methoden und Herangehensweisen - Einf\u00fchrung in das Konzept der linearen Regression und seine praktische Anwendung - Vorstellung des Boston Housing Datasets - Visualisierung ausgew\u00e4hlter Features des Datensatzes - Erl\u00e4uterung der mathematischen Formel f\u00fcr die lineare Regression - \u00dcben der linearen Regression an einem Einfache Beispiel mit 5-6 Datens\u00e4tzen - Vorstellung von Python-Bibliotheken wie NumPy, Pandas, Matplotlib, Scikit - Lehrervortrag - Gruppenarbeit - Diskussion im Plenum Planen / Entscheiden Fachkompetenz : - Auswahl geeigneter Features Personale Kompetenz : - F\u00e4higkeit zur Zusammenarbeit in Gruppen - Entscheidungsf\u00e4higkeit - Auswahl geeigneter Datens\u00e4tze / Features - Diskussion im Plenum Durchf\u00fchren Fachkompetenz : - Anwendung der linearen Regression in Python - Interpretation der Ergebnisse Personale Kompetenz : - F\u00e4higkeit zur Zusammenarbeit in Gruppen - Kreativit\u00e4t - Anwendung der linearen Regression in Python auf die ausgew\u00e4hlten Datens\u00e4tze / Features - Interpretation der Ergebnisse - Erstellung von eigenen Modellen - Gruppenarbeit - Diskussion im Plenum Kontrollieren / Bewerten Fachkompetenz : - Bewertung der Ergebnisse anhand von Kriterien Personale Kompetenz : - F\u00e4higkeit zur Selbstreflexion - Feedbackf\u00e4higkeit - Pr\u00e4sentation der Ergebnisse durch jede Gruppe - Bewertung der Ergebnisse anhand von Kriterien wie Genauigkeit, Kreativit\u00e4t und Interpretation der Ergebnisse - \u00dcben des Gelernten in Gruppen am Beispiel weiterer Datens\u00e4tze - Gruppenpr\u00e4sentation - Bewertung im Plenum -Gruppenpr\u00e4sentation Reflektieren Fachkompetenz : - Reflexion \u00fcber das Gelernte Personale Kompetenz : - F\u00e4higkeit zur Selbstreflexion - Offenheit f\u00fcr Feedback - Verfassen einer Antwort auf eine Email vom Geschaftsf\u00fchrer - Feedback von Lehrer oder Lehrerin - Feedbackrunde","title":"Vorausgesetzte F\u00e4higkeiten und Kenntnisse"},{"location":"LS5/#arbeitsmaterialien-links","text":"Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen oder als pdf GitHub Repository Weitere Datens\u00e4tze k\u00f6nnen \u00fcber die Seite https://www.kaggle.com/","title":"Arbeitsmaterialien / Links"},{"location":"LS5/#schulische-entscheidungen","text":"Zur Entwicklung des Vorhersagemodells wird die Programmiersprache Python verwendet und das Jupyter Notebook genutzt.","title":"Schulische Entscheidungen"},{"location":"LS5/#leistungsnachweise","text":"Die zu haltenden Pr\u00e4sentationen k\u00f6nnen zur Leistungskontrolle verwendet werden!","title":"Leistungsnachweise"},{"location":"LS5/#mogliche-verknupfungen-zu-anderen-lernfeldern-fachern","text":"","title":"M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern"},{"location":"LS6/","text":"FIDP - Lernfeld 10c LS 10.6: K-nearest Neighbor (KNN) Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3 Kompetenzformulierung \" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.6: K-nearest Neighbor (KNN) 8 Unterrichtsstunden Handlungssituation Ein Online Fahrradh\u00e4ndler m\u00f6chte seinen Kunden stets Fahrr\u00e4der in der optimale Rahmenh\u00f6he anbieten. Dazu soll eine App inkl. Vorhersagemodell entwickelt werden, so dass der Kunde lediglich ein Foto aufnehmen muss und ihm die optimale Rahmenh\u00f6he empfohlen wird. Eine Abteilung der ChangeIT GmbH ist bereits damit beauftragt aus einem Bild die Schrittl\u00e4nge und die K\u00f6rpergr\u00f6\u00dfe (jeweils in cm) zu ermitteln. Sie sollen ein Vorhersagemodell entwickeln, welches dem Kunden die richtige Rahmengr\u00f6\u00dfe vorschl\u00e4gt. Der Online Fahrradh\u00e4ndler hat bereits Daten in Form von Erfahrungswerten vorliegen und stellt ihnen diese in Form einer CSV Datei zur Verf\u00fcgung. Handlungsergebnis Vorhersagemodell auf der Grundlage von KNN Vorausgesetzte F\u00e4higkeiten und Kenntnisse Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Exploration der Daten Die Sch\u00fclerinnen und Sch\u00fcler visualisieren die zur Verf\u00fcgung gestellten Daten. Einzelarbeit Planen / Entscheiden Erste Ideen f\u00fcr die Entwicklung eines Vorhersagemodells entwickeln Die Sch\u00fclerinnen und Sch\u00fcler entwickeln erste Ideen f\u00fcr die Implementierung eines Vorhersagemodells Einzelarbeit Plenum Durchf\u00fchren KNN Algorithmus anwenden Die Sch\u00fclerinnen und Sch\u00fcler wenden den KNN Algorithmus an und beobachten die Qualit\u00e4t des Vorhersagemodells anhand unterschiedlicher k-Werte Sie nutzen Python Bibliotheken zum Implementieren des Vorhersagemodells Einzelarbeit Plenum Kontrollieren / Bewerten Metriken anwenden Die Sch\u00fclerinnen und Sch\u00fcler beurteilen die Qualit\u00e4t ihres Vorhersagemodells indem Sie Metriken berechnen wie Accuracy , Precision , Recall . Sie diskutieren im Klassenverband \u00fcber die Aussagekraft der Metriken Einzelarbeit Plenum Reflektieren Die Sch\u00fclerinnen und Sch\u00fcler reflektieren die Unterrichtseinheit durch Beantwortung einiger Fragen zum KNN Algorithmus Einzelarbeit Arbeitsmaterialien / Links Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository Schulische Entscheidungen Die Implementierung des Algorithmus erfolgt in Python unter Zuhilfenahme von Jupyter Notebook Leistungsnachweise Kurzer Test zu Metriken in Klassifizierungsalgorithmen. M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern","title":"LS10c.6"},{"location":"LS6/#fidp-lernfeld-10c","text":"","title":"FIDP - Lernfeld 10c"},{"location":"LS6/#ls-106-k-nearest-neighbor-knn","text":"Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3","title":"LS 10.6: K-nearest Neighbor (KNN)"},{"location":"LS6/#kompetenzformulierung","text":"\" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.6: K-nearest Neighbor (KNN) 8 Unterrichtsstunden","title":"Kompetenzformulierung"},{"location":"LS6/#handlungssituation","text":"Ein Online Fahrradh\u00e4ndler m\u00f6chte seinen Kunden stets Fahrr\u00e4der in der optimale Rahmenh\u00f6he anbieten. Dazu soll eine App inkl. Vorhersagemodell entwickelt werden, so dass der Kunde lediglich ein Foto aufnehmen muss und ihm die optimale Rahmenh\u00f6he empfohlen wird. Eine Abteilung der ChangeIT GmbH ist bereits damit beauftragt aus einem Bild die Schrittl\u00e4nge und die K\u00f6rpergr\u00f6\u00dfe (jeweils in cm) zu ermitteln. Sie sollen ein Vorhersagemodell entwickeln, welches dem Kunden die richtige Rahmengr\u00f6\u00dfe vorschl\u00e4gt. Der Online Fahrradh\u00e4ndler hat bereits Daten in Form von Erfahrungswerten vorliegen und stellt ihnen diese in Form einer CSV Datei zur Verf\u00fcgung.","title":"Handlungssituation"},{"location":"LS6/#handlungsergebnis","text":"Vorhersagemodell auf der Grundlage von KNN","title":"Handlungsergebnis"},{"location":"LS6/#vorausgesetzte-fahigkeiten-und-kenntnisse","text":"Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Exploration der Daten Die Sch\u00fclerinnen und Sch\u00fcler visualisieren die zur Verf\u00fcgung gestellten Daten. Einzelarbeit Planen / Entscheiden Erste Ideen f\u00fcr die Entwicklung eines Vorhersagemodells entwickeln Die Sch\u00fclerinnen und Sch\u00fcler entwickeln erste Ideen f\u00fcr die Implementierung eines Vorhersagemodells Einzelarbeit Plenum Durchf\u00fchren KNN Algorithmus anwenden Die Sch\u00fclerinnen und Sch\u00fcler wenden den KNN Algorithmus an und beobachten die Qualit\u00e4t des Vorhersagemodells anhand unterschiedlicher k-Werte Sie nutzen Python Bibliotheken zum Implementieren des Vorhersagemodells Einzelarbeit Plenum Kontrollieren / Bewerten Metriken anwenden Die Sch\u00fclerinnen und Sch\u00fcler beurteilen die Qualit\u00e4t ihres Vorhersagemodells indem Sie Metriken berechnen wie Accuracy , Precision , Recall . Sie diskutieren im Klassenverband \u00fcber die Aussagekraft der Metriken Einzelarbeit Plenum Reflektieren Die Sch\u00fclerinnen und Sch\u00fcler reflektieren die Unterrichtseinheit durch Beantwortung einiger Fragen zum KNN Algorithmus Einzelarbeit","title":"Vorausgesetzte F\u00e4higkeiten und Kenntnisse"},{"location":"LS6/#arbeitsmaterialien-links","text":"Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository","title":"Arbeitsmaterialien / Links"},{"location":"LS6/#schulische-entscheidungen","text":"Die Implementierung des Algorithmus erfolgt in Python unter Zuhilfenahme von Jupyter Notebook","title":"Schulische Entscheidungen"},{"location":"LS6/#leistungsnachweise","text":"Kurzer Test zu Metriken in Klassifizierungsalgorithmen.","title":"Leistungsnachweise"},{"location":"LS6/#mogliche-verknupfungen-zu-anderen-lernfeldern-fachern","text":"","title":"M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern"},{"location":"LS7/","text":"FIDP - Lernfeld 10c LS 10.7: Neuronale Netze Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3 Kompetenzformulierung \" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.7: Neuronale Netze 12 Unterrichtsstunden Handlungssituation Die Firma Home-IoT ist eine bekannter Hersteller von Smart Home Produkten. Es ist geplant f\u00fcr diese Firma eine smarte Lichtsteuerung \"AI Light\" zu entwickeln, die an die jeweiligen Anforderungen der Kunden angepasst werden kann. Der Chefentwickler der Abteilung Daten- und Prozessanalyse der ChangeIT GmbH beauftragt Sie damit ein Neuronalen Netz zu entwickeln und dieses f\u00fcr eine exemplarische Anforderung zu trainieren. Handlungsergebnis Ein trainiertes neuronales Netz f\u00fcr die Lichtsteuerung Vorausgesetzte F\u00e4higkeiten und Kenntnisse Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Analyse des Problems Die Sch\u00fclerinnen und Sch\u00fcler analysieren das Problem und entwerfen eine Wahrheitstabelle f\u00fcr die Lichtsteuerung gem\u00e4\u00df Kundenanforderung - Einzelarbeit Planen / Entscheiden Layer Struktur von Neuronalen Netzen Aktivierungsfunktionen Die Sch\u00fclerinnen und Sch\u00fcler entwerfen die Architektur eines neuronalen Netzes mit Input, Output und Hidden Layer und entscheiden sich f\u00fcr eine geeignete Aktivierungsfunktion - Einzelarbeit - Plenum Durchf\u00fchren Forward- und Backward Propagation Trainieren von neuronales Netz Die Sch\u00fclerinnen und Sch\u00fclern entwerfen das neuronale Netz mit Hilfe von TensorFlow und trainieren dieses mit den ermittelten Daten - Einzelarbeit - Plenum Kontrollieren / Bewerten Qualitative Aspekte (Metriken) von neuronalen Netzes Anhand von Testdaten bewerten die Sch\u00fclerinnen und Sch\u00fcler die Qualit\u00e4t des Modells und entwickeln Ideen die Qualit\u00e4t der Vorhersagen zu verbessern. - Einzelarbeit - Plenum Reflektieren Die Phasen zur Entwicklung eines neuronalen Netzes reflextieren - Die Sch\u00fclerinnen und Sch\u00fcler beantworten Fragen zur Unterrichtseinheit - Die Sch\u00fclerinnen und Sch\u00fcler w\u00e4hlen einen eigenen Datensatz und versuchen eine mit dem Datensatz verkn\u00fcpfte Fragestellung mit Hilfe eines neuronalen Netzes zu beantworten - Einzelarbeit - Gruppenarbeit - Plenum Arbeitsmaterialien / Links Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository Schulische Entscheidungen Zur Implementierung und Testen des neuronalen Netzes wird die Programmiersprache Python mit der Bibliothek TensorFlow verwendet Als Entwicklungsumgebung wir VS Code mit Jupyter Notebook genutzt. Leistungsnachweise Die Pr\u00e4sentationen zu unterschiedlichen Datens\u00e4tzen in der Reflexionsphase der Unterrichtseinheit k\u00f6nnen zur Bewertung genutzt werden. M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern Politik / Religion: Ethische und moralische Fragestellungen im Bezug zur Anwendung von neuronalen Netzen.","title":"LS10c.7"},{"location":"LS7/#fidp-lernfeld-10c","text":"","title":"FIDP - Lernfeld 10c"},{"location":"LS7/#ls-107-neuronale-netze","text":"Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3","title":"LS 10.7: Neuronale Netze"},{"location":"LS7/#kompetenzformulierung","text":"\" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.7: Neuronale Netze 12 Unterrichtsstunden","title":"Kompetenzformulierung"},{"location":"LS7/#handlungssituation","text":"Die Firma Home-IoT ist eine bekannter Hersteller von Smart Home Produkten. Es ist geplant f\u00fcr diese Firma eine smarte Lichtsteuerung \"AI Light\" zu entwickeln, die an die jeweiligen Anforderungen der Kunden angepasst werden kann. Der Chefentwickler der Abteilung Daten- und Prozessanalyse der ChangeIT GmbH beauftragt Sie damit ein Neuronalen Netz zu entwickeln und dieses f\u00fcr eine exemplarische Anforderung zu trainieren.","title":"Handlungssituation"},{"location":"LS7/#handlungsergebnis","text":"Ein trainiertes neuronales Netz f\u00fcr die Lichtsteuerung","title":"Handlungsergebnis"},{"location":"LS7/#vorausgesetzte-fahigkeiten-und-kenntnisse","text":"Handlungskompetenz (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Analyse des Problems Die Sch\u00fclerinnen und Sch\u00fcler analysieren das Problem und entwerfen eine Wahrheitstabelle f\u00fcr die Lichtsteuerung gem\u00e4\u00df Kundenanforderung - Einzelarbeit Planen / Entscheiden Layer Struktur von Neuronalen Netzen Aktivierungsfunktionen Die Sch\u00fclerinnen und Sch\u00fcler entwerfen die Architektur eines neuronalen Netzes mit Input, Output und Hidden Layer und entscheiden sich f\u00fcr eine geeignete Aktivierungsfunktion - Einzelarbeit - Plenum Durchf\u00fchren Forward- und Backward Propagation Trainieren von neuronales Netz Die Sch\u00fclerinnen und Sch\u00fclern entwerfen das neuronale Netz mit Hilfe von TensorFlow und trainieren dieses mit den ermittelten Daten - Einzelarbeit - Plenum Kontrollieren / Bewerten Qualitative Aspekte (Metriken) von neuronalen Netzes Anhand von Testdaten bewerten die Sch\u00fclerinnen und Sch\u00fcler die Qualit\u00e4t des Modells und entwickeln Ideen die Qualit\u00e4t der Vorhersagen zu verbessern. - Einzelarbeit - Plenum Reflektieren Die Phasen zur Entwicklung eines neuronalen Netzes reflextieren - Die Sch\u00fclerinnen und Sch\u00fcler beantworten Fragen zur Unterrichtseinheit - Die Sch\u00fclerinnen und Sch\u00fcler w\u00e4hlen einen eigenen Datensatz und versuchen eine mit dem Datensatz verkn\u00fcpfte Fragestellung mit Hilfe eines neuronalen Netzes zu beantworten - Einzelarbeit - Gruppenarbeit - Plenum","title":"Vorausgesetzte F\u00e4higkeiten und Kenntnisse"},{"location":"LS7/#arbeitsmaterialien-links","text":"Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository","title":"Arbeitsmaterialien / Links"},{"location":"LS7/#schulische-entscheidungen","text":"Zur Implementierung und Testen des neuronalen Netzes wird die Programmiersprache Python mit der Bibliothek TensorFlow verwendet Als Entwicklungsumgebung wir VS Code mit Jupyter Notebook genutzt.","title":"Schulische Entscheidungen"},{"location":"LS7/#leistungsnachweise","text":"Die Pr\u00e4sentationen zu unterschiedlichen Datens\u00e4tzen in der Reflexionsphase der Unterrichtseinheit k\u00f6nnen zur Bewertung genutzt werden.","title":"Leistungsnachweise"},{"location":"LS7/#mogliche-verknupfungen-zu-anderen-lernfeldern-fachern","text":"Politik / Religion: Ethische und moralische Fragestellungen im Bezug zur Anwendung von neuronalen Netzen.","title":"M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern"},{"location":"LS8/","text":"FIDP - Lernfeld 10c LS 10.8: Cloud KI Systeme nutzen Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3 Kompetenzformulierung \" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.8: Cloud KI Systeme nutzen 12 Unterrichtsstunden Handlungssituation Sie arbeiten bei der ChangeIT GmbH in der Abteilung Datenanalyse und erhalten folgenden Auftrag: Der Besitzer eines Eigenheims m\u00f6chte gerne wissen welche Autos f\u00fcr wie lange auf seinen Parkplatz parken. Die Kamera vor dem Haus mach dazu alle 5 Minuten ein Bild von dem Parkplatz. Sie erhalten der Auftrag eine Anwendungssystem zu entwickeln welches das KFZ Kennzeichen erfasst und dieses mit einem Zeitstempel in eine Datenbank schreibt. Da das Antrainieren einer eigenen Texterkennungs KI zu aufwendig erscheint, entscheidet sich die Gesch\u00e4ftsf\u00fchrung dazu einen Cloud Dienstleister zu nutzen. Da bereits ein Azure Konto existiert, soll der Azure Bilderkennungsdienst genutzt werden. Handlungsergebnis Bildanalyse (KFZ Kennzeichen) eines geparkten Autos. Vorausgesetzte F\u00e4higkeiten und Kenntnisse Handlungskompetenz \\(\\newline\\) (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Die Sch\u00fclerinnen und Sch\u00fcler Analysieren den Auftrag und identifizieren einzelne Arbeitspakete. F\u00fcr diese Arbeitspakete bestimmen Sie, ob dazu noch weitere Informationen notwendig sind. \\(\\newline\\) F\u00fcr das Arbeitspaket des maschinellen Sehens mit Azure lesen Sie den zur Verf\u00fcgung gestellten Artikel. - Zerlegen des Auftrages in einzelne Arbeitspakete \\(\\newline\\) - Die Azure REST Api Einzelarbeit / Klassenplenum \\(\\newline\\) \\(\\newline\\) Hinweise f\u00fcr den DU : Das Gliedern der Arbeitspakete kann in Moodle \u00fcber die Aktivit\u00e4t Moodle Board erfolgen Planen / Entscheiden Die Sch\u00fclerinnen und Sch\u00fclern Planen in Kleingruppen das weitere Vorgehen zum Bew\u00e4ltigen des Arbeitsauftrages. Dazu Priorisieren Sie einzelne Arbeitspakete nach ihrer Zeitlichen Reihenfolge und Wichtigkeit Priorisieren von Arbeitspaketen Gruppenarbeit Durchf\u00fchren Die Sch\u00fclerinnen und Sch\u00fcler implementieren die gew\u00fcnschte Funktionalit\u00e4t entsprechend des Arbeitsauftrages Implementieren der Funktionalit\u00e4t Moodle Aufgabe A1 bis A3 ( A4 ist optional) Kontrollieren / Bewerten Die Sch\u00fclerinnen und Sch\u00fcler bewerten das Ergebnis hinsichtlich der Qualit\u00e4t. \\(\\newline\\) Die Sch\u00fclerinnen und Sch\u00fcler erstellen f\u00fcr den Kunden ein Angebot aus denen die Kosten f\u00fcr den T\u00e4glichen Betrieb des Systems ersichtlich werden. - \u00dcberpr\u00fcfen der Funktionalit\u00e4t. \\(\\newline\\) \\(\\newline\\) - Erstellen eines Angebotes Gruppenarbeit \\(\\newline\\) Moodle Aufgabe A5 Reflektieren Die Sch\u00fclerinnen und Sch\u00fcler erhalten eine Email eines Autobesitzers und verfassen eine Antwort, die m\u00f6gliche Datenschutzprobleme thematisiert Schreiben einer Email bzgl. Datenschutzaspekte bei automatischen KI Systemen Moodle Aufgabe A6 Arbeitsmaterialien / Links Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository Schulische Entscheidungen F\u00fcr die Nutzung des Azure Dienstes kann der Zugang genutzt werden, der im Rahmen des O365 Angebots vorliegt. Die Implementierung der Software sollte in Python unter Verwendung von VS-Code erfolgen. Hier sollte m\u00f6glichst das in der Python Extension enthaltene Jupyter Notebook verwenden werden. Leistungsnachweise Bewertung des Handlungsergebnisses ggf. im Form eines Fachgespr\u00e4ches mit den einzelnen Arbeitsgruppen. M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern Politik / Ethik: Sekund\u00e4re personengebundene Daten","title":"LS10c.8"},{"location":"LS8/#fidp-lernfeld-10c","text":"","title":"FIDP - Lernfeld 10c"},{"location":"LS8/#ls-108-cloud-ki-systeme-nutzen","text":"Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3","title":"LS 10.8: Cloud KI Systeme nutzen"},{"location":"LS8/#kompetenzformulierung","text":"\" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.8: Cloud KI Systeme nutzen 12 Unterrichtsstunden","title":"Kompetenzformulierung"},{"location":"LS8/#handlungssituation","text":"Sie arbeiten bei der ChangeIT GmbH in der Abteilung Datenanalyse und erhalten folgenden Auftrag: Der Besitzer eines Eigenheims m\u00f6chte gerne wissen welche Autos f\u00fcr wie lange auf seinen Parkplatz parken. Die Kamera vor dem Haus mach dazu alle 5 Minuten ein Bild von dem Parkplatz. Sie erhalten der Auftrag eine Anwendungssystem zu entwickeln welches das KFZ Kennzeichen erfasst und dieses mit einem Zeitstempel in eine Datenbank schreibt. Da das Antrainieren einer eigenen Texterkennungs KI zu aufwendig erscheint, entscheidet sich die Gesch\u00e4ftsf\u00fchrung dazu einen Cloud Dienstleister zu nutzen. Da bereits ein Azure Konto existiert, soll der Azure Bilderkennungsdienst genutzt werden.","title":"Handlungssituation"},{"location":"LS8/#handlungsergebnis","text":"Bildanalyse (KFZ Kennzeichen) eines geparkten Autos.","title":"Handlungsergebnis"},{"location":"LS8/#vorausgesetzte-fahigkeiten-und-kenntnisse","text":"Handlungskompetenz \\(\\newline\\) (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren Die Sch\u00fclerinnen und Sch\u00fcler Analysieren den Auftrag und identifizieren einzelne Arbeitspakete. F\u00fcr diese Arbeitspakete bestimmen Sie, ob dazu noch weitere Informationen notwendig sind. \\(\\newline\\) F\u00fcr das Arbeitspaket des maschinellen Sehens mit Azure lesen Sie den zur Verf\u00fcgung gestellten Artikel. - Zerlegen des Auftrages in einzelne Arbeitspakete \\(\\newline\\) - Die Azure REST Api Einzelarbeit / Klassenplenum \\(\\newline\\) \\(\\newline\\) Hinweise f\u00fcr den DU : Das Gliedern der Arbeitspakete kann in Moodle \u00fcber die Aktivit\u00e4t Moodle Board erfolgen Planen / Entscheiden Die Sch\u00fclerinnen und Sch\u00fclern Planen in Kleingruppen das weitere Vorgehen zum Bew\u00e4ltigen des Arbeitsauftrages. Dazu Priorisieren Sie einzelne Arbeitspakete nach ihrer Zeitlichen Reihenfolge und Wichtigkeit Priorisieren von Arbeitspaketen Gruppenarbeit Durchf\u00fchren Die Sch\u00fclerinnen und Sch\u00fcler implementieren die gew\u00fcnschte Funktionalit\u00e4t entsprechend des Arbeitsauftrages Implementieren der Funktionalit\u00e4t Moodle Aufgabe A1 bis A3 ( A4 ist optional) Kontrollieren / Bewerten Die Sch\u00fclerinnen und Sch\u00fcler bewerten das Ergebnis hinsichtlich der Qualit\u00e4t. \\(\\newline\\) Die Sch\u00fclerinnen und Sch\u00fcler erstellen f\u00fcr den Kunden ein Angebot aus denen die Kosten f\u00fcr den T\u00e4glichen Betrieb des Systems ersichtlich werden. - \u00dcberpr\u00fcfen der Funktionalit\u00e4t. \\(\\newline\\) \\(\\newline\\) - Erstellen eines Angebotes Gruppenarbeit \\(\\newline\\) Moodle Aufgabe A5 Reflektieren Die Sch\u00fclerinnen und Sch\u00fcler erhalten eine Email eines Autobesitzers und verfassen eine Antwort, die m\u00f6gliche Datenschutzprobleme thematisiert Schreiben einer Email bzgl. Datenschutzaspekte bei automatischen KI Systemen Moodle Aufgabe A6","title":"Vorausgesetzte F\u00e4higkeiten und Kenntnisse"},{"location":"LS8/#arbeitsmaterialien-links","text":"Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository","title":"Arbeitsmaterialien / Links"},{"location":"LS8/#schulische-entscheidungen","text":"F\u00fcr die Nutzung des Azure Dienstes kann der Zugang genutzt werden, der im Rahmen des O365 Angebots vorliegt. Die Implementierung der Software sollte in Python unter Verwendung von VS-Code erfolgen. Hier sollte m\u00f6glichst das in der Python Extension enthaltene Jupyter Notebook verwenden werden.","title":"Schulische Entscheidungen"},{"location":"LS8/#leistungsnachweise","text":"Bewertung des Handlungsergebnisses ggf. im Form eines Fachgespr\u00e4ches mit den einzelnen Arbeitsgruppen.","title":"Leistungsnachweise"},{"location":"LS8/#mogliche-verknupfungen-zu-anderen-lernfeldern-fachern","text":"Politik / Ethik: Sekund\u00e4re personengebundene Daten","title":"M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern"},{"location":"LS9/","text":"FIDP - Lernfeld 10c LS 10.9: Verst\u00e4rkendes Lernen Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3 Kompetenzformulierung \" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.9: Verst\u00e4rkendes Lernen 11 Unterrichtsstunden Handlungssituation Ein gro\u00dfer deutscher internationaler Automobilkonzern, plant die Einf\u00fchrung einer autonomen Taxiflotte. Dabei sollen Fahrg\u00e4ste ein fest definierten Stationen die M\u00f6glichkeit haben eine Fahrt zu buchen und eine weitere Station als Zielort anzugeben. Der Chefentwickler der Abteilung Daten- und Prozessanalyse der ChangeIT GmbH schl\u00e4gt vor dieses Problem mit Hilfe des verst\u00e4rkenden Lernens zu l\u00f6sen. Die Kollegen der Anwendungsentwicklung haben dazu bereits eine Simulationsumgebung geschaffen. Ihre Aufgabe wird es sein, einen Lernalgorithmus zu entwickeln, der in dieser Simulationsumgebung Fahrg\u00e4ste aufnimmt und optimal zu ihrem Ziel bef\u00f6rdert. Handlungsergebnis Machine Learning Modell zum automatischen Bef\u00f6rdern von Passagieren Vorausgesetzte F\u00e4higkeiten und Kenntnisse Handlungskompetenz \\(\\newline\\) (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren - Grundlagen des Reinforced Learning - Analysieren der Simulationsumgebung Die Sch\u00fclerinnen und Sch\u00fcler informieren sich \u00fcber Grundbegriffe des Reinforced Learning, wie Agent , Environment , Action , Reward - Sie untersuchen die Simulationsumgebung und f\u00fchre erste Aktionen durch und beobachten die Reaktion der Simulationsumgebung - Einzelarbeit - Plenum - Als Umgebung wird das Taxi Environment der RIL Umgebung aus dem OpenAI Paket verwendet Planen / Entscheiden - Brute Force Ansatz Die Sch\u00fclerinnen und Sch\u00fcler versuchen zun\u00e4chst die Aufgabe mittels eine Brute Force Ansatzes zu l\u00f6sen - Einzelarbeit - Plenum Durchf\u00fchren - Q-Learning Algorithmus Die Sch\u00fclerinnen und Sch\u00fcler implementieren den Q-Learning Algorithmus und trainieren dieses anhand des gegebenen Modells - Einzelarbeit - Plenum Kontrollieren / Bewerten - Qualit\u00e4tsaspekte f\u00fcr das RiL Modell Die Sch\u00fclerinnen und Sch\u00fcler beurteilen die Qualit\u00e4t des entwickelten Modells, Sie ver\u00e4ndern wichtige Parameter um ggf. die Qualit\u00e4t zu verbessern - Einzelarbeit Reflektieren - Die Sch\u00fclerinnen und Sch\u00fcler erarbeiten ein ML-Modell f\u00fcr das Reinforced Learning anhand einer weiteren Umgebung - Die Sch\u00fclerinnen und Sch\u00fcler beantworten Fragen zur Lerneinheit Erarbeiten eines ML Modells an einer anderen Umgebung auf dem GYM Framework - Gruppenarbeit - als weitere Umgebungen bieten sich folgende Umgebungen an ( FrozenLake , Cliff Walking , MountainCar und CartPole ) Arbeitsmaterialien / Links Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository Schulische Entscheidungen Die Implementierung der Software sollte in Python unter Verwendung von VS-Code erfolgen. Hier sollte m\u00f6glichst das in der Python Extension enthaltene Jupyter Notebook verwenden werden. Als Simulationsumgebung wir das GYM Framework von OpenAI verwendet Leistungsnachweise Bewertung des Handlungsergebnisses aus der Reflexionsphase ggf. im Form eines Fachgespr\u00e4ches mit den einzelnen Arbeitsgruppen. M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern","title":"LS10c.9"},{"location":"LS9/#fidp-lernfeld-10c","text":"","title":"FIDP - Lernfeld 10c"},{"location":"LS9/#ls-109-verstarkendes-lernen","text":"Lernfeld Bildungsgang Ausbildungsjahr LF 10c: \\(\\newline\\) Werkzeuge des maschinellen Lernens einsetzen Fachinformatiker f\u00fcr Daten- und Prozessanalyse (FIDP) 3","title":"LS 10.9: Verst\u00e4rkendes Lernen"},{"location":"LS9/#kompetenzformulierung","text":"\" Die Sch\u00fclerinnen und Sch\u00fcler verf\u00fcgen \u00fcber die Kompetenz, maschinelles Lernen zur Probleml\u00f6sung anzuwenden und den Lernfortschritt des Entscheidungssystems zu begleiten \". Die Sch\u00fclerinnen und Sch\u00fcler stellen Einsatzm\u00f6glichkeiten des maschinellen Lernens dar . Auf dieser Basis entscheiden sie \u00fcber die betriebswirtschaftlich sinnvolle Eignung maschinellen Lernens bez\u00fcglich kundenspezifischer Problemstellungen. Sie f\u00fchren die ben\u00f6tigten Daten zusammen. Dazu analysieren sie freie und kommerzielle Datenquellen und w\u00e4hlen diese nach Eignung zur L\u00f6sung der Aufgabe durch maschinelles Lernen aus. Die Sch\u00fclerinnen und Sch\u00fcler ber\u00fccksichtigen datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Sie legen f\u00fcr die Aufgabenstellung maschinellen Lernens ad\u00e4quate Werkzeuge und Systeme fest . Sie bereiten das ausgew\u00e4hlte System technisch vor und implementieren die Schnittstellen zum Datenimport. Die Sch\u00fclerinnen und Sch\u00fcler \u00fcberwachen die technische Funktionsf\u00e4higkeit im Hinblick auf den Lernfortschritt des Systems. Sie reflektieren die Wirksamkeit des angelernten Entscheidungssystems. Dabei diskutieren sie auch datenschutzrechtliche, moralische und wirtschaftliche Aspekte. Curricularer Bezug Titel der Lernsituation (Kurzfassung) Geplanter Zeitrichtwert Rahmenlehrplan f\u00fcr Fachinformatiker f\u00fcr Daten- und Prozessanalyse in der Fassung vom 13.12.2019, S. 27 LS 10.9: Verst\u00e4rkendes Lernen 11 Unterrichtsstunden","title":"Kompetenzformulierung"},{"location":"LS9/#handlungssituation","text":"Ein gro\u00dfer deutscher internationaler Automobilkonzern, plant die Einf\u00fchrung einer autonomen Taxiflotte. Dabei sollen Fahrg\u00e4ste ein fest definierten Stationen die M\u00f6glichkeit haben eine Fahrt zu buchen und eine weitere Station als Zielort anzugeben. Der Chefentwickler der Abteilung Daten- und Prozessanalyse der ChangeIT GmbH schl\u00e4gt vor dieses Problem mit Hilfe des verst\u00e4rkenden Lernens zu l\u00f6sen. Die Kollegen der Anwendungsentwicklung haben dazu bereits eine Simulationsumgebung geschaffen. Ihre Aufgabe wird es sein, einen Lernalgorithmus zu entwickeln, der in dieser Simulationsumgebung Fahrg\u00e4ste aufnimmt und optimal zu ihrem Ziel bef\u00f6rdert.","title":"Handlungssituation"},{"location":"LS9/#handlungsergebnis","text":"Machine Learning Modell zum automatischen Bef\u00f6rdern von Passagieren","title":"Handlungsergebnis"},{"location":"LS9/#vorausgesetzte-fahigkeiten-und-kenntnisse","text":"Handlungskompetenz \\(\\newline\\) (Fachkompetenz und Personale Kompetenz) Inhalte Sozialform/Methoden Informieren bzw. Analysieren - Grundlagen des Reinforced Learning - Analysieren der Simulationsumgebung Die Sch\u00fclerinnen und Sch\u00fcler informieren sich \u00fcber Grundbegriffe des Reinforced Learning, wie Agent , Environment , Action , Reward - Sie untersuchen die Simulationsumgebung und f\u00fchre erste Aktionen durch und beobachten die Reaktion der Simulationsumgebung - Einzelarbeit - Plenum - Als Umgebung wird das Taxi Environment der RIL Umgebung aus dem OpenAI Paket verwendet Planen / Entscheiden - Brute Force Ansatz Die Sch\u00fclerinnen und Sch\u00fcler versuchen zun\u00e4chst die Aufgabe mittels eine Brute Force Ansatzes zu l\u00f6sen - Einzelarbeit - Plenum Durchf\u00fchren - Q-Learning Algorithmus Die Sch\u00fclerinnen und Sch\u00fcler implementieren den Q-Learning Algorithmus und trainieren dieses anhand des gegebenen Modells - Einzelarbeit - Plenum Kontrollieren / Bewerten - Qualit\u00e4tsaspekte f\u00fcr das RiL Modell Die Sch\u00fclerinnen und Sch\u00fcler beurteilen die Qualit\u00e4t des entwickelten Modells, Sie ver\u00e4ndern wichtige Parameter um ggf. die Qualit\u00e4t zu verbessern - Einzelarbeit Reflektieren - Die Sch\u00fclerinnen und Sch\u00fcler erarbeiten ein ML-Modell f\u00fcr das Reinforced Learning anhand einer weiteren Umgebung - Die Sch\u00fclerinnen und Sch\u00fcler beantworten Fragen zur Lerneinheit Erarbeiten eines ML Modells an einer anderen Umgebung auf dem GYM Framework - Gruppenarbeit - als weitere Umgebungen bieten sich folgende Umgebungen an ( FrozenLake , Cliff Walking , MountainCar und CartPole )","title":"Vorausgesetzte F\u00e4higkeiten und Kenntnisse"},{"location":"LS9/#arbeitsmaterialien-links","text":"Moodle-Kurs: LF10c: Werkzeuge des maschinellen Lernens einsetzen GitHub Repository","title":"Arbeitsmaterialien / Links"},{"location":"LS9/#schulische-entscheidungen","text":"Die Implementierung der Software sollte in Python unter Verwendung von VS-Code erfolgen. Hier sollte m\u00f6glichst das in der Python Extension enthaltene Jupyter Notebook verwenden werden. Als Simulationsumgebung wir das GYM Framework von OpenAI verwendet","title":"Schulische Entscheidungen"},{"location":"LS9/#leistungsnachweise","text":"Bewertung des Handlungsergebnisses aus der Reflexionsphase ggf. im Form eines Fachgespr\u00e4ches mit den einzelnen Arbeitsgruppen.","title":"Leistungsnachweise"},{"location":"LS9/#mogliche-verknupfungen-zu-anderen-lernfeldern-fachern","text":"","title":"M\u00f6gliche Verkn\u00fcpfungen zu anderen Lernfeldern / F\u00e4chern"},{"location":"ML_im_Alltag/","text":"Maschine Leaning im Alltag Handlungssituation Die Firma ChangeIT GmbH plant ihre Initiativen im Bereich des Maschine Learnings auszubauen. Sie werden vom Gesch\u00e4ftsf\u00fchrer der ChangeIT GmbH damit beauftragt zu ermitteln welche Einsatzgebiete Maschine-Learning heute erm\u00f6glicht und wo man im Alltag damit in Ber\u00fchrung kommt. Brainstorming In einer Brainstorming-Phase soll zun\u00e4chst gesammelt werden in welchen Situationen man mit Maschine-Learning in Ber\u00fchrung kommt. Ergebnisse Kategorisieren / Gruppieren Versuchen Sie in Gruppen die gesammelten Begriffe zu gruppieren (evtl. in Form einer Mindmap). Gibt es z.B. Gemeinsamkeiten wie. Statistisches Problem Arithmetisches Problem Neuronales Netzwerk (KI) etc. Reflexion F\u00fchre Sie im Klassenverband ein kurzes \"Blitzlicht\" durch, in dem Sie folgenden Satz erg\u00e4nzen. \"Neu f\u00fcr mich war ...\"","title":"Maschine Leaning im Alltag"},{"location":"ML_im_Alltag/#maschine-leaning-im-alltag","text":"","title":"Maschine Leaning im Alltag"},{"location":"ML_im_Alltag/#handlungssituation","text":"Die Firma ChangeIT GmbH plant ihre Initiativen im Bereich des Maschine Learnings auszubauen. Sie werden vom Gesch\u00e4ftsf\u00fchrer der ChangeIT GmbH damit beauftragt zu ermitteln welche Einsatzgebiete Maschine-Learning heute erm\u00f6glicht und wo man im Alltag damit in Ber\u00fchrung kommt.","title":"Handlungssituation"},{"location":"ML_im_Alltag/#brainstorming","text":"In einer Brainstorming-Phase soll zun\u00e4chst gesammelt werden in welchen Situationen man mit Maschine-Learning in Ber\u00fchrung kommt.","title":"Brainstorming"},{"location":"ML_im_Alltag/#ergebnisse-kategorisieren-gruppieren","text":"Versuchen Sie in Gruppen die gesammelten Begriffe zu gruppieren (evtl. in Form einer Mindmap). Gibt es z.B. Gemeinsamkeiten wie. Statistisches Problem Arithmetisches Problem Neuronales Netzwerk (KI) etc.","title":"Ergebnisse Kategorisieren / Gruppieren"},{"location":"ML_im_Alltag/#reflexion","text":"F\u00fchre Sie im Klassenverband ein kurzes \"Blitzlicht\" durch, in dem Sie folgenden Satz erg\u00e4nzen. \"Neu f\u00fcr mich war ...\"","title":"Reflexion"},{"location":"RegressionsAnalyse/","text":"Lineare Regression Handlungssituation Ein gro\u00dfes internationales Immobilienb\u00fcro beauftragt die Abteilung Daten- und Prozessanalyse der ChangeIT GmbH mit der Entwicklung eines Modells zur Vorhersage von Immobilienpreise. Sie wirken ma\u00dfgeblich an der Entwicklung des Vorhersagemodells mit und Beurteilen die Qualit\u00e4t des Modells. Die Daten Der Kunde stellt ihnen die Daten als CSV Datei zur Verf\u00fcgung. Sie k\u00f6nnen diese Datei hier laden. Der Datensatz hat dabei folgenden Aufbau. Die einzelnen Spalten bedeuten dabei: CRIM: Kriminalit\u00e4tsrate pro Kopf ZN: Anteil der Wohngrundst\u00fccke \u00fcber 25.000 Quadratfu\u00df INDUS: Anteil der Nicht-Einzelhandels-Gesch\u00e4ftsfl\u00e4chen pro Stadt CHAS: Charles River-Dummy-Variable (1, falls das Grundst\u00fcck an einem Fluss liegt, sonst 0) NOX: Stickoxidkonzentration (in Teilen pro 10 Millionen) RM: Durchschnittliche Anzahl von Zimmern pro Wohnung AGE: Anteil der im Jahr 1940 oder fr\u00fcher errichteten Wohnungen DIS: Gewichteter Abstand zu f\u00fcnf Bostoner Arbeitszentren RAD: Index der Zug\u00e4nglichkeit zu radialen Autobahnen TAX: Vollst\u00e4ndige Immobiliensteuer pro 10.000 Dollar PTRATIO: Sch\u00fcler-Lehrer-Verh\u00e4ltnis nach Stadtteilen B: Anteil an Afroamerikanern im Verh\u00e4ltnis zur Bev\u00f6lkerung pro Stadt LSTAT: Prozentsatz der Bev\u00f6lkerung mit niedrigem sozio\u00f6konomischem Status MEDV: Medianwert von Eigentumswohnungen in 1000er Dollar Im weiteren Verlauf wollen wir uns zun\u00e4chst anschauen welchen Einfluss das jeweilige Feature auf den Wert einer Eigentumswohnung ( MEDV ) hat. Zur Bestimmung nutzen wir die Perason-Korrelation des Wertes mit dem Zielfeature ( MEDV ). Der Pearson-Korrelationskoeffizient (r) misst die lineare Beziehung zwischen zwei Datens\u00e4tzen. Die allgemeine Formel lautet: \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\\) wobei: \\(x_i\\) und \\(y_i\\) die Werte der einzelnen Datenpunkte in den beiden Datens\u00e4tzen sind, \\(\\bar{x}\\) und \\(\\bar{y}\\) die Mittelwerte der entsprechenden Datens\u00e4tze sind. Wir k\u00f6nnen diese Formel unter Verwendung der Definitionen von Standardabweichung und Kovarianz vereinfachen. Die Kovarianz zwischen zwei Variablen X und Y ist definiert als: \\(\\text{Cov}(X,Y) = \\frac{1}{n}\\sum (x_i - \\bar{x})(y_i - \\bar{y})\\) Die Standardabweichung einer Variable X ist definiert als: \\(\\sigma_X = \\sqrt{\\frac{1}{n}\\sum (x_i - \\bar{x})^2}\\) und analog f\u00fcr Y. Setzen wir diese Definitionen in die Formel des Pearson-Korrelationskoeffizienten ein, erhalten wir: \\(r = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\) Dies ist die vereinfachte Formel des Pearson-Korrelationskoeffizienten, die zeigt, dass der Korrelationskoeffizient gleich dem Quotienten aus der Kovarianz der beiden Variablen und dem Produkt ihrer Standardabweichungen ist. Diese Formel ist besonders n\u00fctzlich, da sie direkt die Kovarianz und die Standardabweichungen verwendet, was die Berechnung in praktischen Anwendungen erleichtert. Hinweise : Siehe hierzu hauch das Studyfix Video unter https://studyflix.de/statistik/pearson-korrelation-1051 \u00dcbung 1 Es gelten folgende Beispieldaten: X Y 0.55 1.11 0.72 1.58 0.60 1.28 0.54 1.10 0.42 0.89 0.65 1.33 0.44 1.02 0.89 1.76 0.96 1.96 0.38 0.68 X = [ 0.548814 , 0.715189 , 0.602763 , 0.544883 , 0.423655 , 0.645894 , 0.437587 , 0.891773 , 0.963663 , 0.383442 ] Y = [ 1.112031 , 1.575806 , 1.281631 , 1.101934 , 0.891696 , 1.325156 , 1.024582 , 1.763030 , 1.958632 , 0.681473 ] Sch\u00e4tzen Sie zun\u00e4chst den Wert der Korrelation zwischen \\(X\\) und \\(Y\\) ab und bestimmen Sie im Anschluss daran den Pearson-Korrelationskoeffizient \\(r\\) . Ein Kollege der Abteilung Datenanalyse hat sich bereits die Daten angeschaut und basierend auf dem Daten die Korrelation der Daten als Heatmap dargestellt. Er stellt Ihnen das Ergebnis seiner Analyse zur Verf\u00fcgung: Aufgabe 1 W\u00e4hlen Sie anhand der Heatmap zun\u00e4chst ein weiteres Feature aus und stellen Sie dessen Einfluss auf den Wert einer Eigentumswohnung ( MEDV ) in Form einer Grafik (Scatter Plot) dar. Beispielhaft ist hier die Beziehung zwischen den Preis der Immobilie ( medv ) und der Anzahl der Zimmer ( rm ) dargestellt. lineare Regression Die lineare Regression versucht nun durch diese Datenmenge eine gerade zu legen, um auf diese Weise Vorhersagen von zuk\u00fcnftigen Ereignissen zu bestimmen. Wenn dieses gelingt, dann k\u00f6nnte z.B. ein Immobilienpreis anhand der Gr\u00f6\u00dfe der Wohnung bestimmt werden. Weiter Informationen: Studifix: Lineare Regression Eine lineare Gleichung hat dabei folgende allgemeine Form. \\[f(x) = a \\cdot x + b\\] Wobei a die Steigung und b der y-Achsenabschnitt der Geraden ist. Gesucht ist also eine L\u00f6sung mit den Werten a und b , die durch die gew\u00e4hlte Datenmenge eine Gerade repr\u00e4sentiert, wobei der Fehlern (also die Abweichung des Gerade zu den einzelnen Datenpunkten) zu minimieren ist. Die L\u00f6sung des Problem kann dabei iterativ als auch arithmetisch erfolgen. Eine arithmetische L\u00f6sung sieht dabei wie folgt aus. \\(a = \\frac{n\\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n\\sum_{i=1}^n x_i^2 - (\\sum_{i=1}^n x_i)^2}\\) \\(b = \\frac{\\sum_{i=1}^n y_i - a\\sum_{i=1}^n x_i}{n}\\) Dabei sind \\(x_i\\) f\u00fcr das o.g. Beispiel die Werte f\u00fcr die Anzahl der Zimmer pro Wohneinheit ( rm ) und \\(y_i\\) der daraus resultierende Preis der Wohnung. \u00dcbungsaufgabe 2 Gehen Sie von folgenden Daten aus: x = [ 4 , 6 , 8 , 10 , 12 ] y = [ 10 , 20 , 24 , 32 , 45 ] Wenn Sie diese Daten visualisieren erhalten Sie folgenden Darstellung. Berechnen Sie mit Hilfe der o.g. Formel die Geradengleichung \\(f(x) = a \\cdot x + b\\) . Hinweis : Nutzen Sie zur Berechnung und ggf. auch zur Visualisierung die zur Verf\u00fcgung gestellte Excel Tabelle lrg_ueb1.xls \u00dcbungsaufgabe 3 \u00dcberpr\u00fcfen Sie ihre berechneten Werte, indem Sie die Gerade \\(f(x) = a \\cdot x + b\\) in die Datenmenge einzeichen. Z.B. mit Hilfe des folgenden Python Codes. a = 2 # Ihr berechneter Wert f\u00fcr a b = 4 # Ihr berechneter Wert f\u00fcr b x = [ 4 , 6 , 8 , 10 , 12 ] y = [ 10 , 20 , 24 , 32 , 45 ] gx = [ 4 , 12 ] gy = [ a * 4 + b , a * 12 + b ] plt . plot ( gx , gy , color = \"red\" ) plt . scatter ( x , y ) plt . show () Das Berechnen der Steigung a und des Achsenabschnitts b ist nat\u00fcrlich auch mit Hilfe einer Methode aus der Bibliothek sklearn.linear_model konkret die Klasse LinearRegression . Importieren Sie also zun\u00e4chst die entsprechende Klasse: from sklearn.linear_model import LinearRegression import numpy as np # Gegebene Daten x = np . array ([ 4 , 6 , 8 , 10 , 12 ]) . reshape ( - 1 , 1 ) y = np . array ([ 10 , 20 , 24 , 32 , 45 ]) # Erstellung und Anpassung des linearen Regressionsmodells model = LinearRegression () model . fit ( x , y ) # Koeffizienten der Regressionsgeraden slope = model . coef_ [ 0 ] intercept = model . intercept_ \u00dcber die Methode fit() dieser Klasse k\u00f6nnen die Parameter a und b bestimmt werden. Diese befinden sich nach korrektem Aufruf der Methode im Attribut coef_[0] (f\u00fcr die Steigung a ) und intercept_ (f\u00fcr den Achsenabschnitt b ). \u00dcberpr\u00fcfen Sie mit Hilfe der Methode fit() ihre zuvor berechneten Werte und tragen Sie die Steigungsgerade in den Scatter Plot ein. \u00dcber die Methode predict() des Modells k\u00f6nnen Sie nun anhand des Modells Aussagen \u00fcber das Verhalten des Systems machen. Testen Sie diese indem Sie sich z.B. eine Aussage \u00fcber den Wert von \\(x=7\\) ausgeben lassen! Aufgabe 2 Bestimmen Sie die aus Aufgabe 1 ermittelten Korrelationen ( Features ) die Gerade \\(f(x) = a \\cdot x + b\\) und tragen Sie diese wie in der \u00dcbung in die Datenmenge ein. Erstellen Sie ferner mittels der Methode predict() des Modells eine Aussage, indem Sie den Wert des Features \u00fcbergeben und den Immobilienpreis angezeigt bekommen! Diskutieren Sie im Klassenverband, ob das nun erstellte Vorhersagemodell zum Prognostizieren des Hauspreises ausreichend ist, oder wie dieses verbessert werden kann? Anpassen des Modells f\u00fcr mehrere Features (Multiple Lineare Regression) Zur Optimierung des Modells sollen auch die anderen Features des Datensatzes genutzt werden. Dieses Verfahren nennt sich Multiple Lineare Regression. Gesucht wird dabei folgende Gleichung. \\[f(x) = a_1 \\cdot x_1 + a_2 \\cdot x_2+a_3 \\cdot x_3 + b\\] Dabei sind \\(a_1\\) bis \\(a_n\\) Faktoren f\u00fcr die einzelnen Features \\(x_1\\) bis \\(x_n\\) . Diese einzelnen Faktoren gilt es nun iterativ zu bestimmen. Weitere Informationen: Studifix: Multiple Lineare Regression Zur \u00dcberpr\u00fcfung des daraus entstandene Vorhersagemodells muss nun der Datensatz zun\u00e4chst in Trainingsdaten und Testdaten geteilt werden. Eine Teilung von 80/20 ist hierf\u00fcr ein g\u00e4ngiges Mittel. Zum Aufteilen ein Datensatzes in Trainings- / Testdaten dient auch wieder eine Methode aus dem Paket sklearn.model_selection , konkret die Methode train_test_split() die dieser Aufteilung vornimmt. from sklearn.model_selection import train_test_split feature_names = [ \"rm\" , \"b\" , \"dis\" ] X_train , X_test , y_train , y_test = train_test_split ( df [ feature_names ], df [ 'medv' ], test_size = 0.2 , random_state = 0 ) Trainieren des Modells Anschlie\u00dfend kann das Modell mit den Trainingsdaten trainiert werden. model = LinearRegression () model . fit ( X_train , y_train ) Die Einzelnen Faktoren \\(a_1\\) bis \\(a_n\\) befinden sich dabei wie zuvor in dem Attribut model.coeff_ in Form eines Arrays und model.intercept der Wert f\u00fcr den Achsenabschnitt b . Um nun aufgrund unseres Modells eine Vorhersage zu treffen, dient die Funktion predict im Objekt model . Dieses erwartet einen Datensatz als \u00dcbergabeparameter. R2 - Metrik \u00dcber die Qualit\u00e4t des Modells gibt u.a. Der Wert \\(R^2\\) (Determinationskoeffizient) Auskunft. Je h\u00f6her der \\(R^2\\) -Wert, desto besser die Vorhersage. from sklearn.metrics import r2_score y_pred = model . predict ( X_test ) r2 = r2_score ( y_test , y_pred ) print ( 'R^2: ' , r2 ) Aufgabe 3 F\u00fchren Sie wie beschrieben die Regressions Analyse mit 3-4 wichtigen Features mit dem Datensatz durch. W\u00e4hlen Sie dann einen Datensatz aus der Menge der Testdaten und lassen sich mit der Funktion predict() das Ergebnis f\u00fcr einen Immobilienpreis ausgeben. \u00dcberpr\u00fcfen Sie ferner die Qualit\u00e4t ihres Vorhersagemodells, indem Sie den Wertt \\(R^2\\) bestimmen. Diskutieren Sie Ma\u00dfnahmen, wie die Qualit\u00e4t des Vorhersagemodells verbessert werden kann! Aufgabe 4 Entscheiden Sie sich in einer Kleingruppe f\u00fcr einen geeigneten Datensatz zur Durchf\u00fchrung einer Regressionsanalyse und f\u00fchren Sie die Analyse durch. Erstellen Sie auf der Grundlage ihrer Analyse eine kurze Pr\u00e4sentation und Pr\u00e4sentieren diese anschlie\u00dfend kurz der Klasse ihr Ergebnis. Die Pr\u00e4sentation sollte beinhalten: Art / Beschreibung des Datensatzes Zu untersuchende Fragestellung Vorgehensweise Interpretation der Ergebnisse Folgende Datens\u00e4tze k\u00f6nnen z.B. genutzt werden: El Nino Dataset : Oceanographic and surface meteorological readings taken from a series of buoys positioned throughout the equatorial Pacific. Download / Beschreibung: https://www.kaggle.com/datasets/uciml/el-nino-dataset eBay auction data : Auction data from various eBay.com objects over various length auctions. Download / Beschreibung: https://www.kaggle.com/code/yingyingchen/ebay-auction-data-analysis/data Bike Sharing Dataset : Hourly and daily count of rental bikes in a large city. Download / Beschreibung: https://www.kaggle.com/datasets/hmavrodiev/london-bike-sharing-dataset Oder w\u00e4hlen Sie einen eigenen Datensatz z.B. aus https://www.kaggle.com/ ! Reflexion der Unterrichtseinheit Sie erhalten von ihrem Gesch\u00e4ftsf\u00fchrer Dr. W\u00f6hler die unten abgebildete Email. Verfassen Sie auf diese Email eine Antwort. \"Sehr geehrte Mitarbeiter der Abteilung Daten- und Prozessanalyse, ich gratuliere Ihnen von ganzem Herzen zu Ihrer erfolgreichen Fertigstellung des Modells zur Vorhersage von Immobilienpreisen. Alle Anstrengungen, die Sie auf sich genommen haben, um dieses Modell zum Laufen zu bringen, werden nun mit dem Ergebnis belohnt. Ausserdem m\u00f6chte ich betonen, dass wir in Zukunft alle Anforderungen f\u00fcr ein Modell datenbasierter Vorhersagen mit den Mitteln der linearen Regression l\u00f6sen sollten. Der Grund ist, dass lineare Regression uns die gr\u00f6\u00dftm\u00f6gliche Genauigkeit beim Entwerfen oder Verbessern von Modellen gew\u00e4hrleistet. Noch einmal herzlichen Gl\u00fcckwunsch an alle, die an diesem Projekt beteiligt waren. Bleiben Sie alle weiterhin motiviert, kreativ und innovativ. Vielen Dank, Dr. W\u00f6hler, Gesch\u00e4ftsf\u00fchrer ChangeIT GmbH\"","title":"Lineare Regression"},{"location":"RegressionsAnalyse/#lineare-regression","text":"","title":"Lineare Regression"},{"location":"RegressionsAnalyse/#handlungssituation","text":"Ein gro\u00dfes internationales Immobilienb\u00fcro beauftragt die Abteilung Daten- und Prozessanalyse der ChangeIT GmbH mit der Entwicklung eines Modells zur Vorhersage von Immobilienpreise. Sie wirken ma\u00dfgeblich an der Entwicklung des Vorhersagemodells mit und Beurteilen die Qualit\u00e4t des Modells.","title":"Handlungssituation"},{"location":"RegressionsAnalyse/#die-daten","text":"Der Kunde stellt ihnen die Daten als CSV Datei zur Verf\u00fcgung. Sie k\u00f6nnen diese Datei hier laden. Der Datensatz hat dabei folgenden Aufbau. Die einzelnen Spalten bedeuten dabei: CRIM: Kriminalit\u00e4tsrate pro Kopf ZN: Anteil der Wohngrundst\u00fccke \u00fcber 25.000 Quadratfu\u00df INDUS: Anteil der Nicht-Einzelhandels-Gesch\u00e4ftsfl\u00e4chen pro Stadt CHAS: Charles River-Dummy-Variable (1, falls das Grundst\u00fcck an einem Fluss liegt, sonst 0) NOX: Stickoxidkonzentration (in Teilen pro 10 Millionen) RM: Durchschnittliche Anzahl von Zimmern pro Wohnung AGE: Anteil der im Jahr 1940 oder fr\u00fcher errichteten Wohnungen DIS: Gewichteter Abstand zu f\u00fcnf Bostoner Arbeitszentren RAD: Index der Zug\u00e4nglichkeit zu radialen Autobahnen TAX: Vollst\u00e4ndige Immobiliensteuer pro 10.000 Dollar PTRATIO: Sch\u00fcler-Lehrer-Verh\u00e4ltnis nach Stadtteilen B: Anteil an Afroamerikanern im Verh\u00e4ltnis zur Bev\u00f6lkerung pro Stadt LSTAT: Prozentsatz der Bev\u00f6lkerung mit niedrigem sozio\u00f6konomischem Status MEDV: Medianwert von Eigentumswohnungen in 1000er Dollar Im weiteren Verlauf wollen wir uns zun\u00e4chst anschauen welchen Einfluss das jeweilige Feature auf den Wert einer Eigentumswohnung ( MEDV ) hat. Zur Bestimmung nutzen wir die Perason-Korrelation des Wertes mit dem Zielfeature ( MEDV ). Der Pearson-Korrelationskoeffizient (r) misst die lineare Beziehung zwischen zwei Datens\u00e4tzen. Die allgemeine Formel lautet: \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\\) wobei: \\(x_i\\) und \\(y_i\\) die Werte der einzelnen Datenpunkte in den beiden Datens\u00e4tzen sind, \\(\\bar{x}\\) und \\(\\bar{y}\\) die Mittelwerte der entsprechenden Datens\u00e4tze sind. Wir k\u00f6nnen diese Formel unter Verwendung der Definitionen von Standardabweichung und Kovarianz vereinfachen. Die Kovarianz zwischen zwei Variablen X und Y ist definiert als: \\(\\text{Cov}(X,Y) = \\frac{1}{n}\\sum (x_i - \\bar{x})(y_i - \\bar{y})\\) Die Standardabweichung einer Variable X ist definiert als: \\(\\sigma_X = \\sqrt{\\frac{1}{n}\\sum (x_i - \\bar{x})^2}\\) und analog f\u00fcr Y. Setzen wir diese Definitionen in die Formel des Pearson-Korrelationskoeffizienten ein, erhalten wir: \\(r = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\) Dies ist die vereinfachte Formel des Pearson-Korrelationskoeffizienten, die zeigt, dass der Korrelationskoeffizient gleich dem Quotienten aus der Kovarianz der beiden Variablen und dem Produkt ihrer Standardabweichungen ist. Diese Formel ist besonders n\u00fctzlich, da sie direkt die Kovarianz und die Standardabweichungen verwendet, was die Berechnung in praktischen Anwendungen erleichtert. Hinweise : Siehe hierzu hauch das Studyfix Video unter https://studyflix.de/statistik/pearson-korrelation-1051","title":"Die Daten"},{"location":"RegressionsAnalyse/#ubung-1","text":"Es gelten folgende Beispieldaten: X Y 0.55 1.11 0.72 1.58 0.60 1.28 0.54 1.10 0.42 0.89 0.65 1.33 0.44 1.02 0.89 1.76 0.96 1.96 0.38 0.68 X = [ 0.548814 , 0.715189 , 0.602763 , 0.544883 , 0.423655 , 0.645894 , 0.437587 , 0.891773 , 0.963663 , 0.383442 ] Y = [ 1.112031 , 1.575806 , 1.281631 , 1.101934 , 0.891696 , 1.325156 , 1.024582 , 1.763030 , 1.958632 , 0.681473 ] Sch\u00e4tzen Sie zun\u00e4chst den Wert der Korrelation zwischen \\(X\\) und \\(Y\\) ab und bestimmen Sie im Anschluss daran den Pearson-Korrelationskoeffizient \\(r\\) . Ein Kollege der Abteilung Datenanalyse hat sich bereits die Daten angeschaut und basierend auf dem Daten die Korrelation der Daten als Heatmap dargestellt. Er stellt Ihnen das Ergebnis seiner Analyse zur Verf\u00fcgung:","title":"\u00dcbung 1"},{"location":"RegressionsAnalyse/#aufgabe-1","text":"W\u00e4hlen Sie anhand der Heatmap zun\u00e4chst ein weiteres Feature aus und stellen Sie dessen Einfluss auf den Wert einer Eigentumswohnung ( MEDV ) in Form einer Grafik (Scatter Plot) dar. Beispielhaft ist hier die Beziehung zwischen den Preis der Immobilie ( medv ) und der Anzahl der Zimmer ( rm ) dargestellt.","title":"Aufgabe 1"},{"location":"RegressionsAnalyse/#lineare-regression_1","text":"Die lineare Regression versucht nun durch diese Datenmenge eine gerade zu legen, um auf diese Weise Vorhersagen von zuk\u00fcnftigen Ereignissen zu bestimmen. Wenn dieses gelingt, dann k\u00f6nnte z.B. ein Immobilienpreis anhand der Gr\u00f6\u00dfe der Wohnung bestimmt werden. Weiter Informationen: Studifix: Lineare Regression Eine lineare Gleichung hat dabei folgende allgemeine Form. \\[f(x) = a \\cdot x + b\\] Wobei a die Steigung und b der y-Achsenabschnitt der Geraden ist. Gesucht ist also eine L\u00f6sung mit den Werten a und b , die durch die gew\u00e4hlte Datenmenge eine Gerade repr\u00e4sentiert, wobei der Fehlern (also die Abweichung des Gerade zu den einzelnen Datenpunkten) zu minimieren ist. Die L\u00f6sung des Problem kann dabei iterativ als auch arithmetisch erfolgen. Eine arithmetische L\u00f6sung sieht dabei wie folgt aus. \\(a = \\frac{n\\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n\\sum_{i=1}^n x_i^2 - (\\sum_{i=1}^n x_i)^2}\\) \\(b = \\frac{\\sum_{i=1}^n y_i - a\\sum_{i=1}^n x_i}{n}\\) Dabei sind \\(x_i\\) f\u00fcr das o.g. Beispiel die Werte f\u00fcr die Anzahl der Zimmer pro Wohneinheit ( rm ) und \\(y_i\\) der daraus resultierende Preis der Wohnung.","title":"lineare Regression"},{"location":"RegressionsAnalyse/#ubungsaufgabe-2","text":"Gehen Sie von folgenden Daten aus: x = [ 4 , 6 , 8 , 10 , 12 ] y = [ 10 , 20 , 24 , 32 , 45 ] Wenn Sie diese Daten visualisieren erhalten Sie folgenden Darstellung. Berechnen Sie mit Hilfe der o.g. Formel die Geradengleichung \\(f(x) = a \\cdot x + b\\) . Hinweis : Nutzen Sie zur Berechnung und ggf. auch zur Visualisierung die zur Verf\u00fcgung gestellte Excel Tabelle lrg_ueb1.xls","title":"\u00dcbungsaufgabe 2"},{"location":"RegressionsAnalyse/#ubungsaufgabe-3","text":"\u00dcberpr\u00fcfen Sie ihre berechneten Werte, indem Sie die Gerade \\(f(x) = a \\cdot x + b\\) in die Datenmenge einzeichen. Z.B. mit Hilfe des folgenden Python Codes. a = 2 # Ihr berechneter Wert f\u00fcr a b = 4 # Ihr berechneter Wert f\u00fcr b x = [ 4 , 6 , 8 , 10 , 12 ] y = [ 10 , 20 , 24 , 32 , 45 ] gx = [ 4 , 12 ] gy = [ a * 4 + b , a * 12 + b ] plt . plot ( gx , gy , color = \"red\" ) plt . scatter ( x , y ) plt . show () Das Berechnen der Steigung a und des Achsenabschnitts b ist nat\u00fcrlich auch mit Hilfe einer Methode aus der Bibliothek sklearn.linear_model konkret die Klasse LinearRegression . Importieren Sie also zun\u00e4chst die entsprechende Klasse: from sklearn.linear_model import LinearRegression import numpy as np # Gegebene Daten x = np . array ([ 4 , 6 , 8 , 10 , 12 ]) . reshape ( - 1 , 1 ) y = np . array ([ 10 , 20 , 24 , 32 , 45 ]) # Erstellung und Anpassung des linearen Regressionsmodells model = LinearRegression () model . fit ( x , y ) # Koeffizienten der Regressionsgeraden slope = model . coef_ [ 0 ] intercept = model . intercept_ \u00dcber die Methode fit() dieser Klasse k\u00f6nnen die Parameter a und b bestimmt werden. Diese befinden sich nach korrektem Aufruf der Methode im Attribut coef_[0] (f\u00fcr die Steigung a ) und intercept_ (f\u00fcr den Achsenabschnitt b ). \u00dcberpr\u00fcfen Sie mit Hilfe der Methode fit() ihre zuvor berechneten Werte und tragen Sie die Steigungsgerade in den Scatter Plot ein. \u00dcber die Methode predict() des Modells k\u00f6nnen Sie nun anhand des Modells Aussagen \u00fcber das Verhalten des Systems machen. Testen Sie diese indem Sie sich z.B. eine Aussage \u00fcber den Wert von \\(x=7\\) ausgeben lassen!","title":"\u00dcbungsaufgabe 3"},{"location":"RegressionsAnalyse/#aufgabe-2","text":"Bestimmen Sie die aus Aufgabe 1 ermittelten Korrelationen ( Features ) die Gerade \\(f(x) = a \\cdot x + b\\) und tragen Sie diese wie in der \u00dcbung in die Datenmenge ein. Erstellen Sie ferner mittels der Methode predict() des Modells eine Aussage, indem Sie den Wert des Features \u00fcbergeben und den Immobilienpreis angezeigt bekommen! Diskutieren Sie im Klassenverband, ob das nun erstellte Vorhersagemodell zum Prognostizieren des Hauspreises ausreichend ist, oder wie dieses verbessert werden kann?","title":"Aufgabe 2"},{"location":"RegressionsAnalyse/#anpassen-des-modells-fur-mehrere-features-multiple-lineare-regression","text":"Zur Optimierung des Modells sollen auch die anderen Features des Datensatzes genutzt werden. Dieses Verfahren nennt sich Multiple Lineare Regression. Gesucht wird dabei folgende Gleichung. \\[f(x) = a_1 \\cdot x_1 + a_2 \\cdot x_2+a_3 \\cdot x_3 + b\\] Dabei sind \\(a_1\\) bis \\(a_n\\) Faktoren f\u00fcr die einzelnen Features \\(x_1\\) bis \\(x_n\\) . Diese einzelnen Faktoren gilt es nun iterativ zu bestimmen. Weitere Informationen: Studifix: Multiple Lineare Regression Zur \u00dcberpr\u00fcfung des daraus entstandene Vorhersagemodells muss nun der Datensatz zun\u00e4chst in Trainingsdaten und Testdaten geteilt werden. Eine Teilung von 80/20 ist hierf\u00fcr ein g\u00e4ngiges Mittel. Zum Aufteilen ein Datensatzes in Trainings- / Testdaten dient auch wieder eine Methode aus dem Paket sklearn.model_selection , konkret die Methode train_test_split() die dieser Aufteilung vornimmt. from sklearn.model_selection import train_test_split feature_names = [ \"rm\" , \"b\" , \"dis\" ] X_train , X_test , y_train , y_test = train_test_split ( df [ feature_names ], df [ 'medv' ], test_size = 0.2 , random_state = 0 )","title":"Anpassen des Modells f\u00fcr mehrere Features (Multiple Lineare Regression)"},{"location":"RegressionsAnalyse/#trainieren-des-modells","text":"Anschlie\u00dfend kann das Modell mit den Trainingsdaten trainiert werden. model = LinearRegression () model . fit ( X_train , y_train ) Die Einzelnen Faktoren \\(a_1\\) bis \\(a_n\\) befinden sich dabei wie zuvor in dem Attribut model.coeff_ in Form eines Arrays und model.intercept der Wert f\u00fcr den Achsenabschnitt b . Um nun aufgrund unseres Modells eine Vorhersage zu treffen, dient die Funktion predict im Objekt model . Dieses erwartet einen Datensatz als \u00dcbergabeparameter.","title":"Trainieren des Modells"},{"location":"RegressionsAnalyse/#r2-metrik","text":"\u00dcber die Qualit\u00e4t des Modells gibt u.a. Der Wert \\(R^2\\) (Determinationskoeffizient) Auskunft. Je h\u00f6her der \\(R^2\\) -Wert, desto besser die Vorhersage. from sklearn.metrics import r2_score y_pred = model . predict ( X_test ) r2 = r2_score ( y_test , y_pred ) print ( 'R^2: ' , r2 )","title":"R2 - Metrik"},{"location":"RegressionsAnalyse/#aufgabe-3","text":"F\u00fchren Sie wie beschrieben die Regressions Analyse mit 3-4 wichtigen Features mit dem Datensatz durch. W\u00e4hlen Sie dann einen Datensatz aus der Menge der Testdaten und lassen sich mit der Funktion predict() das Ergebnis f\u00fcr einen Immobilienpreis ausgeben. \u00dcberpr\u00fcfen Sie ferner die Qualit\u00e4t ihres Vorhersagemodells, indem Sie den Wertt \\(R^2\\) bestimmen. Diskutieren Sie Ma\u00dfnahmen, wie die Qualit\u00e4t des Vorhersagemodells verbessert werden kann!","title":"Aufgabe 3"},{"location":"RegressionsAnalyse/#aufgabe-4","text":"Entscheiden Sie sich in einer Kleingruppe f\u00fcr einen geeigneten Datensatz zur Durchf\u00fchrung einer Regressionsanalyse und f\u00fchren Sie die Analyse durch. Erstellen Sie auf der Grundlage ihrer Analyse eine kurze Pr\u00e4sentation und Pr\u00e4sentieren diese anschlie\u00dfend kurz der Klasse ihr Ergebnis. Die Pr\u00e4sentation sollte beinhalten: Art / Beschreibung des Datensatzes Zu untersuchende Fragestellung Vorgehensweise Interpretation der Ergebnisse Folgende Datens\u00e4tze k\u00f6nnen z.B. genutzt werden: El Nino Dataset : Oceanographic and surface meteorological readings taken from a series of buoys positioned throughout the equatorial Pacific. Download / Beschreibung: https://www.kaggle.com/datasets/uciml/el-nino-dataset eBay auction data : Auction data from various eBay.com objects over various length auctions. Download / Beschreibung: https://www.kaggle.com/code/yingyingchen/ebay-auction-data-analysis/data Bike Sharing Dataset : Hourly and daily count of rental bikes in a large city. Download / Beschreibung: https://www.kaggle.com/datasets/hmavrodiev/london-bike-sharing-dataset Oder w\u00e4hlen Sie einen eigenen Datensatz z.B. aus https://www.kaggle.com/ !","title":"Aufgabe 4"},{"location":"RegressionsAnalyse/#reflexion-der-unterrichtseinheit","text":"Sie erhalten von ihrem Gesch\u00e4ftsf\u00fchrer Dr. W\u00f6hler die unten abgebildete Email. Verfassen Sie auf diese Email eine Antwort. \"Sehr geehrte Mitarbeiter der Abteilung Daten- und Prozessanalyse, ich gratuliere Ihnen von ganzem Herzen zu Ihrer erfolgreichen Fertigstellung des Modells zur Vorhersage von Immobilienpreisen. Alle Anstrengungen, die Sie auf sich genommen haben, um dieses Modell zum Laufen zu bringen, werden nun mit dem Ergebnis belohnt. Ausserdem m\u00f6chte ich betonen, dass wir in Zukunft alle Anforderungen f\u00fcr ein Modell datenbasierter Vorhersagen mit den Mitteln der linearen Regression l\u00f6sen sollten. Der Grund ist, dass lineare Regression uns die gr\u00f6\u00dftm\u00f6gliche Genauigkeit beim Entwerfen oder Verbessern von Modellen gew\u00e4hrleistet. Noch einmal herzlichen Gl\u00fcckwunsch an alle, die an diesem Projekt beteiligt waren. Bleiben Sie alle weiterhin motiviert, kreativ und innovativ. Vielen Dank, Dr. W\u00f6hler, Gesch\u00e4ftsf\u00fchrer ChangeIT GmbH\"","title":"Reflexion der Unterrichtseinheit"},{"location":"ReinforcedLearning/","text":"Reinforced Learning Handlungssituation Ein gro\u00dfer deutscher internationaler Automobilkonzern, plant die Einf\u00fchrung einer autonomen Taxiflotte. Dabei sollen Fahrg\u00e4ste ein fest definierten Stationen die M\u00f6glichkeit haben eine Fahrt zu buchen und eine weitere Station als Zielort anzugeben. Der Chefentwickler der Abteilung Daten- und Prozessanalyse der ChangeIT GmbH schl\u00e4gt vor dieses Problem mit Hilfe des verst\u00e4rkenden Lernens zu l\u00f6sen. Die Kollegen der Anwendungsentwicklung haben dazu bereits eine Simulationsumgebung geschaffen. Ihre Aufgabe wird es sein, einen Lernalgorithmus zu entwickeln, der in dieser Simulationsumgebung Fahrg\u00e4ste aufnimmt und optimal zu ihrem Ziel bef\u00f6rdert. Erkl\u00e4rung Reinforced Learning Reinforcement Learning ist eine Methode im Bereich des maschinellen Lernens, bei der ein Agent seine Strategien lernt, indem er seine Umgebung erkundet und durch Interaktion mit dieser belohnt oder bestraft wird. Das Ziel des Agenten ist es, durch wiederholte Interaktionen die Aktionen zu finden, die ihn am meisten belohnen. Ein Beispiel f\u00fcr Reinforcement Learning k\u00f6nnte ein autonomes Fahrzeug sein, das lernen soll, wie es auf einer Stra\u00dfe sicher navigieren kann. Die Umgebung des Agenten besteht aus den Stra\u00dfenzust\u00e4nden, dem Verkehr und anderen Objekten auf der Stra\u00dfe. Der Agent arbeitet hierbei mit einem State-Action-Reward-State-Modell (SARS-Modell). Der Agent beginnt damit, zuf\u00e4llige Aktionen auszuf\u00fchren, w\u00e4hrend er die Umgebung erkundet. Der Zustand des Agenten \u00e4ndert sich abh\u00e4ngig von seinen Aktionen und der Umgebung. Wenn das Auto beispielsweise bremst, weil ein Hindernis auf der Stra\u00dfe auftaucht, wird der Zustand des Autos ver\u00e4ndert. F\u00fcr jede Aktion, die der Agent ausf\u00fchrt, erh\u00e4lt er einen Belohnungswert oder ein Bestrafungssignal, je nachdem, ob die Aktion erfolgreich war oder nicht. Der Agent verwendet die gesammelten Zustandsinformationen, um die besten Aktionen in \u00e4hnlichen Situationen zu identifizieren. Durch erneute Iteration verbessert der Agent schlie\u00dflich seine F\u00e4higkeiten und trifft bessere Entscheidungen in Zukunft. Das SARS-Modell ist eine grundlegende Komponente von Reinforcement Learning. Der Zustand (State) zeigt an, in welchem Zustand das System sich gerade befindet. Die Aktion (Action) ist die Handlung, die der Agent ausf\u00fchrt. Die Belohnung (Reward) wird dem Agenten je nach dem Ergebnis seiner Aktion gegeben. Der Zustand in der Folgezeit zeigt an, wie sich die Umgebung aufgrund der ausgew\u00e4hlten Aktion ver\u00e4ndert hat. Durch die Verwendung des SARS-Modells und die st\u00e4ndige Interaktion mit der Umgebung kann ein Reinforcement Learning-Agent schlie\u00dflich lernen, die besten Entscheidungen zu treffen, um seine Ziele auf effektive Weise zu erreichen. Die Simulationsumgebung Installation der notwendigen Pakete Zun\u00e4chst m\u00fcssen wir die Simulationsumgebung installieren. Am besten nutzt man dazu eine Virtuelle Umgebung. Anschlie\u00dfend k\u00f6nnen die notwendigen Pakete installiert werden. pip install gym==0.26.2 pip install pygame GGf. muss anschlie\u00dfen der Kernel noch einmal neu gestartet werden! Falls Sie Conda installiert haben sollte Sie zuvor Conda via 'conda deaktivate' deaktivieren. Starten der Simulationsumgebung F\u00fchren Sie dann im Anschluss daran den folgenden Python Code in einer Zelle eines Juypter Notebooks aus. import gym env = gym . make ( \"Taxi-v3\" , render_mode = \"ansi\" ) state = env . reset () print ( \"State:\" , state ) print ( env . render ()) Es sollte dabei folgende Ausgabe erscheinen: Der gelb markierte Cursor entspricht dem Taxi. Es gibt vier Stationen (R)ed, (G)reen ,(B)lue und (Y)ellow. Ein Passagier m\u00f6chte dabei von der blau markierten Station abgeholt werden und zur magenta markierten Station gebracht werden. Um diesen Auftrag zu bew\u00e4ltigen stehen dem Agenten folgende Aktionen zur Verf\u00fcgung. Aktion Wert South 0 North 1 East 2 West 3 Pick Passagener 4 Drop Passanger 5 Vgl. Open AI Gym taxi Env Wie jedes Environment stellt auch das Taxi Environment f\u00fcr das Training mittels reinforced Learning einen Zustand zur Verf\u00fcgung in dem sich die Umgebung gerade befindet. State : Der Zustand in dem sich die Umgebung befindet. Nach dem oberen Bild befindet sich die Umgebung im Zustand 182. Unsere Umgebung besteht aus 5x5 Feldern. Zus\u00e4tzlich gibt es 4 Positionen der Stationen. Der Passagier kann dabei an einem der Positionen sein, oder bereits im Taxi (4+1), daher haben wird insgesamt 500 unterschiedliche Zust\u00e4nde in der Umgebung ( \\(5*5*4*(4+1)\\) )! Jeder dieser Werte beschreibt genau die Situation in unserer Umgebung. \u00dcber env.s kann die Umgebung in einen gezielten Zustand gebracht werden. Setzen eines Zustandes Erweitern Sie ihr Programm in der Weise, dass der oben dargestellte Zustand 182 eingenommen wird! Ausf\u00fchren von Aktionen F\u00fchren Sie mit Hilfe der Methode step(int) eine Aktion durch. Die Methode gibt dabei einen Vektor zur\u00fcck der aus folgenden Elemente besteht. next_state , reward , terminated , truncated , info = env . step ( int ) next_state : Der Zustand in der sich die Umgebung befindet, wenn die Aktion ausgef\u00fchrt wurde. reward : Belohnung die es f\u00fcr die Aktion gab terminated : Die Episode ist zu Ende, weil das Ziel erreicht wurde. truncated Die Episode wurde abgebrochen, z.\u202fB. weil die maximale Zeit abgelaufen ist. info Debugging Informationen! Lassen Sie sich die Ergebnisse ihre Aktion auf der Console ausgeben. Die M\u00f6glichkeiten (Possibilities) der Aktionen Neben diesen Werten gibt uns die Umgebung noch die M\u00f6glichkeit die weiteren M\u00f6glichkeiten der Folgeaktionen zu untersuchen. Setzten Sie dazu die Umgebung wieder in den Zustand 182 und lassen Sie sich das P Array ausgeben. Dieses Array hat das Format [Action] [State,reward,done]. Wie wir sehen f\u00fchrt eine Action 1 - North in den Zustands 82. Es gibt einen \" reward von -1. Das Ausf\u00fchren der Action 4 - Pick Passanger verweilt im Zustand 182 und wird 'bestraft' mit einem reward von -10, denn im Zustand 182 befindet sich kein Passagier an der Stelle um ihn aufzunehmen. Weiteres erkunden der Umgebung Nachdem Sie nun die wichtigsten Parameter der Umgebung kennen gelernt haben erkunden Sie ein wenig die Umgebung und beobachten Sie die Parameter reward . F\u00fchren Sie folgende Zelle in einem Jupyter Notebook aus, nachdem die Umgebung initialisiert wurde. \u00dcber die dargestellten Schaltfl\u00e4chen k\u00f6nnen Sie die Aktionen durchf\u00fchren. Transportieren Sie einen Passagier von Startpunkt zum Zielpunkt ! Zuvor muss jedoch noch das Paket ipywidgets installiert werden: pip install ipywidgets Und hier der Python Code import ipywidgets as widgets from IPython.display import clear_output # Funktion, die ausgef\u00fchrt wird, wenn ein Button geklickt wird: def on_button_clicked ( button ): print ( f \" { button . description } wurde geklickt!\" ) clear_output () display ( buttons_hbox ) i = 0 if button == button1 : i = 1 if button == button2 : i = 0 if button == button3 : i = 3 if button == button4 : i = 2 if button == button5 : i = 4 if button == button6 : i = 5 next_state , reward , done , info = env . step ( int ( i )) env . render () print ( \"State:\" , next_state ) print ( \"Action:\" , i ) print ( \"Reward:\" , reward ) print ( \"Done:\" , done ) print ( \"Info:\" , info ) for key , value in env . P [ next_state ] . items (): print ( \"Action \" , key , \": state \" , value [ 0 ][ 1 ], \" reward: \" , value [ 0 ][ 2 ], \" Solved:\" , value [ 0 ][ 3 ]) # Erstelle die beiden Buttons: button1 = widgets . Button ( description = \"Up\" ) button2 = widgets . Button ( description = \"Down\" ) button3 = widgets . Button ( description = \"Left\" ) button4 = widgets . Button ( description = \"Right\" ) button5 = widgets . Button ( description = \"Pick\" ) button6 = widgets . Button ( description = \"Drop\" ) # Weise die Callback-Funktion jedem Button zu: button1 . on_click ( lambda b : on_button_clicked ( b )) button2 . on_click ( lambda b : on_button_clicked ( b )) button3 . on_click ( lambda b : on_button_clicked ( b )) button4 . on_click ( lambda b : on_button_clicked ( b )) button5 . on_click ( lambda b : on_button_clicked ( b )) button6 . on_click ( lambda b : on_button_clicked ( b )) # Gruppiere die Buttons mit HBox: buttons_hbox = widgets . HBox ([ button1 , button2 , button3 , button4 , button5 , button6 ]) # Zeige die Gruppierung an: display ( buttons_hbox ) env . render () Brute Force Ansatz Schreiben Sie nun ein Programm, welches die Aufgabe (den Transport eines Passagiers vom Startpunkt zum Zielpunkt) mittels eines Brute Force Ansatzes l\u00f6st und lassen Sie sich ausgeben wie viele Z\u00fcge dazu notwendig waren! L\u00f6sung Brute Force Ansatz done = False while not done : action = env . action_space . sample () state , reward , done , info = env . step ( action ) print ( \"Timesteps taken: {} \" . format ( epochs )) Q-Learning Algorithmus Der Brute-Force Ansatz liefert unterschiedliche und unakzeptable Werte f\u00fcr das Erledigen eines Auftrages. Der Grund darin ist, dass wir uns nicht den Erfolg einer Aktion in der Umgebung merken. Um dieses 'Ged\u00e4chnis' zu erstellen ben\u00f6tigen wir ein Array, welches den Erfolg einer Aktion in Abh\u00e4ngigkeit vom Zustand speichert. Das Array h\u00e4tte folgendes Aussehen und wir zun\u00e4chst einmal mit 0-Werten initialisiert. state South North East West Pick Drop 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ... 0 0 0 0 0 0 499 0 0 0 0 0 0 Beim Lernen wird es nun wichtig sein, die Summe des reqrds zu maximieren innerhalb einer Epoche . Eine Epoche ist dabei die Anzahl der Schritte bis zum Abliefern des Passagiers. Unser Q-Learning Algorithmus hat dabei folgende Funktion: \\[Q(_{state,aktion})=(1-\\alpha)*Q(_{state,aktion})+\\alpha*[reward+\\gamma*max(Q_{next-state, all -actions})]\\] \\(Q(_{state,aktion})\\) : der erwartete Nutzen (engl. \"expected utility\") bei Auswahl der Aktion 'action' im Zustand 'state' \\(\\alpha\\) : der Lernratenparameter (engl. \"learning rate parameter\"), der bestimmt, inwieweit neue Informationen den bisherigen Q-Wert beeinflussen sollen reward: die Belohnung (engl. \"reward\") nach der Wahl der Aktion 'action' im Zustand 'state' \\(\\gamma\\) : der Abschlagfaktor (engl. \"discount factor\"), der bestimmt, wie wichtig zuk\u00fcnftige Belohnungen im Vergleich zu aktuellen Belohnungen sind \\(max(Q_{next-state, all -actions})\\) : der maximale erwartete Nutzen, den man erh\u00e4lt, wenn man eine Aktion 'action' im n\u00e4chsten Zustand 'next state' w\u00e4hlt. Wir wir sehen, ist dabei, dass das bisher Gelernte den gr\u00f6\u00dften Einfluss hat \\((1 - \\alpha) * Q(state,action)\\) und der Zugewinn geht mit dem Lernratenparameter \\(\\alpha\\) ein. Die Implementierung des Algorithmus in Python kann wie folgt aussehen: import numpy as np import random from IPython.display import clear_output q_table = np . zeros ([ env . observation_space . n , env . action_space . n ]) # Hyperparameters alpha = 0.1 gamma = 0.6 for i in range ( 1 , 100001 ): state = env . reset () epochs , reward , = 0 , 0 , done = False while not done : action = np . argmax ( q_table [ state ]) # Exploit learned values next_state , reward , done , info = env . step ( action ) old_value = q_table [ state , action ] next_max = np . max ( q_table [ next_state ]) new_value = ( 1 - alpha ) * old_value + alpha * ( reward + gamma * next_max ) q_table [ state , action ] = new_value state = next_state epochs += 1 if i % 100 == 0 : clear_output ( wait = True ) print ( f \"Episode: { i } \" ) print ( \"Training finished. \\n \" ) Aufgabe : Trainieren Sie das Modell und lassen Sie sich nach dem Training die Q-Tabelle f\u00fcr den Zustand 182 ausgeben und entscheiden Sie daran, welche Aktion in diesem Zustand den meisten Erfolg bringt. L\u00f6sung : Die Q-Table im Zustands 182 zieht wie folgt aus: Der maximale Wert ist die -2.4510224, d.h. die erfolgreichsten Aktionen in diesem Zustands ist entweder eine Bewegung nach S\u00fcden oder nach Osten! Aufgabe : Entwickeln Sie nach erfolgreichem Training ein Programm, welches Ihnen Aussagen \u00fcber die Qualit\u00e4t des Modells erlaubt. Ver\u00e4ndern Sie ferner wichtige Parameter im Algorithmus und beobachten Sie, wie dieses sich auf die Qualit\u00e4t des Modells auswirken. L\u00f6sung : Hier ein Programm, welches 10 Episoden durchf\u00fchrt und die Anzahl der notwendigen Schritte und ggf. Fehlversuche z\u00e4hlt und visualisiert. penalties , reward = 0 , 0 frames = [] # for animation for i in range ( 1 , 10 ): env . reset () epochs = 0 done = False while not done : action = np . argmax ( q_table [ state ]) state , reward , done , info = env . step ( action ) if reward == - 10 : penalties += 1 # Put each rendered frame into dict for animation frames . append ({ 'frame' : env . render ( mode = 'ansi' ), 'state' : state , 'action' : action , 'reward' : reward , 'done' : done } ) epochs += 1 print ( \"Timesteps taken: {} \" . format ( epochs )) print ( \"Penalties incurred: {} \" . format ( penalties )) Und hier eine einfache Form der Visualisierung: def print_frames ( frames ): j = 0 for i , frame in enumerate ( frames ): clear_output ( wait = True ) env . s = frame [ 'state' ] j = j + 1 print ( f \"Timestep: { j } \" ) print ( f \"State: { frame [ 'state' ] } \" ) print ( f \"Action: { frame [ 'action' ] } \" ) print ( f \"Reward: { frame [ 'reward' ] } \" ) if frame [ 'done' ] == True : j = 0 env . render () sleep ( .5 ) print_frames ( frames ) Aufgabe : Wechseln Sie das Environment auf ein anderes Environment. Z.B. Das FrozenLake Environment. https://www.gymlibrary.dev/environments/toy_text/frozen_lake/ . Das Cliff Walking Environmment https://www.gymlibrary.dev/environments/toy_text/cliff_walking/ Das Mountain Car Einvorinment https://www.gymlibrary.dev/environments/classic_control/mountain_car/ Das CartPole Environment https://www.gymlibrary.dev/environments/classic_control/cart_pole/ Hinweis : Achtung das Mountain Car Environment und das CartPole Environment sind kontinuierliche Umgebungen, d.h. der Zustandsraum kann beliebige Werte annehmen. Daher muss hier der Zustandsraum erst diskretisiert werden! Gehen Sie im weiteren Verlauf wie folgt vor: Untersuchen Sie dabei zun\u00e4chst das Environment, welche Aktionen gibt es, bestimmen Sie den Action Space und den Observation Space. Versuchen Sie einen Brute Force Ansatz Trainieren Sie mit Hilfe des Q-learning Algorithmus ein Modell und beurteilen Sie dessen Qualit\u00e4t Dokumentieren und pr\u00e4sentieren Sie anschlie\u00dfend ihr Vorgehen Musterl\u00f6sung f\u00fcr CartPole Das CartPole Environment besteht aus einem Schlitten, der nach rechts und links bewegt werden kann. Auf dem Schlitten ist eine Pendel installiert. Ziel ist es das Pendel in der aufrechten Position zu behalten. Initialisierung des Environments import gymnasium as gym import numpy as np import math import matplotlib.pyplot as plt # Erstelle die Umgebung env = gym . make ( 'CartPole-v1' , render_mode = \"rgb_array\" ) state = env . reset () print ( \"State=\" + str ( state [ 0 ])) plt . imshow ( env . render ()) plt . grid ( False ) Der Sate ist ein Array aus kontinuierlichen Float Werten, mit folgenden Bedeutungen: Index Bedeutung 0 Cart Position (-4.8 bis +4.8) 1 Cart velocity (+/- unendlich) 2 Pole Angle (- 0.418 bis + 0.418) 3 Pole Angular Velocity (+/- unendlich) Dieser kontinuierliche Zustandsraum muss in einen diskreten umgewandelt werden. Ich entschied mich daher jeden dieser Werte in 10 diskrete Werte umzuwandeln. Der Zustandsraum hat damit \\(10*10*10*10=10000\\) Werte. n_actions = 2 n_states = 10 * 10 * 10 * 10 # Festgelegt q_table = np . zeros ([ n_states , n_actions ]) q_table Die Zuordnung eines States zu einem Index-Wert in diesem Array \u00fcbernimmt die Funktion discret(state):int ! cpos = np . array ( np . linspace ( - 4.8 , 4.8 , 10 )) cvelocity = np . array ( np . linspace ( - 0.5 , 0.5 , 10 )) cpolea = np . array ( np . linspace ( - 0.418 , 0.418 , 10 )) cpolev = np . array ( np . linspace ( - 0.5 , 0.5 , 10 )) def discret ( s ): dsp = np . abs ( cpos - s [ 0 ]) . argmin () dsv = np . abs ( cvelocity - s [ 1 ]) . argmin () dspa = np . abs ( cpolea - s [ 2 ]) . argmin () dspv = np . abs ( cpolev - s [ 3 ]) . argmin () return dsp + dsv + dspa + dspv nr = discret ( state [ 0 ]) nr Nun kann der Q-Leaning Algorithmus implementiert werden! total_episodes = 50000 learning_rate = 0.8 max_steps = 100 gamma = 0.95 epsilon = 1.0 for episode in range ( total_episodes ): state = env . reset ()[ 0 ] done = False for step in range ( max_steps ): if np . random . uniform ( 0 , 1 ) > epsilon : action = np . argmax ( q_table [ discret ( state )]) else : action = env . action_space . sample () new_state , reward , done , info , a = env . step ( action ) if done and step < max_steps - 1 : reward = - 300 # negative reward for falling q_table [ discret ( state )][ action ] = ( q_table [ discret ( state )][ action ] + learning_rate * ( reward + gamma * np . max ( q_table [ discret ( new_state )]) - q_table [ discret ( state )][ action ])) state = new_state if done : break if episode % 100 == 0 : print ( \"Episode:\" + str ( episode )) epsilon = min_epsilon + ( max_epsilon - min_epsilon ) * np . exp ( - decay_rate * episode ) Nach dem Training kann mit der Erfolg am Modell gepr\u00fcft werden. from IPython import display import matplotlib import matplotlib.pyplot as plt % matplotlib inline state = env . reset ()[ 0 ] img = plt . imshow ( env . render ()) # only call this once done = False while not done : action = np . argmax ( q_table [ discret ( state )]) new_state , reward , done , info , a = env . step ( action ) img . set_data ( env . render ()) # just update the data display . display ( plt . gcf ()) display . clear_output ( wait = True ) state = new_state env . close () Fragen zum Verst\u00e4ndnis Welches der folgenden Aussagen ist eine korrekte Beschreibung von Reinforcement Learning? [ ] A. Ein \u00fcberwachtes Lernen, bei dem die Modelle mit Hilfe von explizit beschrifteten Trainingsdaten trainiert werden. [ ] B. Ein un\u00fcberwachtes Lernen, bei dem die Modelle Muster in Daten entdecken, ohne dass menschliche Anleitung erforderlich ist. [ ] C. Ein Art von Lernen, bei dem ein Agent seine Handlungen basierend auf Belohnungen und Strafen optimiert. [ ] D. Ein semi-\u00fcberwachtes Lernen, das eine Mischung aus beschrifteten und unbeschrifteten Daten verwendet. Was ist ein wesentliches Merkmal des Q-Learning-Algorithmus im Reinforcement Learning? [ ] A. Q-Learning verwendet ein Modell der Umgebung, um die beste n\u00e4chste Aktion vorherzusagen. [ ] B. Q-Learning aktualisiert seine Q-Werte basierend auf der Differenz zwischen der erwarteten und der tats\u00e4chlichen Belohnung nach jeder Aktion (auch bekannt als TD-Fehler). [ ] C. Q-Learning erfordert eine vollst\u00e4ndige Kenntnis aller m\u00f6glichen Zust\u00e4nde und Aktionen vor Beginn des Lernprozesses. [ ] D. Q-Learning kann nur bei diskreten Zustands- und Handlungsr\u00e4umen angewendet werden. Was ist \"Deep Reinforcement Learning\"? [ ] A. Ein Ansatz f\u00fcr Reinforcement Learning, der genetische Algorithmen verwendet. [ ] B. Ein Ansatz f\u00fcr Reinforcement Learning, der neuronale Netzwerke verwendet, um Q- oder Value-Funktionen zu approximieren. [ ] C. Ein Ansatz f\u00fcr Reinforcement Learning, der tiefe neuronale Netzwerke verwendet, um die Umgebung des Agenten zu modellieren. [ ] D. Ein Ansatz f\u00fcr Reinforcement Learning, der sich ausschlie\u00dflich auf theoretische Forschung konzentriert und nicht in der Praxis angewendet wird. Welche Aussage \u00fcber Q-Learning ist korrekt? [ ] A. Q-Learning ist eine Art von \u00fcberwachtem Lernen. [ ] B. Q-Learning ist ein modellfreier Reinforcement Learning Algorithmus. [ ] C. Q-Learning erfordert ein vollst\u00e4ndiges Modell der Umgebung, um effektiv zu sein. [ ] D. Q-Learning kann nur in kontinuierlichen Zustandsr\u00e4umen verwendet werden. Was bedeutet Exploration vs Exploitation in Reinforcement Learning? [ ] A. Exploration bezieht sich auf das Erlernen von neuen Strategien, w\u00e4hrend Exploitation das Anwenden dieser Strategien bezeichnet. [ ] B. Exploration bezieht sich auf das Sammeln von Daten, w\u00e4hrend Exploitation die Analyse dieser Daten bezeichnet. [ ] C. Exploration bezieht sich auf das Testen unbekannter Aktionen, w\u00e4hrend Exploitation das Nutzen von bekanntem Wissen zur Maximierung der Belohnung bezeichnet. [ ] D. Exploration bezieht sich auf das Experimentieren mit verschiedenen Modellen, w\u00e4hrend Exploitation das Trainieren eines bestimmten Modells bezeichnet.","title":"Reinforced Learning"},{"location":"ReinforcedLearning/#reinforced-learning","text":"","title":"Reinforced Learning"},{"location":"ReinforcedLearning/#handlungssituation","text":"Ein gro\u00dfer deutscher internationaler Automobilkonzern, plant die Einf\u00fchrung einer autonomen Taxiflotte. Dabei sollen Fahrg\u00e4ste ein fest definierten Stationen die M\u00f6glichkeit haben eine Fahrt zu buchen und eine weitere Station als Zielort anzugeben. Der Chefentwickler der Abteilung Daten- und Prozessanalyse der ChangeIT GmbH schl\u00e4gt vor dieses Problem mit Hilfe des verst\u00e4rkenden Lernens zu l\u00f6sen. Die Kollegen der Anwendungsentwicklung haben dazu bereits eine Simulationsumgebung geschaffen. Ihre Aufgabe wird es sein, einen Lernalgorithmus zu entwickeln, der in dieser Simulationsumgebung Fahrg\u00e4ste aufnimmt und optimal zu ihrem Ziel bef\u00f6rdert.","title":"Handlungssituation"},{"location":"ReinforcedLearning/#erklarung-reinforced-learning","text":"Reinforcement Learning ist eine Methode im Bereich des maschinellen Lernens, bei der ein Agent seine Strategien lernt, indem er seine Umgebung erkundet und durch Interaktion mit dieser belohnt oder bestraft wird. Das Ziel des Agenten ist es, durch wiederholte Interaktionen die Aktionen zu finden, die ihn am meisten belohnen. Ein Beispiel f\u00fcr Reinforcement Learning k\u00f6nnte ein autonomes Fahrzeug sein, das lernen soll, wie es auf einer Stra\u00dfe sicher navigieren kann. Die Umgebung des Agenten besteht aus den Stra\u00dfenzust\u00e4nden, dem Verkehr und anderen Objekten auf der Stra\u00dfe. Der Agent arbeitet hierbei mit einem State-Action-Reward-State-Modell (SARS-Modell). Der Agent beginnt damit, zuf\u00e4llige Aktionen auszuf\u00fchren, w\u00e4hrend er die Umgebung erkundet. Der Zustand des Agenten \u00e4ndert sich abh\u00e4ngig von seinen Aktionen und der Umgebung. Wenn das Auto beispielsweise bremst, weil ein Hindernis auf der Stra\u00dfe auftaucht, wird der Zustand des Autos ver\u00e4ndert. F\u00fcr jede Aktion, die der Agent ausf\u00fchrt, erh\u00e4lt er einen Belohnungswert oder ein Bestrafungssignal, je nachdem, ob die Aktion erfolgreich war oder nicht. Der Agent verwendet die gesammelten Zustandsinformationen, um die besten Aktionen in \u00e4hnlichen Situationen zu identifizieren. Durch erneute Iteration verbessert der Agent schlie\u00dflich seine F\u00e4higkeiten und trifft bessere Entscheidungen in Zukunft. Das SARS-Modell ist eine grundlegende Komponente von Reinforcement Learning. Der Zustand (State) zeigt an, in welchem Zustand das System sich gerade befindet. Die Aktion (Action) ist die Handlung, die der Agent ausf\u00fchrt. Die Belohnung (Reward) wird dem Agenten je nach dem Ergebnis seiner Aktion gegeben. Der Zustand in der Folgezeit zeigt an, wie sich die Umgebung aufgrund der ausgew\u00e4hlten Aktion ver\u00e4ndert hat. Durch die Verwendung des SARS-Modells und die st\u00e4ndige Interaktion mit der Umgebung kann ein Reinforcement Learning-Agent schlie\u00dflich lernen, die besten Entscheidungen zu treffen, um seine Ziele auf effektive Weise zu erreichen.","title":"Erkl\u00e4rung Reinforced Learning"},{"location":"ReinforcedLearning/#die-simulationsumgebung","text":"","title":"Die Simulationsumgebung"},{"location":"ReinforcedLearning/#installation-der-notwendigen-pakete","text":"Zun\u00e4chst m\u00fcssen wir die Simulationsumgebung installieren. Am besten nutzt man dazu eine Virtuelle Umgebung. Anschlie\u00dfend k\u00f6nnen die notwendigen Pakete installiert werden. pip install gym==0.26.2 pip install pygame GGf. muss anschlie\u00dfen der Kernel noch einmal neu gestartet werden! Falls Sie Conda installiert haben sollte Sie zuvor Conda via 'conda deaktivate' deaktivieren.","title":"Installation der notwendigen Pakete"},{"location":"ReinforcedLearning/#starten-der-simulationsumgebung","text":"F\u00fchren Sie dann im Anschluss daran den folgenden Python Code in einer Zelle eines Juypter Notebooks aus. import gym env = gym . make ( \"Taxi-v3\" , render_mode = \"ansi\" ) state = env . reset () print ( \"State:\" , state ) print ( env . render ()) Es sollte dabei folgende Ausgabe erscheinen: Der gelb markierte Cursor entspricht dem Taxi. Es gibt vier Stationen (R)ed, (G)reen ,(B)lue und (Y)ellow. Ein Passagier m\u00f6chte dabei von der blau markierten Station abgeholt werden und zur magenta markierten Station gebracht werden. Um diesen Auftrag zu bew\u00e4ltigen stehen dem Agenten folgende Aktionen zur Verf\u00fcgung. Aktion Wert South 0 North 1 East 2 West 3 Pick Passagener 4 Drop Passanger 5 Vgl. Open AI Gym taxi Env Wie jedes Environment stellt auch das Taxi Environment f\u00fcr das Training mittels reinforced Learning einen Zustand zur Verf\u00fcgung in dem sich die Umgebung gerade befindet. State : Der Zustand in dem sich die Umgebung befindet. Nach dem oberen Bild befindet sich die Umgebung im Zustand 182. Unsere Umgebung besteht aus 5x5 Feldern. Zus\u00e4tzlich gibt es 4 Positionen der Stationen. Der Passagier kann dabei an einem der Positionen sein, oder bereits im Taxi (4+1), daher haben wird insgesamt 500 unterschiedliche Zust\u00e4nde in der Umgebung ( \\(5*5*4*(4+1)\\) )! Jeder dieser Werte beschreibt genau die Situation in unserer Umgebung. \u00dcber env.s kann die Umgebung in einen gezielten Zustand gebracht werden.","title":"Starten der Simulationsumgebung"},{"location":"ReinforcedLearning/#setzen-eines-zustandes","text":"Erweitern Sie ihr Programm in der Weise, dass der oben dargestellte Zustand 182 eingenommen wird!","title":"Setzen eines Zustandes"},{"location":"ReinforcedLearning/#ausfuhren-von-aktionen","text":"F\u00fchren Sie mit Hilfe der Methode step(int) eine Aktion durch. Die Methode gibt dabei einen Vektor zur\u00fcck der aus folgenden Elemente besteht. next_state , reward , terminated , truncated , info = env . step ( int ) next_state : Der Zustand in der sich die Umgebung befindet, wenn die Aktion ausgef\u00fchrt wurde. reward : Belohnung die es f\u00fcr die Aktion gab terminated : Die Episode ist zu Ende, weil das Ziel erreicht wurde. truncated Die Episode wurde abgebrochen, z.\u202fB. weil die maximale Zeit abgelaufen ist. info Debugging Informationen! Lassen Sie sich die Ergebnisse ihre Aktion auf der Console ausgeben.","title":"Ausf\u00fchren von Aktionen"},{"location":"ReinforcedLearning/#die-moglichkeiten-possibilities-der-aktionen","text":"Neben diesen Werten gibt uns die Umgebung noch die M\u00f6glichkeit die weiteren M\u00f6glichkeiten der Folgeaktionen zu untersuchen. Setzten Sie dazu die Umgebung wieder in den Zustand 182 und lassen Sie sich das P Array ausgeben. Dieses Array hat das Format [Action] [State,reward,done]. Wie wir sehen f\u00fchrt eine Action 1 - North in den Zustands 82. Es gibt einen \" reward von -1. Das Ausf\u00fchren der Action 4 - Pick Passanger verweilt im Zustand 182 und wird 'bestraft' mit einem reward von -10, denn im Zustand 182 befindet sich kein Passagier an der Stelle um ihn aufzunehmen.","title":"Die M\u00f6glichkeiten (Possibilities) der Aktionen"},{"location":"ReinforcedLearning/#weiteres-erkunden-der-umgebung","text":"Nachdem Sie nun die wichtigsten Parameter der Umgebung kennen gelernt haben erkunden Sie ein wenig die Umgebung und beobachten Sie die Parameter reward . F\u00fchren Sie folgende Zelle in einem Jupyter Notebook aus, nachdem die Umgebung initialisiert wurde. \u00dcber die dargestellten Schaltfl\u00e4chen k\u00f6nnen Sie die Aktionen durchf\u00fchren. Transportieren Sie einen Passagier von Startpunkt zum Zielpunkt ! Zuvor muss jedoch noch das Paket ipywidgets installiert werden: pip install ipywidgets Und hier der Python Code import ipywidgets as widgets from IPython.display import clear_output # Funktion, die ausgef\u00fchrt wird, wenn ein Button geklickt wird: def on_button_clicked ( button ): print ( f \" { button . description } wurde geklickt!\" ) clear_output () display ( buttons_hbox ) i = 0 if button == button1 : i = 1 if button == button2 : i = 0 if button == button3 : i = 3 if button == button4 : i = 2 if button == button5 : i = 4 if button == button6 : i = 5 next_state , reward , done , info = env . step ( int ( i )) env . render () print ( \"State:\" , next_state ) print ( \"Action:\" , i ) print ( \"Reward:\" , reward ) print ( \"Done:\" , done ) print ( \"Info:\" , info ) for key , value in env . P [ next_state ] . items (): print ( \"Action \" , key , \": state \" , value [ 0 ][ 1 ], \" reward: \" , value [ 0 ][ 2 ], \" Solved:\" , value [ 0 ][ 3 ]) # Erstelle die beiden Buttons: button1 = widgets . Button ( description = \"Up\" ) button2 = widgets . Button ( description = \"Down\" ) button3 = widgets . Button ( description = \"Left\" ) button4 = widgets . Button ( description = \"Right\" ) button5 = widgets . Button ( description = \"Pick\" ) button6 = widgets . Button ( description = \"Drop\" ) # Weise die Callback-Funktion jedem Button zu: button1 . on_click ( lambda b : on_button_clicked ( b )) button2 . on_click ( lambda b : on_button_clicked ( b )) button3 . on_click ( lambda b : on_button_clicked ( b )) button4 . on_click ( lambda b : on_button_clicked ( b )) button5 . on_click ( lambda b : on_button_clicked ( b )) button6 . on_click ( lambda b : on_button_clicked ( b )) # Gruppiere die Buttons mit HBox: buttons_hbox = widgets . HBox ([ button1 , button2 , button3 , button4 , button5 , button6 ]) # Zeige die Gruppierung an: display ( buttons_hbox ) env . render ()","title":"Weiteres erkunden der Umgebung"},{"location":"ReinforcedLearning/#brute-force-ansatz","text":"Schreiben Sie nun ein Programm, welches die Aufgabe (den Transport eines Passagiers vom Startpunkt zum Zielpunkt) mittels eines Brute Force Ansatzes l\u00f6st und lassen Sie sich ausgeben wie viele Z\u00fcge dazu notwendig waren!","title":"Brute Force Ansatz"},{"location":"ReinforcedLearning/#losung-brute-force-ansatz","text":"done = False while not done : action = env . action_space . sample () state , reward , done , info = env . step ( action ) print ( \"Timesteps taken: {} \" . format ( epochs ))","title":"L\u00f6sung Brute Force Ansatz"},{"location":"ReinforcedLearning/#q-learning-algorithmus","text":"Der Brute-Force Ansatz liefert unterschiedliche und unakzeptable Werte f\u00fcr das Erledigen eines Auftrages. Der Grund darin ist, dass wir uns nicht den Erfolg einer Aktion in der Umgebung merken. Um dieses 'Ged\u00e4chnis' zu erstellen ben\u00f6tigen wir ein Array, welches den Erfolg einer Aktion in Abh\u00e4ngigkeit vom Zustand speichert. Das Array h\u00e4tte folgendes Aussehen und wir zun\u00e4chst einmal mit 0-Werten initialisiert. state South North East West Pick Drop 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ... 0 0 0 0 0 0 499 0 0 0 0 0 0 Beim Lernen wird es nun wichtig sein, die Summe des reqrds zu maximieren innerhalb einer Epoche . Eine Epoche ist dabei die Anzahl der Schritte bis zum Abliefern des Passagiers. Unser Q-Learning Algorithmus hat dabei folgende Funktion: \\[Q(_{state,aktion})=(1-\\alpha)*Q(_{state,aktion})+\\alpha*[reward+\\gamma*max(Q_{next-state, all -actions})]\\] \\(Q(_{state,aktion})\\) : der erwartete Nutzen (engl. \"expected utility\") bei Auswahl der Aktion 'action' im Zustand 'state' \\(\\alpha\\) : der Lernratenparameter (engl. \"learning rate parameter\"), der bestimmt, inwieweit neue Informationen den bisherigen Q-Wert beeinflussen sollen reward: die Belohnung (engl. \"reward\") nach der Wahl der Aktion 'action' im Zustand 'state' \\(\\gamma\\) : der Abschlagfaktor (engl. \"discount factor\"), der bestimmt, wie wichtig zuk\u00fcnftige Belohnungen im Vergleich zu aktuellen Belohnungen sind \\(max(Q_{next-state, all -actions})\\) : der maximale erwartete Nutzen, den man erh\u00e4lt, wenn man eine Aktion 'action' im n\u00e4chsten Zustand 'next state' w\u00e4hlt. Wir wir sehen, ist dabei, dass das bisher Gelernte den gr\u00f6\u00dften Einfluss hat \\((1 - \\alpha) * Q(state,action)\\) und der Zugewinn geht mit dem Lernratenparameter \\(\\alpha\\) ein. Die Implementierung des Algorithmus in Python kann wie folgt aussehen: import numpy as np import random from IPython.display import clear_output q_table = np . zeros ([ env . observation_space . n , env . action_space . n ]) # Hyperparameters alpha = 0.1 gamma = 0.6 for i in range ( 1 , 100001 ): state = env . reset () epochs , reward , = 0 , 0 , done = False while not done : action = np . argmax ( q_table [ state ]) # Exploit learned values next_state , reward , done , info = env . step ( action ) old_value = q_table [ state , action ] next_max = np . max ( q_table [ next_state ]) new_value = ( 1 - alpha ) * old_value + alpha * ( reward + gamma * next_max ) q_table [ state , action ] = new_value state = next_state epochs += 1 if i % 100 == 0 : clear_output ( wait = True ) print ( f \"Episode: { i } \" ) print ( \"Training finished. \\n \" ) Aufgabe : Trainieren Sie das Modell und lassen Sie sich nach dem Training die Q-Tabelle f\u00fcr den Zustand 182 ausgeben und entscheiden Sie daran, welche Aktion in diesem Zustand den meisten Erfolg bringt. L\u00f6sung : Die Q-Table im Zustands 182 zieht wie folgt aus: Der maximale Wert ist die -2.4510224, d.h. die erfolgreichsten Aktionen in diesem Zustands ist entweder eine Bewegung nach S\u00fcden oder nach Osten! Aufgabe : Entwickeln Sie nach erfolgreichem Training ein Programm, welches Ihnen Aussagen \u00fcber die Qualit\u00e4t des Modells erlaubt. Ver\u00e4ndern Sie ferner wichtige Parameter im Algorithmus und beobachten Sie, wie dieses sich auf die Qualit\u00e4t des Modells auswirken. L\u00f6sung : Hier ein Programm, welches 10 Episoden durchf\u00fchrt und die Anzahl der notwendigen Schritte und ggf. Fehlversuche z\u00e4hlt und visualisiert. penalties , reward = 0 , 0 frames = [] # for animation for i in range ( 1 , 10 ): env . reset () epochs = 0 done = False while not done : action = np . argmax ( q_table [ state ]) state , reward , done , info = env . step ( action ) if reward == - 10 : penalties += 1 # Put each rendered frame into dict for animation frames . append ({ 'frame' : env . render ( mode = 'ansi' ), 'state' : state , 'action' : action , 'reward' : reward , 'done' : done } ) epochs += 1 print ( \"Timesteps taken: {} \" . format ( epochs )) print ( \"Penalties incurred: {} \" . format ( penalties )) Und hier eine einfache Form der Visualisierung: def print_frames ( frames ): j = 0 for i , frame in enumerate ( frames ): clear_output ( wait = True ) env . s = frame [ 'state' ] j = j + 1 print ( f \"Timestep: { j } \" ) print ( f \"State: { frame [ 'state' ] } \" ) print ( f \"Action: { frame [ 'action' ] } \" ) print ( f \"Reward: { frame [ 'reward' ] } \" ) if frame [ 'done' ] == True : j = 0 env . render () sleep ( .5 ) print_frames ( frames ) Aufgabe : Wechseln Sie das Environment auf ein anderes Environment. Z.B. Das FrozenLake Environment. https://www.gymlibrary.dev/environments/toy_text/frozen_lake/ . Das Cliff Walking Environmment https://www.gymlibrary.dev/environments/toy_text/cliff_walking/ Das Mountain Car Einvorinment https://www.gymlibrary.dev/environments/classic_control/mountain_car/ Das CartPole Environment https://www.gymlibrary.dev/environments/classic_control/cart_pole/ Hinweis : Achtung das Mountain Car Environment und das CartPole Environment sind kontinuierliche Umgebungen, d.h. der Zustandsraum kann beliebige Werte annehmen. Daher muss hier der Zustandsraum erst diskretisiert werden! Gehen Sie im weiteren Verlauf wie folgt vor: Untersuchen Sie dabei zun\u00e4chst das Environment, welche Aktionen gibt es, bestimmen Sie den Action Space und den Observation Space. Versuchen Sie einen Brute Force Ansatz Trainieren Sie mit Hilfe des Q-learning Algorithmus ein Modell und beurteilen Sie dessen Qualit\u00e4t Dokumentieren und pr\u00e4sentieren Sie anschlie\u00dfend ihr Vorgehen","title":"Q-Learning Algorithmus"},{"location":"ReinforcedLearning/#musterlosung-fur-cartpole","text":"Das CartPole Environment besteht aus einem Schlitten, der nach rechts und links bewegt werden kann. Auf dem Schlitten ist eine Pendel installiert. Ziel ist es das Pendel in der aufrechten Position zu behalten.","title":"Musterl\u00f6sung f\u00fcr CartPole"},{"location":"ReinforcedLearning/#initialisierung-des-environments","text":"import gymnasium as gym import numpy as np import math import matplotlib.pyplot as plt # Erstelle die Umgebung env = gym . make ( 'CartPole-v1' , render_mode = \"rgb_array\" ) state = env . reset () print ( \"State=\" + str ( state [ 0 ])) plt . imshow ( env . render ()) plt . grid ( False ) Der Sate ist ein Array aus kontinuierlichen Float Werten, mit folgenden Bedeutungen: Index Bedeutung 0 Cart Position (-4.8 bis +4.8) 1 Cart velocity (+/- unendlich) 2 Pole Angle (- 0.418 bis + 0.418) 3 Pole Angular Velocity (+/- unendlich) Dieser kontinuierliche Zustandsraum muss in einen diskreten umgewandelt werden. Ich entschied mich daher jeden dieser Werte in 10 diskrete Werte umzuwandeln. Der Zustandsraum hat damit \\(10*10*10*10=10000\\) Werte. n_actions = 2 n_states = 10 * 10 * 10 * 10 # Festgelegt q_table = np . zeros ([ n_states , n_actions ]) q_table Die Zuordnung eines States zu einem Index-Wert in diesem Array \u00fcbernimmt die Funktion discret(state):int ! cpos = np . array ( np . linspace ( - 4.8 , 4.8 , 10 )) cvelocity = np . array ( np . linspace ( - 0.5 , 0.5 , 10 )) cpolea = np . array ( np . linspace ( - 0.418 , 0.418 , 10 )) cpolev = np . array ( np . linspace ( - 0.5 , 0.5 , 10 )) def discret ( s ): dsp = np . abs ( cpos - s [ 0 ]) . argmin () dsv = np . abs ( cvelocity - s [ 1 ]) . argmin () dspa = np . abs ( cpolea - s [ 2 ]) . argmin () dspv = np . abs ( cpolev - s [ 3 ]) . argmin () return dsp + dsv + dspa + dspv nr = discret ( state [ 0 ]) nr Nun kann der Q-Leaning Algorithmus implementiert werden! total_episodes = 50000 learning_rate = 0.8 max_steps = 100 gamma = 0.95 epsilon = 1.0 for episode in range ( total_episodes ): state = env . reset ()[ 0 ] done = False for step in range ( max_steps ): if np . random . uniform ( 0 , 1 ) > epsilon : action = np . argmax ( q_table [ discret ( state )]) else : action = env . action_space . sample () new_state , reward , done , info , a = env . step ( action ) if done and step < max_steps - 1 : reward = - 300 # negative reward for falling q_table [ discret ( state )][ action ] = ( q_table [ discret ( state )][ action ] + learning_rate * ( reward + gamma * np . max ( q_table [ discret ( new_state )]) - q_table [ discret ( state )][ action ])) state = new_state if done : break if episode % 100 == 0 : print ( \"Episode:\" + str ( episode )) epsilon = min_epsilon + ( max_epsilon - min_epsilon ) * np . exp ( - decay_rate * episode ) Nach dem Training kann mit der Erfolg am Modell gepr\u00fcft werden. from IPython import display import matplotlib import matplotlib.pyplot as plt % matplotlib inline state = env . reset ()[ 0 ] img = plt . imshow ( env . render ()) # only call this once done = False while not done : action = np . argmax ( q_table [ discret ( state )]) new_state , reward , done , info , a = env . step ( action ) img . set_data ( env . render ()) # just update the data display . display ( plt . gcf ()) display . clear_output ( wait = True ) state = new_state env . close ()","title":"Initialisierung des Environments"},{"location":"ReinforcedLearning/#fragen-zum-verstandnis","text":"Welches der folgenden Aussagen ist eine korrekte Beschreibung von Reinforcement Learning? [ ] A. Ein \u00fcberwachtes Lernen, bei dem die Modelle mit Hilfe von explizit beschrifteten Trainingsdaten trainiert werden. [ ] B. Ein un\u00fcberwachtes Lernen, bei dem die Modelle Muster in Daten entdecken, ohne dass menschliche Anleitung erforderlich ist. [ ] C. Ein Art von Lernen, bei dem ein Agent seine Handlungen basierend auf Belohnungen und Strafen optimiert. [ ] D. Ein semi-\u00fcberwachtes Lernen, das eine Mischung aus beschrifteten und unbeschrifteten Daten verwendet. Was ist ein wesentliches Merkmal des Q-Learning-Algorithmus im Reinforcement Learning? [ ] A. Q-Learning verwendet ein Modell der Umgebung, um die beste n\u00e4chste Aktion vorherzusagen. [ ] B. Q-Learning aktualisiert seine Q-Werte basierend auf der Differenz zwischen der erwarteten und der tats\u00e4chlichen Belohnung nach jeder Aktion (auch bekannt als TD-Fehler). [ ] C. Q-Learning erfordert eine vollst\u00e4ndige Kenntnis aller m\u00f6glichen Zust\u00e4nde und Aktionen vor Beginn des Lernprozesses. [ ] D. Q-Learning kann nur bei diskreten Zustands- und Handlungsr\u00e4umen angewendet werden. Was ist \"Deep Reinforcement Learning\"? [ ] A. Ein Ansatz f\u00fcr Reinforcement Learning, der genetische Algorithmen verwendet. [ ] B. Ein Ansatz f\u00fcr Reinforcement Learning, der neuronale Netzwerke verwendet, um Q- oder Value-Funktionen zu approximieren. [ ] C. Ein Ansatz f\u00fcr Reinforcement Learning, der tiefe neuronale Netzwerke verwendet, um die Umgebung des Agenten zu modellieren. [ ] D. Ein Ansatz f\u00fcr Reinforcement Learning, der sich ausschlie\u00dflich auf theoretische Forschung konzentriert und nicht in der Praxis angewendet wird. Welche Aussage \u00fcber Q-Learning ist korrekt? [ ] A. Q-Learning ist eine Art von \u00fcberwachtem Lernen. [ ] B. Q-Learning ist ein modellfreier Reinforcement Learning Algorithmus. [ ] C. Q-Learning erfordert ein vollst\u00e4ndiges Modell der Umgebung, um effektiv zu sein. [ ] D. Q-Learning kann nur in kontinuierlichen Zustandsr\u00e4umen verwendet werden. Was bedeutet Exploration vs Exploitation in Reinforcement Learning? [ ] A. Exploration bezieht sich auf das Erlernen von neuen Strategien, w\u00e4hrend Exploitation das Anwenden dieser Strategien bezeichnet. [ ] B. Exploration bezieht sich auf das Sammeln von Daten, w\u00e4hrend Exploitation die Analyse dieser Daten bezeichnet. [ ] C. Exploration bezieht sich auf das Testen unbekannter Aktionen, w\u00e4hrend Exploitation das Nutzen von bekanntem Wissen zur Maximierung der Belohnung bezeichnet. [ ] D. Exploration bezieht sich auf das Experimentieren mit verschiedenen Modellen, w\u00e4hrend Exploitation das Trainieren eines bestimmten Modells bezeichnet.","title":"Fragen zum Verst\u00e4ndnis"},{"location":"azure_Texterkennung/","text":"Cloud KI Dienste Der Besitzer eines Eigenheims m\u00f6chte gerne wissen welche Autos f\u00fcr wie lange auf seinen Parkplatz parken. Die Kamera vor dem Haus mach dazu alle 5 Minuten ein Bild von dem Parkplatz. Sie erhalten der Auftrag eine Anwendungssystem zu entwickeln welches das KFZ Kennzeichen erfasst und dieses mit einem Zeitstempel in eine Datenbank schreibt. Da das Antrainieren einer eigenen Texterkennungs KI zu aufwendig erscheint, entscheidet sich die Gesch\u00e4ftsf\u00fchrung dazu einen Cloud Dienstleister zu nutzen. Da bereits ein Azure Konto existiert, soll der Azure Bilderkennungsdienst genutzt werden. Der Bilderkennungsdienst von Azure Azure bietet eine vielzahl von trainierten Neuronalen Netzwerken f\u00fcr diverse Aufgaben an (z.B. Bildanalyse, Sprachsynthese u.v.a.m). In diesem Beispiel wollen wir das Texterkennungssystem von Azure nutzen. Dabei kommunizieren die KI Dienste von Azure \u00fcber eine REST Schnittstelle mit dem Client (das Programm). Einrichten der Texterkennung in Azure Melden Sie sich zun\u00e4chst in Azure an . F\u00fcr die Anmeldung bei Azure muss einmalig eine Telefonnummer verifiziert werden (eine Kreditkarte ist nicht notwendig). Sch\u00fclerinnen und Sch\u00fcler erhalten dabei ein Startguthaben von 100$ ! Nach der Anmeldung erstellen Sie in dem Portal zun\u00e4chst eine neue Ressourcengruppe und f\u00fcgen Sie dieser den Dienst Maschinelle Sehen hinzu. \u00dcber den Reiter Schl\u00fcssel und Endpunkt im Bereich Maschinelles Sehen k\u00f6nnen Sie die Zugriffsschl\u00fcssel f\u00fcr ihren Dienst abfragen. Funktion des Bilderkennungsdienstes Der Bilderkennungsdienst wird \u00fcber zwei HTTP Requests angesprochen. Der erste HTTP Request in Form eines HTTP-Post Befehl stellt dem Dienst das zu analysierende Bild zur Verf\u00fcgung. Entweder in Form eines JSON: { \"url\" : \"http Adresse des Bildes\" } Oder die Bilddatei wird dem Dienst im Body des http Post Befehls \u00fcbertragen. Im Header des HTTP Post Befehls steht der API Schl\u00fcssel mit dem key Ocp-Apim-Subscription-Key : POST https://{Name der Ressourcegruppe}.cognitiveservices. azure.com/vision/v3.2/read/analyze HTTP/1.1 content-type: application/json Ocp-Apim-Subscription-Key: {api-key} { \"url\":\"https://raw.githubusercontent.com/MicrosoftDocs/ azure-docs/master/articles/ cognitive-services/Computer-vision/ Images/readsample.jpg\" } Liefert der Dienst den Status Code 202 Accepted zur\u00fcck, so enth\u00e4lt der Header des Response einen key Operation-Location , der zum Abholen des Ergebnisses dient. HTTP/1.1 202 Accepted Content-Length: 0 Operation-Location: https://{Name der Ressourceguppe}. cognitiveservices.azure.com/vision/v3.2 /read/analyzeResults/ 8bba342e-f1eb-4e24-8702-db823f615a38 x-envoy-upstream-service-time: 584 CSP-Billing-Usage: CognitiveServices.ComputerVision.Transaction=1 apim-request-id: 8bba342e-f1eb-4e24-8702-db823f615a38 Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-content-type-options: nosniff Date: Sat, 15 Oct 2022 16:11:18 GMT Connection: close \u00dcber einen zweiten HTTP-Get Request auf die Adresse Operation-Location kann das Ergebnis in Form eines JSON abgeholt werden. Dabei muss erneut der Ocp-Apim-Subscription-Key angegeben werden. GET https://{Name der Ressourcegruppe}. cognitiveservices.azure.com /vision/v3.2/read/analyzeResults/ 54d749b7-697e-4a8b-a2f2-8caefa1ba053 HTTP/1.1 Ocp-Apim-Subscription-Key: {api-key} Die Funktion des Texterkennung Dienstes stellt das folgende Sequenzdiagramm dar. Aufgaben Aufgabe 1 Lesen Sie sich noch einmal aufmerksam die Informationen zum Bilderkennungsdienst von Azure durch. Melden Sie sich dann bei Azure an und erzeugen Sie wie beschrieben eine Ressourcengruppe und f\u00fcgen Sie dort einen Bilderkennungsdienst \"Maschinelles Sehen\" hinzu. Notieren Sie sich dich Zugangsdaten zum Dienst und f\u00fchren Sie eine HTTP Post Request zum Dienst durch. Z.B. via dem Programm CURL curl --request POST --url https://{Ressourcegruppe}.cognitiveservices.azure.com/vision/v3.2/read/analyze --header 'content-type: application/json' --header 'ocp-apim-subscription-key: {api key}' --data '{\"url\":\"https://raw.githubusercontent.com/MicrosoftDocs/azure-docs /master/articles/cognitive-services/ Computer-vision/Images/readsample.jpg\"}' Schreiben Sie ein Programm (die Programmiersprache ist hier beliebig) welchen des Request durchf\u00fchrt und lassen Sie sich den Head des Responses auf der Konsole ausgeben. Modifizieren Sie ihr Programm, dass statt dem JSON die Bytes des Bildes mit dem parkenden Auto \u00fcbertragen werden. Sollten Sie Python als Programmiersprache verwenden, so hilft diese Code Schnipsel ihnen den request zu erzeugen und die Bilddaten im Body des Requests zu \u00fcbertragen! import requests files = [( 'file' , ( image_name , open ( image_name , 'rb' ), 'image/jpeg' ))] headers = { 'Ocp-Apim-Subscription-Key' : api_key } r2 = requests . post ( \"https://\" + api_endpunkt + \".cognitiveservices.azure.com / vision / v3 .2 / read / analyze \", files = files , headers = headers ) ol = r2 . headers [ \"Operation-Location\" ] print ( ol ) Aufgabe 2 Notieren Sie sich die URL, die Sie im Header Operation-Location aus Aufgabe 1 erhalten haben und gehen Sie dann wie folgt vor. Erzeugen Sie einen GET Request auf die Adresse Operation-Location . Wobei im Header der API key als Ocp-Apim-Subscription-Key anzugeben ist und lassen Sie sich die Antwort des Requests auf der Console ausgeben. curl --request GET --url https://{Name der Ressourcengruppe}. cognitiveservices.azure.com/vision/v3.2 /read/analyzeResults/ 54d749b7-697e-4a8b-a2f2-8caefa1ba053 --header 'ocp-apim-subscription-key: {API key}' Erweitern Sie ihr Programm aus Aufgabe 1 um diesen Get-Request und lassen Sie sich den Body des Response auf der Konsole ausgeben. Aufgabe 3 Analysieren Sie das im Response erhaltene JSON und zeichnen Sie auf das Bild den Bereich des erkannten Textes und den Text ein. Wenn Sie als Programmiersprache Python verwenden, so k\u00f6nnen Sie hierzu die Bibliothek mathplotlib verwenden. { \"boundingBox\" : [ 130 , 655 , 418 , 664 , 413 , 825 , 124 , 823 ], \"text\" : \"The\" , \"confidence\" : 0.998 }, Aufgabe 4 (Zusatz) Speichern die abschlie\u00dfend den gelesenen Text mit aktuellem Zeitstempel in eine Datenbank Aufgabe 5 Kalkulation der Kosten Nachdem die technische Umsetzung des Arbeitsauftrages realisiert wurde. Stellt sich der Kunde die Frage, wie hoch die laufenden Kosten f\u00fcr den bereitgestellten Dienst sind? Recherchieren Sie in der Azure Dokumention nach den Kosten f\u00fcr den Dienst \"maschinelles Sehen\" und erstellen Sie f\u00fcr den Kunden ein Angebot, welches die monatlichen Kosten darstellt. Datenschutz Nach dem Realisieren der Aufgabe erhalten Sie eine weitergeleitete Email vom Auftraggeber mit folgendem Inhalt: Sehr geehrte Damen und Herren, wie ich feststellen konnte nimmt ihre \u00dcberwachungskamera Fotos von meinem Auto auf und erfasst dabei auch das KFZ Kennzeichen. Mit Blick auf die DSGVO m\u00f6chte ich Sie bitten dieses Aufzeichnungen von personengebundenen Daten zu Unterlassen. Sollten Sie ihren Dienst nicht innerhalb der n\u00e4chsten Woche abstellen, so werde ich mit rechtliche Schritte vorbehalten. Mit freundlichen Gr\u00fc\u00dfen Dr. Harald W\u00f6hler Beantworten Sie diese Email, nachdem Sie sich zuvor \u00fcber die rechtlichen Grundlagen informiert haben!","title":"Cloud KI Dienste"},{"location":"azure_Texterkennung/#cloud-ki-dienste","text":"Der Besitzer eines Eigenheims m\u00f6chte gerne wissen welche Autos f\u00fcr wie lange auf seinen Parkplatz parken. Die Kamera vor dem Haus mach dazu alle 5 Minuten ein Bild von dem Parkplatz. Sie erhalten der Auftrag eine Anwendungssystem zu entwickeln welches das KFZ Kennzeichen erfasst und dieses mit einem Zeitstempel in eine Datenbank schreibt. Da das Antrainieren einer eigenen Texterkennungs KI zu aufwendig erscheint, entscheidet sich die Gesch\u00e4ftsf\u00fchrung dazu einen Cloud Dienstleister zu nutzen. Da bereits ein Azure Konto existiert, soll der Azure Bilderkennungsdienst genutzt werden.","title":"Cloud KI Dienste"},{"location":"azure_Texterkennung/#der-bilderkennungsdienst-von-azure","text":"Azure bietet eine vielzahl von trainierten Neuronalen Netzwerken f\u00fcr diverse Aufgaben an (z.B. Bildanalyse, Sprachsynthese u.v.a.m). In diesem Beispiel wollen wir das Texterkennungssystem von Azure nutzen. Dabei kommunizieren die KI Dienste von Azure \u00fcber eine REST Schnittstelle mit dem Client (das Programm).","title":"Der Bilderkennungsdienst von Azure"},{"location":"azure_Texterkennung/#einrichten-der-texterkennung-in-azure","text":"Melden Sie sich zun\u00e4chst in Azure an . F\u00fcr die Anmeldung bei Azure muss einmalig eine Telefonnummer verifiziert werden (eine Kreditkarte ist nicht notwendig). Sch\u00fclerinnen und Sch\u00fcler erhalten dabei ein Startguthaben von 100$ ! Nach der Anmeldung erstellen Sie in dem Portal zun\u00e4chst eine neue Ressourcengruppe und f\u00fcgen Sie dieser den Dienst Maschinelle Sehen hinzu. \u00dcber den Reiter Schl\u00fcssel und Endpunkt im Bereich Maschinelles Sehen k\u00f6nnen Sie die Zugriffsschl\u00fcssel f\u00fcr ihren Dienst abfragen.","title":"Einrichten der Texterkennung in Azure"},{"location":"azure_Texterkennung/#funktion-des-bilderkennungsdienstes","text":"Der Bilderkennungsdienst wird \u00fcber zwei HTTP Requests angesprochen. Der erste HTTP Request in Form eines HTTP-Post Befehl stellt dem Dienst das zu analysierende Bild zur Verf\u00fcgung. Entweder in Form eines JSON: { \"url\" : \"http Adresse des Bildes\" } Oder die Bilddatei wird dem Dienst im Body des http Post Befehls \u00fcbertragen. Im Header des HTTP Post Befehls steht der API Schl\u00fcssel mit dem key Ocp-Apim-Subscription-Key : POST https://{Name der Ressourcegruppe}.cognitiveservices. azure.com/vision/v3.2/read/analyze HTTP/1.1 content-type: application/json Ocp-Apim-Subscription-Key: {api-key} { \"url\":\"https://raw.githubusercontent.com/MicrosoftDocs/ azure-docs/master/articles/ cognitive-services/Computer-vision/ Images/readsample.jpg\" } Liefert der Dienst den Status Code 202 Accepted zur\u00fcck, so enth\u00e4lt der Header des Response einen key Operation-Location , der zum Abholen des Ergebnisses dient. HTTP/1.1 202 Accepted Content-Length: 0 Operation-Location: https://{Name der Ressourceguppe}. cognitiveservices.azure.com/vision/v3.2 /read/analyzeResults/ 8bba342e-f1eb-4e24-8702-db823f615a38 x-envoy-upstream-service-time: 584 CSP-Billing-Usage: CognitiveServices.ComputerVision.Transaction=1 apim-request-id: 8bba342e-f1eb-4e24-8702-db823f615a38 Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-content-type-options: nosniff Date: Sat, 15 Oct 2022 16:11:18 GMT Connection: close \u00dcber einen zweiten HTTP-Get Request auf die Adresse Operation-Location kann das Ergebnis in Form eines JSON abgeholt werden. Dabei muss erneut der Ocp-Apim-Subscription-Key angegeben werden. GET https://{Name der Ressourcegruppe}. cognitiveservices.azure.com /vision/v3.2/read/analyzeResults/ 54d749b7-697e-4a8b-a2f2-8caefa1ba053 HTTP/1.1 Ocp-Apim-Subscription-Key: {api-key} Die Funktion des Texterkennung Dienstes stellt das folgende Sequenzdiagramm dar.","title":"Funktion des Bilderkennungsdienstes"},{"location":"azure_Texterkennung/#aufgaben","text":"","title":"Aufgaben"},{"location":"azure_Texterkennung/#aufgabe-1","text":"Lesen Sie sich noch einmal aufmerksam die Informationen zum Bilderkennungsdienst von Azure durch. Melden Sie sich dann bei Azure an und erzeugen Sie wie beschrieben eine Ressourcengruppe und f\u00fcgen Sie dort einen Bilderkennungsdienst \"Maschinelles Sehen\" hinzu. Notieren Sie sich dich Zugangsdaten zum Dienst und f\u00fchren Sie eine HTTP Post Request zum Dienst durch. Z.B. via dem Programm CURL curl --request POST --url https://{Ressourcegruppe}.cognitiveservices.azure.com/vision/v3.2/read/analyze --header 'content-type: application/json' --header 'ocp-apim-subscription-key: {api key}' --data '{\"url\":\"https://raw.githubusercontent.com/MicrosoftDocs/azure-docs /master/articles/cognitive-services/ Computer-vision/Images/readsample.jpg\"}' Schreiben Sie ein Programm (die Programmiersprache ist hier beliebig) welchen des Request durchf\u00fchrt und lassen Sie sich den Head des Responses auf der Konsole ausgeben. Modifizieren Sie ihr Programm, dass statt dem JSON die Bytes des Bildes mit dem parkenden Auto \u00fcbertragen werden. Sollten Sie Python als Programmiersprache verwenden, so hilft diese Code Schnipsel ihnen den request zu erzeugen und die Bilddaten im Body des Requests zu \u00fcbertragen! import requests files = [( 'file' , ( image_name , open ( image_name , 'rb' ), 'image/jpeg' ))] headers = { 'Ocp-Apim-Subscription-Key' : api_key } r2 = requests . post ( \"https://\" + api_endpunkt + \".cognitiveservices.azure.com / vision / v3 .2 / read / analyze \", files = files , headers = headers ) ol = r2 . headers [ \"Operation-Location\" ] print ( ol )","title":"Aufgabe 1"},{"location":"azure_Texterkennung/#aufgabe-2","text":"Notieren Sie sich die URL, die Sie im Header Operation-Location aus Aufgabe 1 erhalten haben und gehen Sie dann wie folgt vor. Erzeugen Sie einen GET Request auf die Adresse Operation-Location . Wobei im Header der API key als Ocp-Apim-Subscription-Key anzugeben ist und lassen Sie sich die Antwort des Requests auf der Console ausgeben. curl --request GET --url https://{Name der Ressourcengruppe}. cognitiveservices.azure.com/vision/v3.2 /read/analyzeResults/ 54d749b7-697e-4a8b-a2f2-8caefa1ba053 --header 'ocp-apim-subscription-key: {API key}' Erweitern Sie ihr Programm aus Aufgabe 1 um diesen Get-Request und lassen Sie sich den Body des Response auf der Konsole ausgeben.","title":"Aufgabe 2"},{"location":"azure_Texterkennung/#aufgabe-3","text":"Analysieren Sie das im Response erhaltene JSON und zeichnen Sie auf das Bild den Bereich des erkannten Textes und den Text ein. Wenn Sie als Programmiersprache Python verwenden, so k\u00f6nnen Sie hierzu die Bibliothek mathplotlib verwenden. { \"boundingBox\" : [ 130 , 655 , 418 , 664 , 413 , 825 , 124 , 823 ], \"text\" : \"The\" , \"confidence\" : 0.998 },","title":"Aufgabe 3"},{"location":"azure_Texterkennung/#aufgabe-4-zusatz","text":"Speichern die abschlie\u00dfend den gelesenen Text mit aktuellem Zeitstempel in eine Datenbank","title":"Aufgabe 4 (Zusatz)"},{"location":"azure_Texterkennung/#aufgabe-5","text":"Kalkulation der Kosten Nachdem die technische Umsetzung des Arbeitsauftrages realisiert wurde. Stellt sich der Kunde die Frage, wie hoch die laufenden Kosten f\u00fcr den bereitgestellten Dienst sind? Recherchieren Sie in der Azure Dokumention nach den Kosten f\u00fcr den Dienst \"maschinelles Sehen\" und erstellen Sie f\u00fcr den Kunden ein Angebot, welches die monatlichen Kosten darstellt.","title":"Aufgabe 5"},{"location":"azure_Texterkennung/#datenschutz","text":"Nach dem Realisieren der Aufgabe erhalten Sie eine weitergeleitete Email vom Auftraggeber mit folgendem Inhalt: Sehr geehrte Damen und Herren, wie ich feststellen konnte nimmt ihre \u00dcberwachungskamera Fotos von meinem Auto auf und erfasst dabei auch das KFZ Kennzeichen. Mit Blick auf die DSGVO m\u00f6chte ich Sie bitten dieses Aufzeichnungen von personengebundenen Daten zu Unterlassen. Sollten Sie ihren Dienst nicht innerhalb der n\u00e4chsten Woche abstellen, so werde ich mit rechtliche Schritte vorbehalten. Mit freundlichen Gr\u00fc\u00dfen Dr. Harald W\u00f6hler Beantworten Sie diese Email, nachdem Sie sich zuvor \u00fcber die rechtlichen Grundlagen informiert haben!","title":"Datenschutz"},{"location":"k-mean/","text":"K-Mean Clustering Handlungssituation Ein Regionaler Stromanbieter will in seinem Versorgungsgebiet Lades\u00e4ulen f\u00fcr Elektroautos anbieten. Dabei sollen m\u00f6glichst alle Kunden einen kurzen Weg zur angebotenen Lades\u00e4ule habe.[^1] Die Abteilung Daten- und Prozessanalyse der ChangeIT GmbH wird damit beauftragt diese Standorte zu ermitteln.. Der Kunde stellt dazu einen Plan zur Verf\u00fcgung, der die Wohnorte der Kunden, die ein Elektroauto besitzen enth\u00e4lt. [^1]: Vgl. Brandt, Y., Eickhoff-Schachtebeck, A. und Strecker, K. (2022): \u201eSchulbuch starkeSeiten Informatik Jahrgang 9/10 Differenzierende Ausgabe Niedersachsen\u201c, Klett-Verlag 2022 Aufgabe 1 - Datenexploration und -Visualisierung Laden Sie die zur Verf\u00fcgung gestellten CSV Daten ( Ladestationen.csv ). Diese Datei enth\u00e4lt die Positionen der Elektro Auto Besitzer in der Region. Stelle Sie diese Positionen grafisch dar (vorzugsweise als graue Punkte in einem Koordinatensystem). Aufgabe 2 In dem dargestellten Gebiet sollen nun 3 Ladestationen gebaut werden. W\u00e4hlen Sie drei m\u00f6glich Standorte f\u00fcr die Ladestationen ( a,b,c ), so dass jeder Besitzer einer E-Autos einen m\u00f6glichst kurzen Weg zu einer Ladestation hat. Speicher Sie diese Werte in einem zweidimensionalen Array ab. l_station = [[ ax , ay ],[ bx , by ],[ cx , cy ]] Zeichnen Sie zun\u00e4chst die ausgew\u00e4hlten Punkte in das Diagramm ein! Aufgabe 3 Doch welche E-Auto Besitzer fahren jetzt zu welcher Lades\u00e4ule? Im n\u00e4chsten Schritt wollen wir dieses ermitteln. Dazu ist die Entfernung einer Ladestation zu einem E-Auto zu bestimmen. Da wir diese Information \u00f6fter brauchen soll eine Funktion entwickelt werden mit dem Namen distance(ax,ay,px,py) , die die Entfernung zweier Punkte bestimmt. Die Entfernung kann dabei \u00fcber den Satz des Pythagoras ermittel werden. Es gilt: \\[c = \\sqrt{a^2 + b^2}\\] bzw. \\[c = \\sqrt{(ax-px)^2 + (ay-py)^2}\\] Im n-dimensionalen Raum wird diese Abstand auch als Euklidische Distanz bezeichnet. F\u00fcr einen n-dimensionalen Raum sieht die Berechnung des Euklidische Distanz wir folgt aus: \\[ \\sqrt{\\sum_{i=1}^{n}(q_i-p_i)^2} \\] Hierbei beschreibt \\(n\\) die Anzahl der Dimensionen der Punkte \\(p\\) und \\(q\\) und \\(p_i\\) und \\(q_i\\) stellen die Werte der i-ten Dimension der jeweiligen Punkte dar. L\u00f6sung Aufgabe 3 import math def distance ( x1 , y1 , x2 , y2 ): dist = math . sqrt (( x2 - x1 ) ** 2 + ( y2 - y1 ) ** 2 ) return dist Aufgabe 4 Teilen Sie die Datenmenge nun in 3 Mengen ein, jede Menge sind dabei die Autos, die zu der jeweiligen Ladestation fahren w\u00fcrden (also wo der Abstand zur jeweiligen Ladestation am k\u00fcrzesten ist). Stellen Sie diese 3 Mengen dar und f\u00e4rben Sie geeigneter Weise die Punkte entsprechend ein. L\u00f6sung Aufgabe 4 Aufgabe 5 Die zun\u00e4chst willk\u00fcrliche Wahl der Position der Ladestationen hat zu einem ersten \u00dcberblick gef\u00fchrt. Dabei ist die Position der Lades\u00e4ulen sicherlich nicht optimal. Jetzt wo wir die 3 Mengen gebildet haben, kann eine optimierte Position der Lades\u00e4ulen ermittelt werden, indem man einfach das Mittel dieser Datenmenge bestimmt. x-Mittelpunkt: \\((x_1 + x_2 + ... + x_n)/n\\) y-Mittelpunkt: \\((y_1 + y_2 + ... + y_n)/n\\) Ermitteln Sie f\u00fcr jede der 3 Datenmenge eine neue optimale Position und zeichnen Sie diese in das Diagramm ein. L\u00f6sung Aufgabe 5 Aufgabe 6 - iteration des Algorithmus Wenn Sie jetzt erneut anhand der neuen Position der Lades\u00e4ulen die k\u00fcrzesten Weg ermitteln die ein E-Auto zur jeweiligen Lades\u00e4ule hat, kann es passieren, dass das Auto nun zu einer anderen S\u00e4ule fahren w\u00fcrde. Unser Algorithmus muss also iterativ so lange weiter laufen, bis sich die Mengen nicht mehr ver\u00e4ndern. Dann haben wir die optimale Position der Lades\u00e4ulen gefunden. Wiederholen Sie den den Vorgang mindestens f\u00fcr wei weiter Durchg\u00e4nge und beobachten Sie, wie sich die Position der Lades\u00e4ulen ver\u00e4ndert! Aufgabe 7 - Python Module verwenden Das Python Modul KMeans aus dem Paket sklearn.cluster \u00fcbernimmt genau diese Aufgabe. Der hier aufgef\u00fchrte Beispielcode konfiguriert dieses Modul: import numpy as np from sklearn.cluster import KMeans # Definiere die Anzahl der Cluster k = 3 # Erstelle ein KMeans-Objekt mit der Anzahl der Cluster kmeans = KMeans ( n_clusters = k ) # Wende den K-Means-Algorithmus auf die Daten an kmeans . fit ( data ) # Ermittle die Positionen der Zentroiden (d.h. der Punkte in der Mitte jedes Clusters) centroids = kmeans . cluster_centers_ # Gib die Positionen der Zentroiden aus print ( \"Die Zentroiden sind:\" ) centroids Verwenden Sie dieses Modul und wenden Sie es auf die Datenmenge der E-Auto Besitzer an. Tragen Sie die optimalen Positionen in die Karte ein und visualisieren Sie die Gruppen (Cluster). Aufgabe 8 - Einsatzgebiete f\u00fcr das k-mean Clustering Recherchieren Sie im Internet nach weiteren Einsatzm\u00f6glichkeiten / Use-Cases f\u00fcr die Verwendung des k-mean Clusterings . Fassen Sie die Einsatzm\u00f6glichkeit in einem Wort zusammen und erstellen Sie eine Wortwolke. Fragen zum Verst\u00e4ndnis Was ist das Hauptziel des K-Means-Clustering-Algorithmus? [ ] Die Anzahl der Datenpunkte in einem Datensatz zu reduzieren [ ] Die optimale Anzahl von Clustern in einem Datensatz zu bestimmen [ ] Datenpunkte in Clustern zu gruppieren, basierend auf ihrer \u00c4hnlichkeit [ ] Eine lineare Regression durchzuf\u00fchren Welche Entfernungsformel wird im K-Means-Algorithmus verwendet? [ ] Manhattan-Distanz [ ] Kosinus-\u00c4hnlichkeit [ ] Pearson-Korrelation [ ] Euklidische Distanz Wann kann der K-Means-Algorithmus als konvergiert betrachtet werden? [ ] Wenn sich die Clusterzentren nicht mehr signifikant ver\u00e4ndern [ ] Wenn alle Datenpunkte gleichm\u00e4\u00dfig \u00fcber die Cluster verteilt sind [ ] Wenn die Anzahl der Cluster gleich der Anzahl der Datenpunkte ist [ ] Wenn die Summe der Abst\u00e4nde zwischen den Datenpunkten und ihren jeweiligen Clustern minimiert ist Welches Python-Paket enth\u00e4lt den KMeans-Algorithmus, der in dieser Unterrichtseinheit verwendet wurde? [ ] pandas [ ] seaborn [ ] matplotlib [ ] sklearn.cluster Welche der folgenden Anwendungen ist ein typisches Anwendungsgebiet f\u00fcr K-Means-Clustering? [ ] Bildkomprimierung [ ] Textklassifikation [ ] Zeitreihenanalyse [ ] Anomalieerkennung","title":"K-Mean Clustering"},{"location":"k-mean/#k-mean-clustering","text":"","title":"K-Mean Clustering"},{"location":"k-mean/#handlungssituation","text":"Ein Regionaler Stromanbieter will in seinem Versorgungsgebiet Lades\u00e4ulen f\u00fcr Elektroautos anbieten. Dabei sollen m\u00f6glichst alle Kunden einen kurzen Weg zur angebotenen Lades\u00e4ule habe.[^1] Die Abteilung Daten- und Prozessanalyse der ChangeIT GmbH wird damit beauftragt diese Standorte zu ermitteln.. Der Kunde stellt dazu einen Plan zur Verf\u00fcgung, der die Wohnorte der Kunden, die ein Elektroauto besitzen enth\u00e4lt. [^1]: Vgl. Brandt, Y., Eickhoff-Schachtebeck, A. und Strecker, K. (2022): \u201eSchulbuch starkeSeiten Informatik Jahrgang 9/10 Differenzierende Ausgabe Niedersachsen\u201c, Klett-Verlag 2022","title":"Handlungssituation"},{"location":"k-mean/#aufgabe-1-datenexploration-und-visualisierung","text":"Laden Sie die zur Verf\u00fcgung gestellten CSV Daten ( Ladestationen.csv ). Diese Datei enth\u00e4lt die Positionen der Elektro Auto Besitzer in der Region. Stelle Sie diese Positionen grafisch dar (vorzugsweise als graue Punkte in einem Koordinatensystem).","title":"Aufgabe 1 - Datenexploration und -Visualisierung"},{"location":"k-mean/#aufgabe-2","text":"In dem dargestellten Gebiet sollen nun 3 Ladestationen gebaut werden. W\u00e4hlen Sie drei m\u00f6glich Standorte f\u00fcr die Ladestationen ( a,b,c ), so dass jeder Besitzer einer E-Autos einen m\u00f6glichst kurzen Weg zu einer Ladestation hat. Speicher Sie diese Werte in einem zweidimensionalen Array ab. l_station = [[ ax , ay ],[ bx , by ],[ cx , cy ]] Zeichnen Sie zun\u00e4chst die ausgew\u00e4hlten Punkte in das Diagramm ein!","title":"Aufgabe 2"},{"location":"k-mean/#aufgabe-3","text":"Doch welche E-Auto Besitzer fahren jetzt zu welcher Lades\u00e4ule? Im n\u00e4chsten Schritt wollen wir dieses ermitteln. Dazu ist die Entfernung einer Ladestation zu einem E-Auto zu bestimmen. Da wir diese Information \u00f6fter brauchen soll eine Funktion entwickelt werden mit dem Namen distance(ax,ay,px,py) , die die Entfernung zweier Punkte bestimmt. Die Entfernung kann dabei \u00fcber den Satz des Pythagoras ermittel werden. Es gilt: \\[c = \\sqrt{a^2 + b^2}\\] bzw. \\[c = \\sqrt{(ax-px)^2 + (ay-py)^2}\\] Im n-dimensionalen Raum wird diese Abstand auch als Euklidische Distanz bezeichnet. F\u00fcr einen n-dimensionalen Raum sieht die Berechnung des Euklidische Distanz wir folgt aus: \\[ \\sqrt{\\sum_{i=1}^{n}(q_i-p_i)^2} \\] Hierbei beschreibt \\(n\\) die Anzahl der Dimensionen der Punkte \\(p\\) und \\(q\\) und \\(p_i\\) und \\(q_i\\) stellen die Werte der i-ten Dimension der jeweiligen Punkte dar.","title":"Aufgabe 3"},{"location":"k-mean/#losung-aufgabe-3","text":"import math def distance ( x1 , y1 , x2 , y2 ): dist = math . sqrt (( x2 - x1 ) ** 2 + ( y2 - y1 ) ** 2 ) return dist","title":"L\u00f6sung Aufgabe 3"},{"location":"k-mean/#aufgabe-4","text":"Teilen Sie die Datenmenge nun in 3 Mengen ein, jede Menge sind dabei die Autos, die zu der jeweiligen Ladestation fahren w\u00fcrden (also wo der Abstand zur jeweiligen Ladestation am k\u00fcrzesten ist). Stellen Sie diese 3 Mengen dar und f\u00e4rben Sie geeigneter Weise die Punkte entsprechend ein.","title":"Aufgabe 4"},{"location":"k-mean/#losung-aufgabe-4","text":"","title":"L\u00f6sung Aufgabe 4"},{"location":"k-mean/#aufgabe-5","text":"Die zun\u00e4chst willk\u00fcrliche Wahl der Position der Ladestationen hat zu einem ersten \u00dcberblick gef\u00fchrt. Dabei ist die Position der Lades\u00e4ulen sicherlich nicht optimal. Jetzt wo wir die 3 Mengen gebildet haben, kann eine optimierte Position der Lades\u00e4ulen ermittelt werden, indem man einfach das Mittel dieser Datenmenge bestimmt. x-Mittelpunkt: \\((x_1 + x_2 + ... + x_n)/n\\) y-Mittelpunkt: \\((y_1 + y_2 + ... + y_n)/n\\) Ermitteln Sie f\u00fcr jede der 3 Datenmenge eine neue optimale Position und zeichnen Sie diese in das Diagramm ein.","title":"Aufgabe 5"},{"location":"k-mean/#losung-aufgabe-5","text":"","title":"L\u00f6sung Aufgabe 5"},{"location":"k-mean/#aufgabe-6-iteration-des-algorithmus","text":"Wenn Sie jetzt erneut anhand der neuen Position der Lades\u00e4ulen die k\u00fcrzesten Weg ermitteln die ein E-Auto zur jeweiligen Lades\u00e4ule hat, kann es passieren, dass das Auto nun zu einer anderen S\u00e4ule fahren w\u00fcrde. Unser Algorithmus muss also iterativ so lange weiter laufen, bis sich die Mengen nicht mehr ver\u00e4ndern. Dann haben wir die optimale Position der Lades\u00e4ulen gefunden. Wiederholen Sie den den Vorgang mindestens f\u00fcr wei weiter Durchg\u00e4nge und beobachten Sie, wie sich die Position der Lades\u00e4ulen ver\u00e4ndert!","title":"Aufgabe 6 - iteration des Algorithmus"},{"location":"k-mean/#aufgabe-7-python-module-verwenden","text":"Das Python Modul KMeans aus dem Paket sklearn.cluster \u00fcbernimmt genau diese Aufgabe. Der hier aufgef\u00fchrte Beispielcode konfiguriert dieses Modul: import numpy as np from sklearn.cluster import KMeans # Definiere die Anzahl der Cluster k = 3 # Erstelle ein KMeans-Objekt mit der Anzahl der Cluster kmeans = KMeans ( n_clusters = k ) # Wende den K-Means-Algorithmus auf die Daten an kmeans . fit ( data ) # Ermittle die Positionen der Zentroiden (d.h. der Punkte in der Mitte jedes Clusters) centroids = kmeans . cluster_centers_ # Gib die Positionen der Zentroiden aus print ( \"Die Zentroiden sind:\" ) centroids Verwenden Sie dieses Modul und wenden Sie es auf die Datenmenge der E-Auto Besitzer an. Tragen Sie die optimalen Positionen in die Karte ein und visualisieren Sie die Gruppen (Cluster).","title":"Aufgabe 7 - Python Module verwenden"},{"location":"k-mean/#aufgabe-8-einsatzgebiete-fur-das-k-mean-clustering","text":"Recherchieren Sie im Internet nach weiteren Einsatzm\u00f6glichkeiten / Use-Cases f\u00fcr die Verwendung des k-mean Clusterings . Fassen Sie die Einsatzm\u00f6glichkeit in einem Wort zusammen und erstellen Sie eine Wortwolke.","title":"Aufgabe 8 - Einsatzgebiete f\u00fcr das k-mean Clustering"},{"location":"k-mean/#fragen-zum-verstandnis","text":"Was ist das Hauptziel des K-Means-Clustering-Algorithmus? [ ] Die Anzahl der Datenpunkte in einem Datensatz zu reduzieren [ ] Die optimale Anzahl von Clustern in einem Datensatz zu bestimmen [ ] Datenpunkte in Clustern zu gruppieren, basierend auf ihrer \u00c4hnlichkeit [ ] Eine lineare Regression durchzuf\u00fchren Welche Entfernungsformel wird im K-Means-Algorithmus verwendet? [ ] Manhattan-Distanz [ ] Kosinus-\u00c4hnlichkeit [ ] Pearson-Korrelation [ ] Euklidische Distanz Wann kann der K-Means-Algorithmus als konvergiert betrachtet werden? [ ] Wenn sich die Clusterzentren nicht mehr signifikant ver\u00e4ndern [ ] Wenn alle Datenpunkte gleichm\u00e4\u00dfig \u00fcber die Cluster verteilt sind [ ] Wenn die Anzahl der Cluster gleich der Anzahl der Datenpunkte ist [ ] Wenn die Summe der Abst\u00e4nde zwischen den Datenpunkten und ihren jeweiligen Clustern minimiert ist Welches Python-Paket enth\u00e4lt den KMeans-Algorithmus, der in dieser Unterrichtseinheit verwendet wurde? [ ] pandas [ ] seaborn [ ] matplotlib [ ] sklearn.cluster Welche der folgenden Anwendungen ist ein typisches Anwendungsgebiet f\u00fcr K-Means-Clustering? [ ] Bildkomprimierung [ ] Textklassifikation [ ] Zeitreihenanalyse [ ] Anomalieerkennung","title":"Fragen zum Verst\u00e4ndnis"},{"location":"knn/","text":"KNN - K Nearest Neighbor Handlungssituation Ein Online Fahrradh\u00e4ndler m\u00f6chte seinen Kunden stets Fahrr\u00e4der in der optimale Rahmenh\u00f6he anbieten. Dazu soll eine App inkl. Vorhersagemodell entwickelt werden, so dass der Kunde lediglich ein Foto aufnehmen muss und ihm die optimale Rahmenh\u00f6he empfohlen wird. Eine Abteilung der ChangeIT GmbH ist bereits damit beauftragt aus einem Bild die Schrittl\u00e4nge und die K\u00f6rpergr\u00f6\u00dfe (jeweils in cm) zu ermitteln. Sie sollen ein Vorhersagemodell entwickeln, welches dem Kunden die richtige Rahmengr\u00f6\u00dfe vorschl\u00e4gt. Der Online Fahrradh\u00e4ndler hat bereits Daten in Form von Erfahrungswerten vorliegen und stellt ihnen diese in Form einer CSV Datei zur Verf\u00fcgung. Exploration der Daten Stelle Sie zun\u00e4chst die Daten, die Sie vom Online H\u00e4ndler erhalten haben grafisch dar ( CSV Datei ). Zur besseren Unterscheidung der Kategorien (Klassen) sollte jede Rahmenh\u00f6he (S,M,L,XL) in einer anderen Farbe dargestellt werden. Erste \u00dcberlegungen zum Vorhersagemodell Angenommen ein Kunde wird ausgemessen und es ergibt sich eine K\u00f6rpergr\u00f6\u00dfe von 163cm und eine Schrittl\u00e4nge von 84cm (der graue Punkt in der unteren Abbildung), welchen Rahmengr\u00f6\u00dfe k\u00f6nnte man dem Kunden empfehlen? Nun man k\u00f6nnte zun\u00e4chst auf die Idee kommen sich den Datenpunkt zu suchen, der vom gesuchten den geringsten Abstand hat. Also ben\u00f6tigen wir eine Funktion, die und die Entfernung der Datenpunkte bestimmt. \\[c = \\sqrt{a^2 + b^2}\\] bzw. \\[c = \\sqrt{(ax-px)^2 + (ay-py)^2}\\] Im n-dimensionalen Raum wird diese Abstand auch als Euklidische Distanz bezeichnet. F\u00fcr einen n-dimensionalen Raum sieht die Berechnung des Euklidische Distanz wir folgt aus: \\[ \\sqrt{\\sum_{i=1}^{n}(q_i-p_i)^2} \\] Hierbei beschreibt \\(n\\) die Anzahl der Dimensionen der Punkte \\(p\\) und \\(q\\) und \\(p_i\\) und \\(q_i\\) stellen die Werte der i-ten Dimension der jeweiligen Punkte dar. Entwickeln sie eine Funktion distance(ax,ay,px,py) , die die Entfernung zweier Punkte bestimmt. Iterieren Sie durch alle gegeben Datenpunkte und bestimmen Sie den Punkt, der vom gesuchten Punkt (K\u00f6rpergr\u00f6\u00dfe=163cm / Schrittl\u00e4nge=84 cm) den geringsten Abstand hat. Diskutieren Sie im Klassenverband, ob wir bereits ein geeignetes Vorhersagemodell gefunden haben? Und wie wir die Qualit\u00e4t unseres gefundenen Modells \u00fcberpr\u00fcfen k\u00f6nnen? Testen des Vorhersagemodells Wollen wir unsere Modell qualitativ \u00fcberpr\u00fcfen sollten wir zun\u00e4chst unseren Datenbestand teilen in Trainingsdaten und Testdaten. Anhand der Testdaten k\u00f6nnen wir nun \u00fcberpr\u00fcfen wie gut unser Modell ist. Erstellen Sie ein Programm, welches anhand der Testdaten Vorhersagen zur Rahmenh\u00f6he macht. Und dokumentieren Sie, wie of das Modell eine richtige Vorhersage macht und wie oft das Modell falsch liegt. L\u00f6sung import pandas as pd ok = 0 ; err = 0 for i in range ( 0 , len ( test )): d = pd . DataFrame ( columns = [ \"distance\" , \"Koerpergroesse\" , \"Schrittlaenge\" , \"Rahmenh\u00f6he\" ]) testData = test . iloc [ i ] for index , row in learn . iterrows (): d . loc [ len ( d )] = [ distance ( testData [ \"Koerpergroesse\" ], testData [ \"Schrittlaenge\" ], row [ \"Koerpergroesse\" ], row [ \"Schrittlaenge\" ]), row [ \"Koerpergroesse\" ], row [ \"Schrittlaenge\" ], row [ \"Rahmengroesse\" ]] d . sort_values ( by = [ \"distance\" ], inplace = True ) print ( \"Vorhersage \" + d . iloc [ 0 ][ \"Rahmenh\u00f6he\" ] + \" Ist \" + testData [ \"Rahmengroesse\" ]) if ( d . iloc [ 0 ][ \"Rahmenh\u00f6he\" ] == testData [ \"Rahmengroesse\" ]): print ( \"OK\" ) ok = ok + 1 else : print ( \"Fehler\" ) err = err + 1 print ( \"OK \" + str ( ok ) + \" Fehler \" + str ( err )) Andere k-Werte Bisher schauen und wir uns den n\u00e4chsten Nachbarn im Datenraum an ( \\(k=1\\) ). Doch wie w\u00fcrde sich die Qualit\u00e4t unseres Modells \u00e4ndern, wenn wir z.B. die n\u00e4chsten 3 Nachbarn ( \\(k=3\\) ) betrachten w\u00fcrden? Experimentieren Sie mit ihrem Modell mit anderen k-Werten und analysieren Sie die Qualit\u00e4t des Vorhersagemodells. Hinweis : Wenn wir einen anderen k-Wert als 1 verwenden, m\u00fcssen wir, nachdem wir das Ergebnis-Array [\"Entfernung\",\"Rahmengr\u00f6\u00dfe\"] sortiert haben heraus finden welche Klasse in den k-letzten Eintr\u00e4gen am meisten vorhanden ist! Metriken Metriken dienen dazu die Qualit\u00e4t von Modellen zu beurteilen. Eine erste einfache Metrik haben wir bereits angewendet, die Genauigkeit ( Accuracy ): Accuracy (Genauigkeit): Die Genauigkeit misst den Prozentsatz der korrekten Vorhersagen im Verh\u00e4ltnis zur Gesamtzahl der Vorhersagen. Es ist eine einfache und h\u00e4ufig verwendete Metrik, aber sie kann irref\u00fchrend sein, wenn die Daten ungleichm\u00e4\u00dfig verteilt sind. Accuracy = \\(\\frac{T_{correct}}{T_{correct} + F_{error}}\\) Dabei sind \\(T_{correct}\\) die Anzahl der richtigen Vorhersagen und \\(F_{error}\\) die Anzahl der falschen Vorhersagen. Neben dieser Metrik gibt es noch eine Vielzahl weiterer Metriken, die je nach Problemstellung eine bessere Aussage \u00fcber die Qualit\u00e4t des Vorhersagemodells liefern. Siehe folgende Abbildung. [^1] [^1]: vgl. https://en.wikipedia.org/wiki/Precision_and_recall Angenommen wir wollen einen Klassifizierer f\u00fcr Corona Tests entwickeln. So liegen alle Personen die Corona haben im gr\u00fcn dargestellten Bereich. Unser Klassifizierer (Test) kann jedoch nicht alle Corona positiven Personen erfassen und es gibt eine Reihe von Tests, die Corona negativ sind obwohl die Person Corona hat. Diese finden sich im Bereich false negative und werden als \\(F_N\\) bezeichnet. Erkennt unser Test eine Corona-Infektion so wird dieses als true positive bezeichnet und als \\(T_P\\) erfassen. In \u00e4hnlicher Weise k\u00f6nnte unser Test auch einen gesunden Menschen als an Corona erkrankt erfassen. Dieses w\u00fcrde als true negativ bezeichnen und als \\(T_N\\) erfasst. Bzw. wenn im optimalen Fall w\u00fcrde der Test keine Corona Infektion ausweisen und der Proband ist auch nicht an Corona erkrankt, so w\u00e4re der Test false positiv oder \\(F_P\\) . Daraus ergeben sich die Werte Precision und Recall Precision (Pr\u00e4zision) und Recall (R\u00fcckruf): Diese beiden Metriken beziehen sich auf das Verh\u00e4ltnis von korrekten positiven Vorhersagen zu allen tats\u00e4chlichen positiven Instanzen. Precision misst, wie oft das Modell richtig vorhergesagt hat, w\u00e4hrend Recall misst, wie viele der tats\u00e4chlichen positiven F\u00e4lle das Modell gefunden hat. Precision = \\(\\frac{TP}{TP + FP}\\) Recall = \\(\\frac{TP}{TP + FN}\\) Eine weitere Metrik ergibt sich aus diesen beiden Werten und wird als F1-Score bezeichnet. F1-Score: Der F1-Score ist das harmonische Mittel zwischen Pr\u00e4zision und R\u00fcckruf und gibt daher ein ausgewogenes Ma\u00df f\u00fcr die Leistung des Modells. F1 Score = \\(2*\\frac{precision* recall}{precision + recall}\\) F\u00fcr unser Problem der Bestimmung der optimalen Rahmenh\u00f6he m\u00fcssen wir die Gr\u00f6\u00dfen Precision und Recall pro Klasse betrachten. Dabei haben wir 4 Klassen (S,M,L,XL). Angenommen unser Modell w\u00fcrde folgende Vorhersagen treffen (dabei sind die gelb markierten Felder die richtigen Gr\u00f6\u00dfen und die blau markierten Felder die vorhergesagten Gr\u00f6\u00dfen). Wie wir sehen macht das Modell bei der Vorhersage der Rahmengr\u00f6\u00dfe \"L\" drei Fehler. Bestimmen Sie f\u00fcr die vier Klassen S,M,L und XL jeweils die Precision und den Recall Wert. L\u00f6sung Metriken \\(P_S=\\frac{TP}{TP + FP}=\\frac{4}{4 + 0}=1\\) \\(P_M=\\frac{TP}{TP + FP}=\\frac{1}{1 + 1}=0.5\\) \\(P_L=\\frac{TP}{TP + FP}=\\frac{2}{2 + 0}=1\\) \\(P_{XL}=\\frac{TP}{TP + FP}=\\frac{6}{6 + 2}=0.75\\) \\(R_S=\\frac{TP}{TP + FN}=\\frac{4}{4 + 0}=1\\) \\(R_M=\\frac{TP}{TP + FN}=\\frac{1}{1 + 0}=1\\) \\(R_L=\\frac{TP}{TP + FN}=\\frac{2}{2 + 3}=0.4\\) \\(R_{XL}=\\frac{TP}{TP + FN}=\\frac{6}{6 + 0}=1\\) \u00dcbungsaufgabe Metriken Ein Klassifizierungsalgorithmus ist in der Lage unterschiedliche Getreidearten wie (W)eizen, (G)erste und (R)oggen zu klassifizieren. Der Algorithmus wurde mit einem Testdatensatz validiert und es kam zu folgendem Ergebnis: Vorhersage Ist W W W W G W G G R R W W R W G G R R R W R R G G G R W R G G Bestimmen Sie f\u00fcr den Algorithmus die Accuracy , Precision und den Recall -Wert. L\u00f6sung \u00dcbung Metriken \\(W_v\\) \\(G_v\\) \\(R_v\\) \\(W_i\\) 3 1 2 \\(G_i\\) 4 \\(R_i\\) 1 1 3 Das ist \\(G_v\\) z.B. der vorhergesagte Weizenwert und \\(G_i\\) der Istwert. F\u00fcr die Accuracy gilt: \\(A=\\frac{T_{correct}}{T_{correct} + F_{error}}=\\frac{10}{10 + 5}=67 \\%\\) F\u00fcr die Precision gilt: \\(P_W=\\frac{T_P}{T_P + F_P}=\\frac{3}{4}=75 \\%\\) \\(P_G=\\frac{T_P}{T_P + F_P}=\\frac{4}{6}=67 \\%\\) \\(P_R=\\frac{T_P}{T_P + F_P}=\\frac{3}{5}=60 \\%\\) F\u00fcr den Recall gilt: \\(R_W=\\frac{T_P}{T_P + F_N}=\\frac{3}{3 + 3}= 50 \\%\\) \\(R_G=\\frac{T_P}{T_P + F_N}=\\frac{4}{4 + 0}= 100 \\%\\) \\(R_R=\\frac{T_P}{T_P + F_N}=\\frac{3}{3 + 2}= 60 \\%\\) Nutzen von Bibliotheken Aus dem Paket sklearn.neighbours kann der KNeighborsClassifier genutzt werden um eine KNN Klassifizierung zu erstellen. import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier # Laden Sie die CSV-Datei in ein Pandas DataFrame. df = pd . read_csv ( 'Rahmenhoehe.csv' ) # Aufteilen der Features und Labels X = df [[ 'Koerpergroesse' , 'Schrittlaenge' ]] y = df [ 'Rahmengroesse' ] # Teilen Sie die Daten in Trainings- und Testdaten auf. X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Skalieren Sie die Merkmale. scaler = StandardScaler () X_train = scaler . fit_transform ( X_train ) X_test = scaler . transform ( X_test ) # Erstellen und Trainieren des KNN-Modells mit k=3. knn = KNeighborsClassifier ( n_neighbors = 3 ) knn . fit ( X_train , y_train ) # Verwenden Sie das trainierte Modell, um Vorhersagen f\u00fcr die Testdaten zu treffen. y_pred = knn . predict ( X_test ) Nutzen Sie die Klasse KNeighborsClassifier und \u00fcberpr\u00fcfen Sie das Modell f\u00fcr verschiedene Werte von k . Bestimmung der Metriken Auch die Metriken k\u00f6nnen mit Hilfe des Paketes sklearn.metrics bestimmt werden. Bestimmen Sie mit Hilfe des unten abgebildeten Programmcodes die Qualit\u00e4t ihres Vorhersagemodells. Hinweis : Die Klasse werden dabei in der Reihenfolge ihres Auftretens in den Testdaten gebildet. from sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score # Berechnen Sie die Vorhersagen f\u00fcr die Testdaten. y_pred = knn . predict ( X_test ) # Reihenfolge der Klassenlabels abrufen class_order = knn . classes_ # Drucken der Reihenfolge der Klassenlabels print ( 'Klassenreihenfolge:' , class_order ) # Berechnen Sie die Accuracy accuracy = accuracy_score ( y_test , y_pred ) print ( 'Accuracy:' , accuracy ) # Berechnen Sie die Precision precision = precision_score ( y_test , y_pred , average = None ) print ( 'Precision:' , precision ) # Berechnen Sie den Recall recall = recall_score ( y_test , y_pred , average = None ) print ( 'Recall:' , recall ) # Berechnen Sie den F1-Score f1 = f1_score ( y_test , y_pred , average = None ) print ( 'F1-Score:' , f1 ) Fragen zum Kapitel 1.) Wie funktioniert KNN im Allgemeinen? [ ] Es speichert alle vorhandenen Daten und sucht nach \u00e4hnlichen Mustern [ ] Es berechnet die Distanz zwischen Punkten und w\u00e4hlt die k n\u00e4chsten Nachbarn aus [ ] Es gibt eine Liste von Regeln zur\u00fcck, um eine Entscheidung zu treffen [ ] Es gibt nur bin\u00e4re Antworten zur\u00fcck 2.) Warum wird der Parameter \"k\" in KNN verwendet? [ ] Um die Anzahl der Dateneingaben zu begrenzen [ ] Um die Genauigkeit des Modells einzuschr\u00e4nken [ ] Um zu bestimmen, wie viele Nachbarn einbezogen werden sollen [ ] Um das Modell einfacher zu gestalten 3.) Welche Art von Problemen kann KNN gut l\u00f6sen? [ ] Probleme mit nur zwei Variablen [ ] Probleme mit vielen Variablen und komplexen Mustererkennungen [ ] Nur einfache lineare Probleme [ ] Probleme, bei denen das Ergebnis auf bin\u00e4rer Logik basiert 4.) Welche Metrik wird zur Bewertung eines KNN-Modells verwendet, das f\u00fcr eine bin\u00e4re Klassifikation eingesetzt wird? [ ] F1-Score [ ] R2-Score [ ] Adjusted R-Squared [ ] Silhouette Coefficient 5.) Welche Python-Bibliothek kann verwendet werden, um KNN in Python zu implementieren? [ ] TensorFlow [ ] PyTorch [ ] Scikit-Learn (sklearn) [ ] Theano","title":"KNN - K Nearest Neighbor"},{"location":"knn/#knn-k-nearest-neighbor","text":"","title":"KNN - K Nearest Neighbor"},{"location":"knn/#handlungssituation","text":"Ein Online Fahrradh\u00e4ndler m\u00f6chte seinen Kunden stets Fahrr\u00e4der in der optimale Rahmenh\u00f6he anbieten. Dazu soll eine App inkl. Vorhersagemodell entwickelt werden, so dass der Kunde lediglich ein Foto aufnehmen muss und ihm die optimale Rahmenh\u00f6he empfohlen wird. Eine Abteilung der ChangeIT GmbH ist bereits damit beauftragt aus einem Bild die Schrittl\u00e4nge und die K\u00f6rpergr\u00f6\u00dfe (jeweils in cm) zu ermitteln. Sie sollen ein Vorhersagemodell entwickeln, welches dem Kunden die richtige Rahmengr\u00f6\u00dfe vorschl\u00e4gt. Der Online Fahrradh\u00e4ndler hat bereits Daten in Form von Erfahrungswerten vorliegen und stellt ihnen diese in Form einer CSV Datei zur Verf\u00fcgung.","title":"Handlungssituation"},{"location":"knn/#exploration-der-daten","text":"Stelle Sie zun\u00e4chst die Daten, die Sie vom Online H\u00e4ndler erhalten haben grafisch dar ( CSV Datei ). Zur besseren Unterscheidung der Kategorien (Klassen) sollte jede Rahmenh\u00f6he (S,M,L,XL) in einer anderen Farbe dargestellt werden.","title":"Exploration der Daten"},{"location":"knn/#erste-uberlegungen-zum-vorhersagemodell","text":"Angenommen ein Kunde wird ausgemessen und es ergibt sich eine K\u00f6rpergr\u00f6\u00dfe von 163cm und eine Schrittl\u00e4nge von 84cm (der graue Punkt in der unteren Abbildung), welchen Rahmengr\u00f6\u00dfe k\u00f6nnte man dem Kunden empfehlen? Nun man k\u00f6nnte zun\u00e4chst auf die Idee kommen sich den Datenpunkt zu suchen, der vom gesuchten den geringsten Abstand hat. Also ben\u00f6tigen wir eine Funktion, die und die Entfernung der Datenpunkte bestimmt. \\[c = \\sqrt{a^2 + b^2}\\] bzw. \\[c = \\sqrt{(ax-px)^2 + (ay-py)^2}\\] Im n-dimensionalen Raum wird diese Abstand auch als Euklidische Distanz bezeichnet. F\u00fcr einen n-dimensionalen Raum sieht die Berechnung des Euklidische Distanz wir folgt aus: \\[ \\sqrt{\\sum_{i=1}^{n}(q_i-p_i)^2} \\] Hierbei beschreibt \\(n\\) die Anzahl der Dimensionen der Punkte \\(p\\) und \\(q\\) und \\(p_i\\) und \\(q_i\\) stellen die Werte der i-ten Dimension der jeweiligen Punkte dar. Entwickeln sie eine Funktion distance(ax,ay,px,py) , die die Entfernung zweier Punkte bestimmt. Iterieren Sie durch alle gegeben Datenpunkte und bestimmen Sie den Punkt, der vom gesuchten Punkt (K\u00f6rpergr\u00f6\u00dfe=163cm / Schrittl\u00e4nge=84 cm) den geringsten Abstand hat. Diskutieren Sie im Klassenverband, ob wir bereits ein geeignetes Vorhersagemodell gefunden haben? Und wie wir die Qualit\u00e4t unseres gefundenen Modells \u00fcberpr\u00fcfen k\u00f6nnen?","title":"Erste \u00dcberlegungen zum Vorhersagemodell"},{"location":"knn/#testen-des-vorhersagemodells","text":"Wollen wir unsere Modell qualitativ \u00fcberpr\u00fcfen sollten wir zun\u00e4chst unseren Datenbestand teilen in Trainingsdaten und Testdaten. Anhand der Testdaten k\u00f6nnen wir nun \u00fcberpr\u00fcfen wie gut unser Modell ist. Erstellen Sie ein Programm, welches anhand der Testdaten Vorhersagen zur Rahmenh\u00f6he macht. Und dokumentieren Sie, wie of das Modell eine richtige Vorhersage macht und wie oft das Modell falsch liegt.","title":"Testen des Vorhersagemodells"},{"location":"knn/#losung","text":"import pandas as pd ok = 0 ; err = 0 for i in range ( 0 , len ( test )): d = pd . DataFrame ( columns = [ \"distance\" , \"Koerpergroesse\" , \"Schrittlaenge\" , \"Rahmenh\u00f6he\" ]) testData = test . iloc [ i ] for index , row in learn . iterrows (): d . loc [ len ( d )] = [ distance ( testData [ \"Koerpergroesse\" ], testData [ \"Schrittlaenge\" ], row [ \"Koerpergroesse\" ], row [ \"Schrittlaenge\" ]), row [ \"Koerpergroesse\" ], row [ \"Schrittlaenge\" ], row [ \"Rahmengroesse\" ]] d . sort_values ( by = [ \"distance\" ], inplace = True ) print ( \"Vorhersage \" + d . iloc [ 0 ][ \"Rahmenh\u00f6he\" ] + \" Ist \" + testData [ \"Rahmengroesse\" ]) if ( d . iloc [ 0 ][ \"Rahmenh\u00f6he\" ] == testData [ \"Rahmengroesse\" ]): print ( \"OK\" ) ok = ok + 1 else : print ( \"Fehler\" ) err = err + 1 print ( \"OK \" + str ( ok ) + \" Fehler \" + str ( err ))","title":"L\u00f6sung"},{"location":"knn/#andere-k-werte","text":"Bisher schauen und wir uns den n\u00e4chsten Nachbarn im Datenraum an ( \\(k=1\\) ). Doch wie w\u00fcrde sich die Qualit\u00e4t unseres Modells \u00e4ndern, wenn wir z.B. die n\u00e4chsten 3 Nachbarn ( \\(k=3\\) ) betrachten w\u00fcrden? Experimentieren Sie mit ihrem Modell mit anderen k-Werten und analysieren Sie die Qualit\u00e4t des Vorhersagemodells. Hinweis : Wenn wir einen anderen k-Wert als 1 verwenden, m\u00fcssen wir, nachdem wir das Ergebnis-Array [\"Entfernung\",\"Rahmengr\u00f6\u00dfe\"] sortiert haben heraus finden welche Klasse in den k-letzten Eintr\u00e4gen am meisten vorhanden ist!","title":"Andere k-Werte"},{"location":"knn/#metriken","text":"Metriken dienen dazu die Qualit\u00e4t von Modellen zu beurteilen. Eine erste einfache Metrik haben wir bereits angewendet, die Genauigkeit ( Accuracy ): Accuracy (Genauigkeit): Die Genauigkeit misst den Prozentsatz der korrekten Vorhersagen im Verh\u00e4ltnis zur Gesamtzahl der Vorhersagen. Es ist eine einfache und h\u00e4ufig verwendete Metrik, aber sie kann irref\u00fchrend sein, wenn die Daten ungleichm\u00e4\u00dfig verteilt sind. Accuracy = \\(\\frac{T_{correct}}{T_{correct} + F_{error}}\\) Dabei sind \\(T_{correct}\\) die Anzahl der richtigen Vorhersagen und \\(F_{error}\\) die Anzahl der falschen Vorhersagen. Neben dieser Metrik gibt es noch eine Vielzahl weiterer Metriken, die je nach Problemstellung eine bessere Aussage \u00fcber die Qualit\u00e4t des Vorhersagemodells liefern. Siehe folgende Abbildung. [^1] [^1]: vgl. https://en.wikipedia.org/wiki/Precision_and_recall Angenommen wir wollen einen Klassifizierer f\u00fcr Corona Tests entwickeln. So liegen alle Personen die Corona haben im gr\u00fcn dargestellten Bereich. Unser Klassifizierer (Test) kann jedoch nicht alle Corona positiven Personen erfassen und es gibt eine Reihe von Tests, die Corona negativ sind obwohl die Person Corona hat. Diese finden sich im Bereich false negative und werden als \\(F_N\\) bezeichnet. Erkennt unser Test eine Corona-Infektion so wird dieses als true positive bezeichnet und als \\(T_P\\) erfassen. In \u00e4hnlicher Weise k\u00f6nnte unser Test auch einen gesunden Menschen als an Corona erkrankt erfassen. Dieses w\u00fcrde als true negativ bezeichnen und als \\(T_N\\) erfasst. Bzw. wenn im optimalen Fall w\u00fcrde der Test keine Corona Infektion ausweisen und der Proband ist auch nicht an Corona erkrankt, so w\u00e4re der Test false positiv oder \\(F_P\\) . Daraus ergeben sich die Werte Precision und Recall Precision (Pr\u00e4zision) und Recall (R\u00fcckruf): Diese beiden Metriken beziehen sich auf das Verh\u00e4ltnis von korrekten positiven Vorhersagen zu allen tats\u00e4chlichen positiven Instanzen. Precision misst, wie oft das Modell richtig vorhergesagt hat, w\u00e4hrend Recall misst, wie viele der tats\u00e4chlichen positiven F\u00e4lle das Modell gefunden hat. Precision = \\(\\frac{TP}{TP + FP}\\) Recall = \\(\\frac{TP}{TP + FN}\\) Eine weitere Metrik ergibt sich aus diesen beiden Werten und wird als F1-Score bezeichnet. F1-Score: Der F1-Score ist das harmonische Mittel zwischen Pr\u00e4zision und R\u00fcckruf und gibt daher ein ausgewogenes Ma\u00df f\u00fcr die Leistung des Modells. F1 Score = \\(2*\\frac{precision* recall}{precision + recall}\\) F\u00fcr unser Problem der Bestimmung der optimalen Rahmenh\u00f6he m\u00fcssen wir die Gr\u00f6\u00dfen Precision und Recall pro Klasse betrachten. Dabei haben wir 4 Klassen (S,M,L,XL). Angenommen unser Modell w\u00fcrde folgende Vorhersagen treffen (dabei sind die gelb markierten Felder die richtigen Gr\u00f6\u00dfen und die blau markierten Felder die vorhergesagten Gr\u00f6\u00dfen). Wie wir sehen macht das Modell bei der Vorhersage der Rahmengr\u00f6\u00dfe \"L\" drei Fehler. Bestimmen Sie f\u00fcr die vier Klassen S,M,L und XL jeweils die Precision und den Recall Wert.","title":"Metriken"},{"location":"knn/#losung-metriken","text":"\\(P_S=\\frac{TP}{TP + FP}=\\frac{4}{4 + 0}=1\\) \\(P_M=\\frac{TP}{TP + FP}=\\frac{1}{1 + 1}=0.5\\) \\(P_L=\\frac{TP}{TP + FP}=\\frac{2}{2 + 0}=1\\) \\(P_{XL}=\\frac{TP}{TP + FP}=\\frac{6}{6 + 2}=0.75\\) \\(R_S=\\frac{TP}{TP + FN}=\\frac{4}{4 + 0}=1\\) \\(R_M=\\frac{TP}{TP + FN}=\\frac{1}{1 + 0}=1\\) \\(R_L=\\frac{TP}{TP + FN}=\\frac{2}{2 + 3}=0.4\\) \\(R_{XL}=\\frac{TP}{TP + FN}=\\frac{6}{6 + 0}=1\\)","title":"L\u00f6sung Metriken"},{"location":"knn/#ubungsaufgabe-metriken","text":"Ein Klassifizierungsalgorithmus ist in der Lage unterschiedliche Getreidearten wie (W)eizen, (G)erste und (R)oggen zu klassifizieren. Der Algorithmus wurde mit einem Testdatensatz validiert und es kam zu folgendem Ergebnis: Vorhersage Ist W W W W G W G G R R W W R W G G R R R W R R G G G R W R G G Bestimmen Sie f\u00fcr den Algorithmus die Accuracy , Precision und den Recall -Wert.","title":"\u00dcbungsaufgabe Metriken"},{"location":"knn/#losung-ubung-metriken","text":"\\(W_v\\) \\(G_v\\) \\(R_v\\) \\(W_i\\) 3 1 2 \\(G_i\\) 4 \\(R_i\\) 1 1 3 Das ist \\(G_v\\) z.B. der vorhergesagte Weizenwert und \\(G_i\\) der Istwert. F\u00fcr die Accuracy gilt: \\(A=\\frac{T_{correct}}{T_{correct} + F_{error}}=\\frac{10}{10 + 5}=67 \\%\\) F\u00fcr die Precision gilt: \\(P_W=\\frac{T_P}{T_P + F_P}=\\frac{3}{4}=75 \\%\\) \\(P_G=\\frac{T_P}{T_P + F_P}=\\frac{4}{6}=67 \\%\\) \\(P_R=\\frac{T_P}{T_P + F_P}=\\frac{3}{5}=60 \\%\\) F\u00fcr den Recall gilt: \\(R_W=\\frac{T_P}{T_P + F_N}=\\frac{3}{3 + 3}= 50 \\%\\) \\(R_G=\\frac{T_P}{T_P + F_N}=\\frac{4}{4 + 0}= 100 \\%\\) \\(R_R=\\frac{T_P}{T_P + F_N}=\\frac{3}{3 + 2}= 60 \\%\\)","title":"L\u00f6sung \u00dcbung Metriken"},{"location":"knn/#nutzen-von-bibliotheken","text":"Aus dem Paket sklearn.neighbours kann der KNeighborsClassifier genutzt werden um eine KNN Klassifizierung zu erstellen. import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier # Laden Sie die CSV-Datei in ein Pandas DataFrame. df = pd . read_csv ( 'Rahmenhoehe.csv' ) # Aufteilen der Features und Labels X = df [[ 'Koerpergroesse' , 'Schrittlaenge' ]] y = df [ 'Rahmengroesse' ] # Teilen Sie die Daten in Trainings- und Testdaten auf. X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Skalieren Sie die Merkmale. scaler = StandardScaler () X_train = scaler . fit_transform ( X_train ) X_test = scaler . transform ( X_test ) # Erstellen und Trainieren des KNN-Modells mit k=3. knn = KNeighborsClassifier ( n_neighbors = 3 ) knn . fit ( X_train , y_train ) # Verwenden Sie das trainierte Modell, um Vorhersagen f\u00fcr die Testdaten zu treffen. y_pred = knn . predict ( X_test ) Nutzen Sie die Klasse KNeighborsClassifier und \u00fcberpr\u00fcfen Sie das Modell f\u00fcr verschiedene Werte von k .","title":"Nutzen von Bibliotheken"},{"location":"knn/#bestimmung-der-metriken","text":"Auch die Metriken k\u00f6nnen mit Hilfe des Paketes sklearn.metrics bestimmt werden. Bestimmen Sie mit Hilfe des unten abgebildeten Programmcodes die Qualit\u00e4t ihres Vorhersagemodells. Hinweis : Die Klasse werden dabei in der Reihenfolge ihres Auftretens in den Testdaten gebildet. from sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score # Berechnen Sie die Vorhersagen f\u00fcr die Testdaten. y_pred = knn . predict ( X_test ) # Reihenfolge der Klassenlabels abrufen class_order = knn . classes_ # Drucken der Reihenfolge der Klassenlabels print ( 'Klassenreihenfolge:' , class_order ) # Berechnen Sie die Accuracy accuracy = accuracy_score ( y_test , y_pred ) print ( 'Accuracy:' , accuracy ) # Berechnen Sie die Precision precision = precision_score ( y_test , y_pred , average = None ) print ( 'Precision:' , precision ) # Berechnen Sie den Recall recall = recall_score ( y_test , y_pred , average = None ) print ( 'Recall:' , recall ) # Berechnen Sie den F1-Score f1 = f1_score ( y_test , y_pred , average = None ) print ( 'F1-Score:' , f1 )","title":"Bestimmung der Metriken"},{"location":"knn/#fragen-zum-kapitel","text":"1.) Wie funktioniert KNN im Allgemeinen? [ ] Es speichert alle vorhandenen Daten und sucht nach \u00e4hnlichen Mustern [ ] Es berechnet die Distanz zwischen Punkten und w\u00e4hlt die k n\u00e4chsten Nachbarn aus [ ] Es gibt eine Liste von Regeln zur\u00fcck, um eine Entscheidung zu treffen [ ] Es gibt nur bin\u00e4re Antworten zur\u00fcck 2.) Warum wird der Parameter \"k\" in KNN verwendet? [ ] Um die Anzahl der Dateneingaben zu begrenzen [ ] Um die Genauigkeit des Modells einzuschr\u00e4nken [ ] Um zu bestimmen, wie viele Nachbarn einbezogen werden sollen [ ] Um das Modell einfacher zu gestalten 3.) Welche Art von Problemen kann KNN gut l\u00f6sen? [ ] Probleme mit nur zwei Variablen [ ] Probleme mit vielen Variablen und komplexen Mustererkennungen [ ] Nur einfache lineare Probleme [ ] Probleme, bei denen das Ergebnis auf bin\u00e4rer Logik basiert 4.) Welche Metrik wird zur Bewertung eines KNN-Modells verwendet, das f\u00fcr eine bin\u00e4re Klassifikation eingesetzt wird? [ ] F1-Score [ ] R2-Score [ ] Adjusted R-Squared [ ] Silhouette Coefficient 5.) Welche Python-Bibliothek kann verwendet werden, um KNN in Python zu implementieren? [ ] TensorFlow [ ] PyTorch [ ] Scikit-Learn (sklearn) [ ] Theano","title":"Fragen zum Kapitel"},{"location":"neuronalesNetz/","text":"Neuronale Netze Handlungssituation Die Firma Home-IoT ist eine bekannter Hersteller von Smart Home Produkten. Es ist geplant f\u00fcr diese Firma eine smarte Lichtsteuerung \"AI Light\" zu entwickeln, die an die jeweiligen Anforderungen der Kunden angepasst werden kann. Der Chefentwickler der Abteilung Daten- und Prozessanalyse der ChangeIT GmbH beauftragt Sie damit ein Neuronalen Netz zu entwickeln und dieses f\u00fcr eine exemplarische Anforderung zu trainieren. AI-Light Die smarte Lichtsteuerung \"AI-Light\" besitzt zwei Sensoren. Pr\u00e4senzerkennung: \u00dcber Sensoren ist das System in der Lage zu erkennen, ob sich Personen im Raum befinden. Tag / Nachterkennung: Das System ist ebenso in der Lage, Tag- / Nachtzeiten zu erkennen. Kundenanforderungen Die Kunden stellen dabei unterschiedliche Anforderungen an das System vlg. [^1]. [^1]: Brandt, Y., Eickhoff-Schachtebeck, A. und Strecker, K. (2022) \u201eSchulbuch starkeSeiten Informatik Jahrgang 9/10 Gymnasium Niedersachsen\u201c, Klett-Verlag 2022, ISBN: 978-3-12-007572-1 In den B\u00fcros der Fabrikhalle sollen die Lampen nachts immer leuchten aus Gr\u00fcnden des Einbrecherschutzes, und tags\u00fcber nur, wenn die Mitarbeiter an ihren Pl\u00e4tzen sind. Im B\u00fcrogeb\u00e4ude der Softwarefirma sollen die Lampen nur nachts leuchten, wenn Mitarbeiter da sind. Tags\u00fcber ist es durch die vielen Fenster immer hell genug. Im alten Geb\u00e4ude der Stadtverwaltung m\u00fcssen die Lampen tags\u00fcber angeschaltet sein, wenn Mitarbeiter da sind, da die Fenster zu wenig Licht hereinlassen. Sollten Mitarbeiter auch nachts arbeiten, m\u00fcssen auch dann die Lampen eingeschaltet werden. Sonst k\u00f6nnen sie aus bleiben. Im Haus der Familie Schmidt sollen die Lampen tags\u00fcber an sein, wenn jemand zuhause ist, und aus sein, wenn keiner da ist. Nachts sollen die Lampen aus sein, wenn die Bewohner im Haus sind und schlafen, und aus Gr\u00fcnden des Einbrecherschutzes an sein, wenn niemand da ist Aufgabe Wahrheitstabelle erzeugen W\u00e4hlen Sie sich eine Anforderung des Kunden aus und erstellen Sie eine Wahrheitstabelle, die alle m\u00f6glichen Eingangssignale darstellt und ob sich die Lampe in der jeweiligen Situation an oder ausgehen soll. L\u00f6sung Wahrheitstabelle erzeugen F\u00fcr ein B\u00fcro in der Fabrikhalle k\u00f6nnte diese Tabelle wie folgt aussehen. Tag / Nacht ( \\(X_1\\) ) Person ( \\(X_2\\) ) Lampe ( \\(Y\\) ) Tag (1) nein (0) aus (0) Tag (1) ja (1) an (1) Nacht (0) nein (0) an (1) Nacht (0) ja (1) an (1) Das neuronale Netz Ein neuronales Netzwerk ist ein Modell, das von der Funktionsweise des menschlichen Gehirns inspiriert ist. Es besteht aus einer Sammlung miteinander verbundener k\u00fcnstlicher Neuronen, die Informationen verarbeiten und weiterleiten. \u00c4hnlich wie biologische Neuronen empfangen auch k\u00fcnstliche Neuronen Eingaben, verarbeiten diese und geben sie als Ausgabe weiter. Die Eingaben werden gewichtet und durch Aktivierungsfunktionen in eine Ausgabe transformiert. Ein einfaches Neuron (man spricht hier auch von einem Perzeptron ) kann dabei wie folgt aussehen. Die Werte \\(X_1\\) bis \\(X_3\\) sind z.B. Sensorwerte oder Werte aus einer vorherigen Stufe. \\(G_1\\) bis \\(G_3\\) sind Gewichtungsfaktoren die im laufe des Trainings des Neuronalen Netzes angepasst werden und zur Initialisierung auf zuf\u00e4llige Werte gesetzt werden. Als Aktivierungsfunktionen k\u00f6nnen Funktionen genutzt werden, wie die Sigmoid-, ReLu- oder Tanh-Funktion bzw. einfache Schwellwerte. Die Funktion gibt an, ob und wie stark das Neuron \"feuert\". Aufgabe : Berechnen Sie den Ausgabewert \\(Y\\) f\u00fcr das Neuron, wenn als Aktivierungsfunktion ein Tanh-Funktion genutzt wird und folgende Eingangsvektoren und Gewichtsvektoren vorliegen. \\(G = \\begin{pmatrix} G_1 \\\\ G_2 \\\\ G_3 \\end{pmatrix}=\\begin{pmatrix} 0.4 \\\\ 0.2 \\\\ -0.5 \\end{pmatrix}\\) \\(X = \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix}=\\begin{pmatrix} 0.7 \\\\ -0.1 \\\\ -0.4 \\end{pmatrix}\\) L\u00f6sung : \\(Y_*= X_1*G_1+X_2*G_2+X_3*G_3\\) \\(Y_*=0.4*0.7+0.2*(-0.1)+(-0.5)*(-0.4)=0.46\\) \\(Y=tanh(0.46)=0.43\\) Diese Ausgaben k\u00f6nnen dann wieder als Eingaben f\u00fcr andere Neuronen dienen, wodurch das Netzwerk Schicht f\u00fcr Schicht komplexere Berechnungen durchf\u00fchren kann. Ein neuronales Netzwerk lernt, indem es seine Gewichte anpasst, basierend auf dem Vergleich zwischen seinen Ausgaben und den erwarteten Ausgaben. Dieser Lernprozess wird durch mathematische Algorithmen unterst\u00fctzt, die als Backpropagation bezeichnet werden. Durch wiederholtes Training auf gro\u00dfen Datens\u00e4tzen kann ein neuronales Netzwerk Muster erkennen, Zusammenh\u00e4nge verstehen und Vorhersagen treffen. Obwohl neuronale Netzwerke nicht genau die gleiche Funktionsweise wie biologische Neuronen haben, sind sie dennoch stark von ihnen inspiriert. Die Idee besteht darin, komplexe Informationsverarbeitung nach dem Vorbild des Gehirns zu erm\u00f6glichen und dadurch komplexe Aufgaben in Bereichen wie Bilderkennung, Spracherkennung, Textanalyse und vielem mehr zu l\u00f6sen. F\u00fcr die Steuerung der Lichtanlage ben\u00f6tigen wir pro Sensors ein Neuron, in diesem Fall w\u00e4re das demnach zwei Neuronen in der Eingangsschicht ( input Layer ). Die Ausgangsschicht ( output layer ) steuert mir einem Neuron die Lampe. F\u00fcr einen ersten Ansatz w\u00e4hlen wir 3 Neuronen in der hidden Layer . Dementsprechend hat unser Neuronales Netz folgendes Aussehen. . Initialisierung des Netzes Die Gewichte \\(W_{11}\\) bis \\(W_{32}\\) , sowie \\(W_4\\) bis \\(W_5\\) werden initial auf zuf\u00e4llige Werte zwischen +1 / -1 gesetzt. \\(W_1 = \\begin{pmatrix} w_{11} & w_{21} & w_{31} \\\\ w_{12} & w_{22} & w_{32} \\end{pmatrix}== \\begin{pmatrix} -0.19 & -0.96 & 0.43 \\\\ -0.23 & 0.97 & 0.46 \\end{pmatrix}\\) \\(W_2 = \\begin{pmatrix} w_4 \\\\ w_5 \\\\ w_6 \\end{pmatrix}=\\begin{pmatrix} -1.0 \\\\ -0.21 \\\\ 0.16 \\end{pmatrix}\\) Als Aktivierungsfunktion \\(f(x)\\) k\u00f6nnen unterschiedliche Funktionen wie die Sigmoid-, ReLu- oder Tanh-Funktion genutzt werden. Wie verwenden in diesem Beispiel die \\(tanh\\) Funktion im hidden Layer und die Sigmoid Funktion in der Ausgabeschicht. Die Sigmoid Funktion ist dabei wie folgt definiert. \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) Hier sind beide Funktion nochmals grafisch dargestellt. Forward Propagation Um nun den Wert der Zwischenschicht \\(O\\) zu ermitteln m\u00fcssen wir folgende Rechnungen durchf\u00fchren. \\(O_1= tanh((X_1*W_{11}+X_2*W_{12})+b_1)\\) \\(O_2= tanh((X_1*W_{21}+X_2*W_{22})+b_2)\\) \\(O_3= tanh((X_1*W_{31}+X_2*W_{32})+b_3)\\) Im ersten Durchgang nehmen wird den X-Vektor wie folgt an: \\(X = \\begin{pmatrix} X_1 & X_2 \\end{pmatrix}=\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\) Die Bias Werte legen wir zun\u00e4chst auf 1 fest. \\(B_1 = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{pmatrix}=\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\) \\(B_4 = 1\\) Mit den angenommen Werten kann nun weiter gerechnet werden: \\(O_1= tanh((X_1*W_{11}+X_2*W_{12})+b_1)= tanh((1*-0.19+0*-0.23)+1)=0.6696\\) \\(O_2= tanh((X_1*W_{21}+X_2*W_{22})+b_2) = tanh((1*-0.96+0*0.97)+1)=0.0006\\) \\(O_3= tanh((X_1*W_{31}+X_2*W_{32})+b_3) = tanh((1*-0.43+0*0.46)+1)=0.5154\\) F\u00fcr die output Layer ergeben sich folgende Werte. \\(O_4'= (O_1*W_4+O_2*W_5+O_3*W_6)+b_4\\) \\(O_4' = (0.6696*-1.0+0.0006*-0.21+0.5154*0.16)+1=0.4127\\) \\(O_4 = Y'= \\frac{1}{1 + e^{-O_4'}}=0.6017\\) Als Ergebnis w\u00fcrde das Neuronale Netz also die Lampe einschalten, was leider falsch w\u00e4re, denn wir hatten ja angegeben, dass es Nacht ist (1) und keine Person anwesend ist (0). Fehlerfunktionen (Loss-Function) Unser Netzwerk hat also einen Fehler gemacht, dieser kann z.B. wie folgt bestimmt werden: \\(E(Y')=Y-Y'=0-0.6017=-0.6017=-60.17\\%\\) Eine andere (besser geeignete) Fehlerfunktion ist der Binary Cross Entropy Error . Dieser ist wie folgt bestimmt: \\(E(Y')=-(Y*ln(Y')+(1-Y)*ln(1-Y'))\\) F\u00fcr unsere Beispiel w\u00fcrde sich also folgender Binary Cross Entropy Error ergeben: \\(E(Y')=-(0*ln(0.6017)+(1-0)*ln(1-0.6017))=0.9205\\) Neben dieser Fehlerfunktion gibt es noch weitere, wie z.B. Categorical Cross Entropy , Mean Squared Error und Cosine Distance . F\u00fcr unsere bin\u00e4res Klassifizierungsproblem (Lampe an/aus) eignet sich jedoch am besten die zuvor berechnete Binary Cross Entropy Error. Back Propagation Damit das Modell nun lernen kann (also sich der Fehler minimiert), m\u00fcssen die Gewichte und die Bias Werte angepasst werden. Dieses geschieht in der Back-Propagation. Neben dem Cross Entropy Error definieren wir noch einen weiteren Parameter, die Lernrate LearningRate . Sie zumeist mit \\(\\alpha\\) bezeichnet und liegt auf einem Wert z.B. \\(\\alpha=0.1\\) . Die LearningRate ist dabei nur ein Faktor um wie viel die Gewichte und Bias Werte angepasst werden, also wie schnell das Neuronale Netz lernen soll. In der Back Propagation geht es nun darum die Loss Funktion nach allen Gewichten und Bias Werten abzuleiten. Durch das Anwenden der Kettenregel kann der Rechenaufwand jedoch reduziert werden. Wir wollen uns nur auf einen Teil unseres Netzwerkes konzentrieren und \"trainieren\" jetzt den Parameter \\(W_4\\) . . \\(\\frac{\\delta E(Y')} {\\delta W_4}=\\frac {\\delta E(Y')} {\\delta Y'}* \\frac {\\delta Y'}{\\delta \\Theta}*\\frac {\\delta \\Theta}{\\delta W_4}\\) Dabei ist: \\(\\frac{\\delta E(Y')} {\\delta W_4}\\) die Partielle Ableitung der Loss Funktion nach dem Gewicht \\(W_4\\) . \\(\\frac {\\delta E(Y')} {\\delta Y'}\\) Ableitung der Loss Funktion nach Y' \\(\\frac {\\delta Y'}{\\delta \\Theta}\\) Ableitung von Y' nach \\(\\Theta\\) , wobei \\(\\Theta\\) die Multiplikation der Gewichte mit den Eingangsgr\u00f6\u00dfen plus den Bias Werten ist. In unserem Fall w\u00e4re also \\(\\Theta=(O_1*W_4+O_2*W_5+O_3*W_6)+B_4\\) . \\(\\frac {\\delta \\Theta}{\\delta W_4}\\) Die Ableitung dieses Wertes nach \\(W_4\\) Die Fehlerfunktion war \\(E(Y')=-((1-Y)*ln((1-Y'))\\) und das abgeleitet nach Y' ergibt \\(\\frac {\\delta E(Y')} {\\delta Y'}=\\frac{(1 - Y)}{(1 - Y')}\\) , bzw. mit unseren Werten \\(\\frac{(1 - 0)}{(1 - 0.6017)}=2.5107\\) Der Wert Y' ergibt sich aus der Sigmoid Funktion \\(\\frac{1}{1 + e^{-\\Theta}}\\) . Die Ableitung der Sigmoid Funktion nach \\(\\Theta\\) ergibt \\(\\frac {\\delta Y'}{\\delta \\Theta}=Y'*(1-Y')=0.6017*(1-0.6017)=0.2397\\) Als letztes erfolgt die Ableitung von \\(\\Theta\\) nach \\(W_4\\) , wobei \\(\\Theta=(O_1*W_4+O_2*W_5+O_3*W_6)+B_4\\) und die Ableitung ist \\(\\frac {\\delta \\Theta}{\\delta W_4}=O_1=0.6696\\) . Durch Anwendung der Kettenregel erhalten wir also: \\(\\frac{\\delta E(Y')} {\\delta W_4}=2.5107*0.2397*0.6696=0.4030\\) , d.h. um unsere Loss Funktion zu optimieren m\u00fcssen wir diesen Wert von \\(W_4\\) abziehen, dieses geschieht aber unter Ber\u00fccksichtigung der Learning Rate . Es gilt: \\(W_{4Neu}=W_4-\\alpha*\\frac{\\delta E(Y')} {\\delta W_4}=-1.0-0.1*0.4030=-1.0403\\) , d.h. unser Wert \\(W_4\\) hat sich erniedrigt von -1.0 auf -1.0403. In \u00e4hnlicher Weise m\u00fcssen wir nun mit den anderen Gewichten \\(W_5\\) und \\(W_6\\) und den Bias Wert \\(B_4\\) verfahren, also jeweils fie Fehlerfunktion ableiten. Dann hat man die Gewichte und Bias Werte der Ausgabeschicht neu berechnet und kann sich dann an die Gewichte und Bias Werte der Hidden-Layer und Eingabeschicht (Input Layer) machen. Implementierung in Python Zum Implementieren dieses Modells nutzen wir die Bibliothek Tensorflow . Der folgende Code importiert die notwendige Bibliothek und legt die Daten f\u00fcr die Lichtsteuerung in einem NumPy Array an. Ein Eingangswert von \"0\" ist jedoch f\u00fcr ein neuronales Netz eher ein ung\u00fcnstiger Wert (da damit die Gewichte schlecht zu trainieren sind). Von daher wird der Wert von \"0\" ersetzt durch einen Wert von \"-1\" import tensorflow as tf import numpy as np # Daten definieren data = np . array ([[ 1 , - 1 , 0 ], [ 1 , 1 , 1 ], [ - 1 , - 1 , 1 ], [ - 1 , 1 , 1 ]]) Anschlie\u00dfend m\u00fcssen die Daten in Eingangs- und Ausgangsdaten aufgeteilt werden. F\u00fcr unsere Aufgabenstellung enthalten die ersten beiden Spalten die Eingangsdaten (Tag_Nacht und Person) und die letzte Spalte die Ausgangsdaten. # Aufteilen der Daten in Features (X) und Labels (y) X = data [:, : - 1 ] # Eingangsdaten: Erste beiden Spalten y = data [:, - 1 ] # Ausgangsdaten: Letzte Spalte Anschlie\u00dfend muss das neuronale Netz aufgebaut werden: . # Definition des neuronalen Netzwerks model = tf . keras . Sequential ([ tf . keras . layers . Dense ( 3 , activation = 'tanh' , input_dim = 2 ), # Hidden-Layer mit 3 Neuronen tf . keras . layers . Dense ( 1 , activation = 'sigmoid' ) # Ausgangsneuron ]) Unser Netz hat 3 Neuronen als hidden layer . Auf dieser Ebene verwenden wir die tanh Funktion als Aktivierungsfunktion. Von der Eingangsebene erhalten wird 2 Daten input_dim . Die drei Neuronen der hidden layer speisen ein Neuron auf der Ausgabeebene ( output layer ), hier verwenden wir die Sigmoid Funktion als Aktivierungsfunktion. Nachdem das Neuronale Netz gebaut wurde, muss es 'compiliert' werden. # Kompilieren des Modells model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) Angegeben wird hier der Optimizer adam , der dazu dient das absolute Minimum im Fehler zu finden. Nachdem das Neuronale Netz kompiliert wird, kann es angelernt werden. model . fit ( X , y , epochs = 600 ) Eine Epoche ist dabei der Zyklus von Forward- und Backward Propagation mit allen Testdaten. Nach dem Training kann das Modell \u00fcberpr\u00fcft werden. # Beispiel-Eingabe f\u00fcr die Vorhersage (Nacht und Person anwesend) input_data = np . array ([[ - 1 , 1 ]]) # Vorhersage f\u00fcr die Klasse \"Lampe\" (Bin\u00e4rklassifikation) prediction = model . predict ( input_data ) print ( prediction ) Das Neuronale Netz liefert z.B. einen Wert von 0.9583882 , welches in unserem Beispiel bedeuten w\u00fcrde, dass die Lampe einzuschalten ist. Dieses w\u00e4re auch korrekt f\u00fcr die Annahme, dass es Nacht ist (-1) und eine Person im Raum anwesend w\u00e4re (1). Die berechneten Gewichte und Bias Werte im Modell k\u00f6nnen \u00fcber folgendes Python Skript ausgegeben werden. for layer in model . layers : weights = layer . get_weights () print ( 'Gewichtungen:' , weights ) F\u00fcr das Output Layer sieht die Ausgabe z.B. wie folgt aus: Gewichtungen: [array([[ 1.6754032 ], [-0.9415936 ], [ 0.18030357]], dtype=float32), array([0.34544384], dtype=float32)] . Das letzte Neuron wird gespeist aus den drei Neuronen der hidden Layer. Die Gewichte sind hier \\(W_4=1.6754\\) , \\(W_5=-0.94159\\) und \\(W_6=0.180303\\) . Das zweite Array listet den Bias Wert \\(b_4=0.34544\\) . Metriken Metriken geben Auskunft \u00fcber die Qualit\u00e4t eines Vorhersagemodells. F\u00fcr unser bin\u00e4res Vorhersagemodell ist es recht einfach eine Metrik zu bestimmen. Man setzt einfach die korrekten Vorhersagen ( \\(T_P\\) und \\(T_N\\) ) allen Vorhersagen ( \\(P + N\\) ) ins Verh\u00e4ltnis und erh\u00e4lt die Bin\u00e4re Genauigkeit ( Binary Accuracy ). Nach dem Trainieren des neuronalen Netzes Sagt unser Netz das Ergebnis wie folgt voraus: [[0.3770217] [0.4830392] [0.5109197] [0.6186656]] W\u00fcrde man die Werte runden so erh\u00e4lt man: Tag / Nacht ( \\(X_1\\) ) Person ( \\(X_2\\) ) Lampe ( \\(Y\\) ) Vorhersage Tag (1) nein (-1) aus (0) 0 Tag (1) ja (1) an (1) 0 Nacht (-1) nein (-1) an (1) 1 Nacht (-1) ja (1) an (1) 1 Unser Modell hat also 2 mal den korrekten Wert f\u00fcr 1 bestimmt ( \\(T_P\\) ) und einmal den korrekten Wert f\u00fcr 0 ( \\(F_P\\) ). Einmal lag das Modell falsch, es w\u00e4re bei \"Tag\" und eine Person anwesend eine 1 heraus kommen m\u00fcssen, dass Modell hat jedoch eine 0 bestimmt ( \\(F_P\\) ). F\u00fcr unser Vorhersagemodell ergebe sich folgende Darstellung: Nun kann f\u00fcr die Vorhersage die Accuracy bestimmt werden: \\(A_{CC}=\\frac {T_P+T_N}{P+N}=\\frac {2+1}{3+1}=0.75\\) Die Accuracy gibt den Prozentwert an, wie viele Vorhersagen korrekt waren. Eine weitere Metrik ist die Genauigkeit ( Precision ). Die Precision ist definiert als: \\(P_{RE}=\\frac {T_P}{T_P+F_P}=\\frac {2}{2+0}=1.0\\) Die Precision misst, wie viele der vom Modell als positiv klassifizierten Beispiele tats\u00e4chlich positiv sind. Eine weitere Metrik ist der Recall (die Sensitivit\u00e4t), er ist definiert als: \\(R_{call}=\\frac {T_P}{T_P+F_N}=\\frac {2}{2+1}=0.6666\\) Der Recall misst, wie viele der tats\u00e4chlich positiven Beispiele vom Modell korrekt als positiv identifiziert wurden. Als letzte Metrik spielt noch der F1 Score eine Rolle, er ist bestimmt als das harmonische Mittel zwischen Precision und Recall . \\(F1_{Score}=2*\\frac {P_{RE}*R_{call}}{P_{RE}+R_{call}}=2*\\frac {1*0.6666}{1+0.6666}=0.80\\) Der F1-Score ist eine Metrik, die das harmonische Mittel aus Precision (Pr\u00e4zision) und Recall (Sensitivit\u00e4t) bildet. Er bietet eine einzige Metrik, die versucht, ein Gleichgewicht zwischen diesen beiden Aspekten herzustellen. S Wenn Sie ein Modell haben, dessen F1-Score hoch ist, bedeutet das im Allgemeinen, dass sowohl die Pr\u00e4zision als auch der Recall des Modells gut sind. Ein hoher F1-Score deutet darauf hin, dass das Modell nicht nur eine hohe Trefferquote hat (hoher Recall), sondern auch eine hohe Genauigkeit in den Vorhersagen, die es macht (hohe Pr\u00e4zision). \u00dcber die Funktion evaluate ist es m\u00f6glich die Genauigkeit des Modells zu bestimmen. Wichtig dabei ist, dass bereits die Metriken beim Kompilieren des Modells mit angegeben werden m\u00fcssen. from tensorflow.keras.metrics import Precision , Recall # [....] # Kompilieren und trainieren des Modells model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' , Precision (), Recall ()]) Anschlie\u00dfend k\u00f6nnen die Metriken dann anhand der Test-Daten bestimmt werden. data = np . array ([[ 1 , - 1 , 0 ], [ 1 , 1 , 1 ], [ - 1 , - 1 , 1 ], [ - 1 , 1 , 1 ]]) X = data [:, : - 1 ] # Eingangsdaten: Erste beiden Spalten y = data [:, - 1 ] # Ausgangsdaten: Letzte Spalte loss , accuracy , precision , recall = model . evaluate ( X , y ) print ( 'Test accuracy:' , accuracy ) print ( 'Test precision:' , precision ) print ( 'Test recall:' , recall ) Und man erh\u00e4lt die vorher ermittelten Werte: Test accuracy: 0.75 Test precision: 1.0 Test recall: 0.6666666865348816 Wenn Sie die bisherigen Informationen aufmerksam gelesen haben, sollte Ihnen die folgende Aufgabe nicht schwer fallen. . Aufgabe: Erstellen Sie auf der Grundlage der bisherigen \u00dcberlegungen ihre Modell eines neuronalen Netzwerkes f\u00fcr die intelligente Lichtsteuerung \"AI-Light\" in Python. Trainieren Sie das Modell entsprechend den von Ihnen in Aufgabe 1 gew\u00e4hlten Anforderungen. Beurteilen Sie die Qualit\u00e4t des Modells und ermitteln Sie geeignete Metriken. Diskutieren Sie wie die Qualit\u00e4t des Modells gesteigert werden kann. Erstellen Sie eine Dokumentation ihres Vorgehens und der Ergebnisse und der \u00dcberlegung (ca. 1 bis 2 Seiten) Ein eigenes Netzwerk entwerfen und trainieren Aufgabe: Bilden Sie Arbeitsgruppen zur ca. 3-4 Mitsch\u00fclern und w\u00e4hlen Sie einen anderen Datensatz (s.u.) f\u00fcr ein bin\u00e4res Klassifizierungsproblem und entwerfen und trainieren Sie ein neuronales Netz. Dokumentieren Sie ihr Vorgehen und Ihr Ergebnis und pr\u00e4sentieren Sie dieses abschlie\u00dfend der Klasse. Die Dokumentation sollte beinhalten: Vorstellen des Datensatzes Entwurf des Netzwerkes inkl. gew\u00e4hlter Layer und Aktivierungsfunktionen Beurteilen der Qualit\u00e4t des Vorhersagemodells (Metriken) M\u00f6gliche Datens\u00e4tze w\u00e4ren (Sie k\u00f6nnen aber auch gerne einen eigenen Datensatz ausw\u00e4hlen): Titanic-Datensatz : Der Titanic-Datensatz enth\u00e4lt Informationen \u00fcber Passagiere an Bord des Schiffes Titanic, einschlie\u00dflich Merkmalen wie Alter, Geschlecht, Klasse und \u00dcberlebensstatus. Dieser Datensatz ist gut geeignet f\u00fcr bin\u00e4re Klassifizierungsaufgaben und kann auch zur Vorhersage des \u00dcberlebens von Passagieren auf anderen Schiffsreisen verwendet werden. URL: https://www.kaggle.com/c/titanic/data Bank Marketing-Datensatz : Dieser Datensatz enth\u00e4lt Informationen zu Kunden einer portugiesischen Bank und ob sie Ja oder Nein f\u00fcr ein Termingeld-Abonnement abgeschlossen haben. Es enth\u00e4lt eine Vielzahl von Kundenmerkmalen wie Alter, Beruf, Familienstand usw., die verwendet werden k\u00f6nnen, um vorherzusagen, ob ein Kunde ein Abonnement abschlie\u00dfen wird oder nicht. URL: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing Breast Cancer Wisconsin (diagnostic) Dataset : Dieser Datensatz enth\u00e4lt Details zu den Zellkernmerkmalen von malignen und benignen Brustgewebeproben sowie einer Diagnose, ob eine Probe maligne oder benign ist. Der Datensatz ist gut geeignet f\u00fcr bin\u00e4re Klassifizierungsaufgaben zum Erkennen von Brustkrebs. URL: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic) Fragen zum Verst\u00e4ndnis Was ist ein Neuron in einem k\u00fcnstlichen neuronalen Netzwerk? [ ] Eine Datenstruktur, die Informationen speichert [ ] Eine Funktion, die das Ergebnis des Netzes berechnet [ ] Eine Einheit, die Eingaben empf\u00e4ngt und eine Ausgabe basierend auf diesen Eingaben generiert [ ] Ein spezielles Modell von neuronalen Netzwerken Welche der folgenden ist KEINE typische Schicht in einem neuronalen Netzwerk? [ ] Eingabeschicht [ ] Ausgabeschicht [ ] Versteckte Schicht [ ] Datenbankschicht Was ist der Zweck einer Aktivierungsfunktion in einem neuronalen Netzwerk? [ ] Um die Genauigkeit des Modells zu erh\u00f6hen [ ] Um die Gr\u00f6\u00dfe des Modells zu reduzieren [ ] Um nicht-lineare Transformationen einzuf\u00fchren [ ] Um die Geschwindigkeit des Trainings zu erh\u00f6hen Welches Problem kann durch zu viel Training eines neuronalen Netzwerks auftreten? [ ] Overfitting [ ] Underfitting [ ] Datenverlust [ ] Netzwerkfehler Welches der folgenden Verfahren wird verwendet, um ein neuronales Netzwerk zu trainieren? [ ] Random Forest Algorithmus [ ] Backpropagation [ ] Linear Regression [ ] Naive Bayes Klassifikation Was ist die Hauptfunktion der \"Verlustfunktion\" (Loss Function) beim Training eines neuronalen Netzwerks? [ ] Sie definiert die Architektur des Netzwerks [ ] Sie bestimmt die Art der Aktivierungsfunktion in den Neuronen [ ] Sie misst, wie gut das Netzwerk die Ausgabe vorhersagt, und wird f\u00fcr die Optimierung des Netzwerks verwendet [ ] Sie dient dazu, das Netzwerk vor Overfitting zu sch\u00fctzen","title":"Neuronale Netze"},{"location":"neuronalesNetz/#neuronale-netze","text":"","title":"Neuronale Netze"},{"location":"neuronalesNetz/#handlungssituation","text":"Die Firma Home-IoT ist eine bekannter Hersteller von Smart Home Produkten. Es ist geplant f\u00fcr diese Firma eine smarte Lichtsteuerung \"AI Light\" zu entwickeln, die an die jeweiligen Anforderungen der Kunden angepasst werden kann. Der Chefentwickler der Abteilung Daten- und Prozessanalyse der ChangeIT GmbH beauftragt Sie damit ein Neuronalen Netz zu entwickeln und dieses f\u00fcr eine exemplarische Anforderung zu trainieren.","title":"Handlungssituation"},{"location":"neuronalesNetz/#ai-light","text":"Die smarte Lichtsteuerung \"AI-Light\" besitzt zwei Sensoren. Pr\u00e4senzerkennung: \u00dcber Sensoren ist das System in der Lage zu erkennen, ob sich Personen im Raum befinden. Tag / Nachterkennung: Das System ist ebenso in der Lage, Tag- / Nachtzeiten zu erkennen.","title":"AI-Light"},{"location":"neuronalesNetz/#kundenanforderungen","text":"Die Kunden stellen dabei unterschiedliche Anforderungen an das System vlg. [^1]. [^1]: Brandt, Y., Eickhoff-Schachtebeck, A. und Strecker, K. (2022) \u201eSchulbuch starkeSeiten Informatik Jahrgang 9/10 Gymnasium Niedersachsen\u201c, Klett-Verlag 2022, ISBN: 978-3-12-007572-1 In den B\u00fcros der Fabrikhalle sollen die Lampen nachts immer leuchten aus Gr\u00fcnden des Einbrecherschutzes, und tags\u00fcber nur, wenn die Mitarbeiter an ihren Pl\u00e4tzen sind. Im B\u00fcrogeb\u00e4ude der Softwarefirma sollen die Lampen nur nachts leuchten, wenn Mitarbeiter da sind. Tags\u00fcber ist es durch die vielen Fenster immer hell genug. Im alten Geb\u00e4ude der Stadtverwaltung m\u00fcssen die Lampen tags\u00fcber angeschaltet sein, wenn Mitarbeiter da sind, da die Fenster zu wenig Licht hereinlassen. Sollten Mitarbeiter auch nachts arbeiten, m\u00fcssen auch dann die Lampen eingeschaltet werden. Sonst k\u00f6nnen sie aus bleiben. Im Haus der Familie Schmidt sollen die Lampen tags\u00fcber an sein, wenn jemand zuhause ist, und aus sein, wenn keiner da ist. Nachts sollen die Lampen aus sein, wenn die Bewohner im Haus sind und schlafen, und aus Gr\u00fcnden des Einbrecherschutzes an sein, wenn niemand da ist","title":"Kundenanforderungen"},{"location":"neuronalesNetz/#aufgabe-wahrheitstabelle-erzeugen","text":"W\u00e4hlen Sie sich eine Anforderung des Kunden aus und erstellen Sie eine Wahrheitstabelle, die alle m\u00f6glichen Eingangssignale darstellt und ob sich die Lampe in der jeweiligen Situation an oder ausgehen soll.","title":"Aufgabe Wahrheitstabelle erzeugen"},{"location":"neuronalesNetz/#losung-wahrheitstabelle-erzeugen","text":"F\u00fcr ein B\u00fcro in der Fabrikhalle k\u00f6nnte diese Tabelle wie folgt aussehen. Tag / Nacht ( \\(X_1\\) ) Person ( \\(X_2\\) ) Lampe ( \\(Y\\) ) Tag (1) nein (0) aus (0) Tag (1) ja (1) an (1) Nacht (0) nein (0) an (1) Nacht (0) ja (1) an (1)","title":"L\u00f6sung Wahrheitstabelle erzeugen"},{"location":"neuronalesNetz/#das-neuronale-netz","text":"Ein neuronales Netzwerk ist ein Modell, das von der Funktionsweise des menschlichen Gehirns inspiriert ist. Es besteht aus einer Sammlung miteinander verbundener k\u00fcnstlicher Neuronen, die Informationen verarbeiten und weiterleiten. \u00c4hnlich wie biologische Neuronen empfangen auch k\u00fcnstliche Neuronen Eingaben, verarbeiten diese und geben sie als Ausgabe weiter. Die Eingaben werden gewichtet und durch Aktivierungsfunktionen in eine Ausgabe transformiert. Ein einfaches Neuron (man spricht hier auch von einem Perzeptron ) kann dabei wie folgt aussehen. Die Werte \\(X_1\\) bis \\(X_3\\) sind z.B. Sensorwerte oder Werte aus einer vorherigen Stufe. \\(G_1\\) bis \\(G_3\\) sind Gewichtungsfaktoren die im laufe des Trainings des Neuronalen Netzes angepasst werden und zur Initialisierung auf zuf\u00e4llige Werte gesetzt werden. Als Aktivierungsfunktionen k\u00f6nnen Funktionen genutzt werden, wie die Sigmoid-, ReLu- oder Tanh-Funktion bzw. einfache Schwellwerte. Die Funktion gibt an, ob und wie stark das Neuron \"feuert\". Aufgabe : Berechnen Sie den Ausgabewert \\(Y\\) f\u00fcr das Neuron, wenn als Aktivierungsfunktion ein Tanh-Funktion genutzt wird und folgende Eingangsvektoren und Gewichtsvektoren vorliegen. \\(G = \\begin{pmatrix} G_1 \\\\ G_2 \\\\ G_3 \\end{pmatrix}=\\begin{pmatrix} 0.4 \\\\ 0.2 \\\\ -0.5 \\end{pmatrix}\\) \\(X = \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix}=\\begin{pmatrix} 0.7 \\\\ -0.1 \\\\ -0.4 \\end{pmatrix}\\) L\u00f6sung : \\(Y_*= X_1*G_1+X_2*G_2+X_3*G_3\\) \\(Y_*=0.4*0.7+0.2*(-0.1)+(-0.5)*(-0.4)=0.46\\) \\(Y=tanh(0.46)=0.43\\) Diese Ausgaben k\u00f6nnen dann wieder als Eingaben f\u00fcr andere Neuronen dienen, wodurch das Netzwerk Schicht f\u00fcr Schicht komplexere Berechnungen durchf\u00fchren kann. Ein neuronales Netzwerk lernt, indem es seine Gewichte anpasst, basierend auf dem Vergleich zwischen seinen Ausgaben und den erwarteten Ausgaben. Dieser Lernprozess wird durch mathematische Algorithmen unterst\u00fctzt, die als Backpropagation bezeichnet werden. Durch wiederholtes Training auf gro\u00dfen Datens\u00e4tzen kann ein neuronales Netzwerk Muster erkennen, Zusammenh\u00e4nge verstehen und Vorhersagen treffen. Obwohl neuronale Netzwerke nicht genau die gleiche Funktionsweise wie biologische Neuronen haben, sind sie dennoch stark von ihnen inspiriert. Die Idee besteht darin, komplexe Informationsverarbeitung nach dem Vorbild des Gehirns zu erm\u00f6glichen und dadurch komplexe Aufgaben in Bereichen wie Bilderkennung, Spracherkennung, Textanalyse und vielem mehr zu l\u00f6sen. F\u00fcr die Steuerung der Lichtanlage ben\u00f6tigen wir pro Sensors ein Neuron, in diesem Fall w\u00e4re das demnach zwei Neuronen in der Eingangsschicht ( input Layer ). Die Ausgangsschicht ( output layer ) steuert mir einem Neuron die Lampe. F\u00fcr einen ersten Ansatz w\u00e4hlen wir 3 Neuronen in der hidden Layer . Dementsprechend hat unser Neuronales Netz folgendes Aussehen. .","title":"Das neuronale Netz"},{"location":"neuronalesNetz/#initialisierung-des-netzes","text":"Die Gewichte \\(W_{11}\\) bis \\(W_{32}\\) , sowie \\(W_4\\) bis \\(W_5\\) werden initial auf zuf\u00e4llige Werte zwischen +1 / -1 gesetzt. \\(W_1 = \\begin{pmatrix} w_{11} & w_{21} & w_{31} \\\\ w_{12} & w_{22} & w_{32} \\end{pmatrix}== \\begin{pmatrix} -0.19 & -0.96 & 0.43 \\\\ -0.23 & 0.97 & 0.46 \\end{pmatrix}\\) \\(W_2 = \\begin{pmatrix} w_4 \\\\ w_5 \\\\ w_6 \\end{pmatrix}=\\begin{pmatrix} -1.0 \\\\ -0.21 \\\\ 0.16 \\end{pmatrix}\\) Als Aktivierungsfunktion \\(f(x)\\) k\u00f6nnen unterschiedliche Funktionen wie die Sigmoid-, ReLu- oder Tanh-Funktion genutzt werden. Wie verwenden in diesem Beispiel die \\(tanh\\) Funktion im hidden Layer und die Sigmoid Funktion in der Ausgabeschicht. Die Sigmoid Funktion ist dabei wie folgt definiert. \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) Hier sind beide Funktion nochmals grafisch dargestellt.","title":"Initialisierung des Netzes"},{"location":"neuronalesNetz/#forward-propagation","text":"Um nun den Wert der Zwischenschicht \\(O\\) zu ermitteln m\u00fcssen wir folgende Rechnungen durchf\u00fchren. \\(O_1= tanh((X_1*W_{11}+X_2*W_{12})+b_1)\\) \\(O_2= tanh((X_1*W_{21}+X_2*W_{22})+b_2)\\) \\(O_3= tanh((X_1*W_{31}+X_2*W_{32})+b_3)\\) Im ersten Durchgang nehmen wird den X-Vektor wie folgt an: \\(X = \\begin{pmatrix} X_1 & X_2 \\end{pmatrix}=\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\) Die Bias Werte legen wir zun\u00e4chst auf 1 fest. \\(B_1 = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{pmatrix}=\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\) \\(B_4 = 1\\) Mit den angenommen Werten kann nun weiter gerechnet werden: \\(O_1= tanh((X_1*W_{11}+X_2*W_{12})+b_1)= tanh((1*-0.19+0*-0.23)+1)=0.6696\\) \\(O_2= tanh((X_1*W_{21}+X_2*W_{22})+b_2) = tanh((1*-0.96+0*0.97)+1)=0.0006\\) \\(O_3= tanh((X_1*W_{31}+X_2*W_{32})+b_3) = tanh((1*-0.43+0*0.46)+1)=0.5154\\) F\u00fcr die output Layer ergeben sich folgende Werte. \\(O_4'= (O_1*W_4+O_2*W_5+O_3*W_6)+b_4\\) \\(O_4' = (0.6696*-1.0+0.0006*-0.21+0.5154*0.16)+1=0.4127\\) \\(O_4 = Y'= \\frac{1}{1 + e^{-O_4'}}=0.6017\\) Als Ergebnis w\u00fcrde das Neuronale Netz also die Lampe einschalten, was leider falsch w\u00e4re, denn wir hatten ja angegeben, dass es Nacht ist (1) und keine Person anwesend ist (0).","title":"Forward Propagation"},{"location":"neuronalesNetz/#fehlerfunktionen-loss-function","text":"Unser Netzwerk hat also einen Fehler gemacht, dieser kann z.B. wie folgt bestimmt werden: \\(E(Y')=Y-Y'=0-0.6017=-0.6017=-60.17\\%\\) Eine andere (besser geeignete) Fehlerfunktion ist der Binary Cross Entropy Error . Dieser ist wie folgt bestimmt: \\(E(Y')=-(Y*ln(Y')+(1-Y)*ln(1-Y'))\\) F\u00fcr unsere Beispiel w\u00fcrde sich also folgender Binary Cross Entropy Error ergeben: \\(E(Y')=-(0*ln(0.6017)+(1-0)*ln(1-0.6017))=0.9205\\) Neben dieser Fehlerfunktion gibt es noch weitere, wie z.B. Categorical Cross Entropy , Mean Squared Error und Cosine Distance . F\u00fcr unsere bin\u00e4res Klassifizierungsproblem (Lampe an/aus) eignet sich jedoch am besten die zuvor berechnete Binary Cross Entropy Error.","title":"Fehlerfunktionen (Loss-Function)"},{"location":"neuronalesNetz/#back-propagation","text":"Damit das Modell nun lernen kann (also sich der Fehler minimiert), m\u00fcssen die Gewichte und die Bias Werte angepasst werden. Dieses geschieht in der Back-Propagation. Neben dem Cross Entropy Error definieren wir noch einen weiteren Parameter, die Lernrate LearningRate . Sie zumeist mit \\(\\alpha\\) bezeichnet und liegt auf einem Wert z.B. \\(\\alpha=0.1\\) . Die LearningRate ist dabei nur ein Faktor um wie viel die Gewichte und Bias Werte angepasst werden, also wie schnell das Neuronale Netz lernen soll. In der Back Propagation geht es nun darum die Loss Funktion nach allen Gewichten und Bias Werten abzuleiten. Durch das Anwenden der Kettenregel kann der Rechenaufwand jedoch reduziert werden. Wir wollen uns nur auf einen Teil unseres Netzwerkes konzentrieren und \"trainieren\" jetzt den Parameter \\(W_4\\) . . \\(\\frac{\\delta E(Y')} {\\delta W_4}=\\frac {\\delta E(Y')} {\\delta Y'}* \\frac {\\delta Y'}{\\delta \\Theta}*\\frac {\\delta \\Theta}{\\delta W_4}\\) Dabei ist: \\(\\frac{\\delta E(Y')} {\\delta W_4}\\) die Partielle Ableitung der Loss Funktion nach dem Gewicht \\(W_4\\) . \\(\\frac {\\delta E(Y')} {\\delta Y'}\\) Ableitung der Loss Funktion nach Y' \\(\\frac {\\delta Y'}{\\delta \\Theta}\\) Ableitung von Y' nach \\(\\Theta\\) , wobei \\(\\Theta\\) die Multiplikation der Gewichte mit den Eingangsgr\u00f6\u00dfen plus den Bias Werten ist. In unserem Fall w\u00e4re also \\(\\Theta=(O_1*W_4+O_2*W_5+O_3*W_6)+B_4\\) . \\(\\frac {\\delta \\Theta}{\\delta W_4}\\) Die Ableitung dieses Wertes nach \\(W_4\\) Die Fehlerfunktion war \\(E(Y')=-((1-Y)*ln((1-Y'))\\) und das abgeleitet nach Y' ergibt \\(\\frac {\\delta E(Y')} {\\delta Y'}=\\frac{(1 - Y)}{(1 - Y')}\\) , bzw. mit unseren Werten \\(\\frac{(1 - 0)}{(1 - 0.6017)}=2.5107\\) Der Wert Y' ergibt sich aus der Sigmoid Funktion \\(\\frac{1}{1 + e^{-\\Theta}}\\) . Die Ableitung der Sigmoid Funktion nach \\(\\Theta\\) ergibt \\(\\frac {\\delta Y'}{\\delta \\Theta}=Y'*(1-Y')=0.6017*(1-0.6017)=0.2397\\) Als letztes erfolgt die Ableitung von \\(\\Theta\\) nach \\(W_4\\) , wobei \\(\\Theta=(O_1*W_4+O_2*W_5+O_3*W_6)+B_4\\) und die Ableitung ist \\(\\frac {\\delta \\Theta}{\\delta W_4}=O_1=0.6696\\) . Durch Anwendung der Kettenregel erhalten wir also: \\(\\frac{\\delta E(Y')} {\\delta W_4}=2.5107*0.2397*0.6696=0.4030\\) , d.h. um unsere Loss Funktion zu optimieren m\u00fcssen wir diesen Wert von \\(W_4\\) abziehen, dieses geschieht aber unter Ber\u00fccksichtigung der Learning Rate . Es gilt: \\(W_{4Neu}=W_4-\\alpha*\\frac{\\delta E(Y')} {\\delta W_4}=-1.0-0.1*0.4030=-1.0403\\) , d.h. unser Wert \\(W_4\\) hat sich erniedrigt von -1.0 auf -1.0403. In \u00e4hnlicher Weise m\u00fcssen wir nun mit den anderen Gewichten \\(W_5\\) und \\(W_6\\) und den Bias Wert \\(B_4\\) verfahren, also jeweils fie Fehlerfunktion ableiten. Dann hat man die Gewichte und Bias Werte der Ausgabeschicht neu berechnet und kann sich dann an die Gewichte und Bias Werte der Hidden-Layer und Eingabeschicht (Input Layer) machen.","title":"Back Propagation"},{"location":"neuronalesNetz/#implementierung-in-python","text":"Zum Implementieren dieses Modells nutzen wir die Bibliothek Tensorflow . Der folgende Code importiert die notwendige Bibliothek und legt die Daten f\u00fcr die Lichtsteuerung in einem NumPy Array an. Ein Eingangswert von \"0\" ist jedoch f\u00fcr ein neuronales Netz eher ein ung\u00fcnstiger Wert (da damit die Gewichte schlecht zu trainieren sind). Von daher wird der Wert von \"0\" ersetzt durch einen Wert von \"-1\" import tensorflow as tf import numpy as np # Daten definieren data = np . array ([[ 1 , - 1 , 0 ], [ 1 , 1 , 1 ], [ - 1 , - 1 , 1 ], [ - 1 , 1 , 1 ]]) Anschlie\u00dfend m\u00fcssen die Daten in Eingangs- und Ausgangsdaten aufgeteilt werden. F\u00fcr unsere Aufgabenstellung enthalten die ersten beiden Spalten die Eingangsdaten (Tag_Nacht und Person) und die letzte Spalte die Ausgangsdaten. # Aufteilen der Daten in Features (X) und Labels (y) X = data [:, : - 1 ] # Eingangsdaten: Erste beiden Spalten y = data [:, - 1 ] # Ausgangsdaten: Letzte Spalte Anschlie\u00dfend muss das neuronale Netz aufgebaut werden: . # Definition des neuronalen Netzwerks model = tf . keras . Sequential ([ tf . keras . layers . Dense ( 3 , activation = 'tanh' , input_dim = 2 ), # Hidden-Layer mit 3 Neuronen tf . keras . layers . Dense ( 1 , activation = 'sigmoid' ) # Ausgangsneuron ]) Unser Netz hat 3 Neuronen als hidden layer . Auf dieser Ebene verwenden wir die tanh Funktion als Aktivierungsfunktion. Von der Eingangsebene erhalten wird 2 Daten input_dim . Die drei Neuronen der hidden layer speisen ein Neuron auf der Ausgabeebene ( output layer ), hier verwenden wir die Sigmoid Funktion als Aktivierungsfunktion. Nachdem das Neuronale Netz gebaut wurde, muss es 'compiliert' werden. # Kompilieren des Modells model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) Angegeben wird hier der Optimizer adam , der dazu dient das absolute Minimum im Fehler zu finden. Nachdem das Neuronale Netz kompiliert wird, kann es angelernt werden. model . fit ( X , y , epochs = 600 ) Eine Epoche ist dabei der Zyklus von Forward- und Backward Propagation mit allen Testdaten. Nach dem Training kann das Modell \u00fcberpr\u00fcft werden. # Beispiel-Eingabe f\u00fcr die Vorhersage (Nacht und Person anwesend) input_data = np . array ([[ - 1 , 1 ]]) # Vorhersage f\u00fcr die Klasse \"Lampe\" (Bin\u00e4rklassifikation) prediction = model . predict ( input_data ) print ( prediction ) Das Neuronale Netz liefert z.B. einen Wert von 0.9583882 , welches in unserem Beispiel bedeuten w\u00fcrde, dass die Lampe einzuschalten ist. Dieses w\u00e4re auch korrekt f\u00fcr die Annahme, dass es Nacht ist (-1) und eine Person im Raum anwesend w\u00e4re (1). Die berechneten Gewichte und Bias Werte im Modell k\u00f6nnen \u00fcber folgendes Python Skript ausgegeben werden. for layer in model . layers : weights = layer . get_weights () print ( 'Gewichtungen:' , weights ) F\u00fcr das Output Layer sieht die Ausgabe z.B. wie folgt aus: Gewichtungen: [array([[ 1.6754032 ], [-0.9415936 ], [ 0.18030357]], dtype=float32), array([0.34544384], dtype=float32)] . Das letzte Neuron wird gespeist aus den drei Neuronen der hidden Layer. Die Gewichte sind hier \\(W_4=1.6754\\) , \\(W_5=-0.94159\\) und \\(W_6=0.180303\\) . Das zweite Array listet den Bias Wert \\(b_4=0.34544\\) .","title":"Implementierung in Python"},{"location":"neuronalesNetz/#metriken","text":"Metriken geben Auskunft \u00fcber die Qualit\u00e4t eines Vorhersagemodells. F\u00fcr unser bin\u00e4res Vorhersagemodell ist es recht einfach eine Metrik zu bestimmen. Man setzt einfach die korrekten Vorhersagen ( \\(T_P\\) und \\(T_N\\) ) allen Vorhersagen ( \\(P + N\\) ) ins Verh\u00e4ltnis und erh\u00e4lt die Bin\u00e4re Genauigkeit ( Binary Accuracy ). Nach dem Trainieren des neuronalen Netzes Sagt unser Netz das Ergebnis wie folgt voraus: [[0.3770217] [0.4830392] [0.5109197] [0.6186656]] W\u00fcrde man die Werte runden so erh\u00e4lt man: Tag / Nacht ( \\(X_1\\) ) Person ( \\(X_2\\) ) Lampe ( \\(Y\\) ) Vorhersage Tag (1) nein (-1) aus (0) 0 Tag (1) ja (1) an (1) 0 Nacht (-1) nein (-1) an (1) 1 Nacht (-1) ja (1) an (1) 1 Unser Modell hat also 2 mal den korrekten Wert f\u00fcr 1 bestimmt ( \\(T_P\\) ) und einmal den korrekten Wert f\u00fcr 0 ( \\(F_P\\) ). Einmal lag das Modell falsch, es w\u00e4re bei \"Tag\" und eine Person anwesend eine 1 heraus kommen m\u00fcssen, dass Modell hat jedoch eine 0 bestimmt ( \\(F_P\\) ). F\u00fcr unser Vorhersagemodell ergebe sich folgende Darstellung: Nun kann f\u00fcr die Vorhersage die Accuracy bestimmt werden: \\(A_{CC}=\\frac {T_P+T_N}{P+N}=\\frac {2+1}{3+1}=0.75\\) Die Accuracy gibt den Prozentwert an, wie viele Vorhersagen korrekt waren. Eine weitere Metrik ist die Genauigkeit ( Precision ). Die Precision ist definiert als: \\(P_{RE}=\\frac {T_P}{T_P+F_P}=\\frac {2}{2+0}=1.0\\) Die Precision misst, wie viele der vom Modell als positiv klassifizierten Beispiele tats\u00e4chlich positiv sind. Eine weitere Metrik ist der Recall (die Sensitivit\u00e4t), er ist definiert als: \\(R_{call}=\\frac {T_P}{T_P+F_N}=\\frac {2}{2+1}=0.6666\\) Der Recall misst, wie viele der tats\u00e4chlich positiven Beispiele vom Modell korrekt als positiv identifiziert wurden. Als letzte Metrik spielt noch der F1 Score eine Rolle, er ist bestimmt als das harmonische Mittel zwischen Precision und Recall . \\(F1_{Score}=2*\\frac {P_{RE}*R_{call}}{P_{RE}+R_{call}}=2*\\frac {1*0.6666}{1+0.6666}=0.80\\) Der F1-Score ist eine Metrik, die das harmonische Mittel aus Precision (Pr\u00e4zision) und Recall (Sensitivit\u00e4t) bildet. Er bietet eine einzige Metrik, die versucht, ein Gleichgewicht zwischen diesen beiden Aspekten herzustellen. S Wenn Sie ein Modell haben, dessen F1-Score hoch ist, bedeutet das im Allgemeinen, dass sowohl die Pr\u00e4zision als auch der Recall des Modells gut sind. Ein hoher F1-Score deutet darauf hin, dass das Modell nicht nur eine hohe Trefferquote hat (hoher Recall), sondern auch eine hohe Genauigkeit in den Vorhersagen, die es macht (hohe Pr\u00e4zision). \u00dcber die Funktion evaluate ist es m\u00f6glich die Genauigkeit des Modells zu bestimmen. Wichtig dabei ist, dass bereits die Metriken beim Kompilieren des Modells mit angegeben werden m\u00fcssen. from tensorflow.keras.metrics import Precision , Recall # [....] # Kompilieren und trainieren des Modells model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' , Precision (), Recall ()]) Anschlie\u00dfend k\u00f6nnen die Metriken dann anhand der Test-Daten bestimmt werden. data = np . array ([[ 1 , - 1 , 0 ], [ 1 , 1 , 1 ], [ - 1 , - 1 , 1 ], [ - 1 , 1 , 1 ]]) X = data [:, : - 1 ] # Eingangsdaten: Erste beiden Spalten y = data [:, - 1 ] # Ausgangsdaten: Letzte Spalte loss , accuracy , precision , recall = model . evaluate ( X , y ) print ( 'Test accuracy:' , accuracy ) print ( 'Test precision:' , precision ) print ( 'Test recall:' , recall ) Und man erh\u00e4lt die vorher ermittelten Werte: Test accuracy: 0.75 Test precision: 1.0 Test recall: 0.6666666865348816 Wenn Sie die bisherigen Informationen aufmerksam gelesen haben, sollte Ihnen die folgende Aufgabe nicht schwer fallen. . Aufgabe: Erstellen Sie auf der Grundlage der bisherigen \u00dcberlegungen ihre Modell eines neuronalen Netzwerkes f\u00fcr die intelligente Lichtsteuerung \"AI-Light\" in Python. Trainieren Sie das Modell entsprechend den von Ihnen in Aufgabe 1 gew\u00e4hlten Anforderungen. Beurteilen Sie die Qualit\u00e4t des Modells und ermitteln Sie geeignete Metriken. Diskutieren Sie wie die Qualit\u00e4t des Modells gesteigert werden kann. Erstellen Sie eine Dokumentation ihres Vorgehens und der Ergebnisse und der \u00dcberlegung (ca. 1 bis 2 Seiten)","title":"Metriken"},{"location":"neuronalesNetz/#ein-eigenes-netzwerk-entwerfen-und-trainieren","text":"Aufgabe: Bilden Sie Arbeitsgruppen zur ca. 3-4 Mitsch\u00fclern und w\u00e4hlen Sie einen anderen Datensatz (s.u.) f\u00fcr ein bin\u00e4res Klassifizierungsproblem und entwerfen und trainieren Sie ein neuronales Netz. Dokumentieren Sie ihr Vorgehen und Ihr Ergebnis und pr\u00e4sentieren Sie dieses abschlie\u00dfend der Klasse. Die Dokumentation sollte beinhalten: Vorstellen des Datensatzes Entwurf des Netzwerkes inkl. gew\u00e4hlter Layer und Aktivierungsfunktionen Beurteilen der Qualit\u00e4t des Vorhersagemodells (Metriken) M\u00f6gliche Datens\u00e4tze w\u00e4ren (Sie k\u00f6nnen aber auch gerne einen eigenen Datensatz ausw\u00e4hlen): Titanic-Datensatz : Der Titanic-Datensatz enth\u00e4lt Informationen \u00fcber Passagiere an Bord des Schiffes Titanic, einschlie\u00dflich Merkmalen wie Alter, Geschlecht, Klasse und \u00dcberlebensstatus. Dieser Datensatz ist gut geeignet f\u00fcr bin\u00e4re Klassifizierungsaufgaben und kann auch zur Vorhersage des \u00dcberlebens von Passagieren auf anderen Schiffsreisen verwendet werden. URL: https://www.kaggle.com/c/titanic/data Bank Marketing-Datensatz : Dieser Datensatz enth\u00e4lt Informationen zu Kunden einer portugiesischen Bank und ob sie Ja oder Nein f\u00fcr ein Termingeld-Abonnement abgeschlossen haben. Es enth\u00e4lt eine Vielzahl von Kundenmerkmalen wie Alter, Beruf, Familienstand usw., die verwendet werden k\u00f6nnen, um vorherzusagen, ob ein Kunde ein Abonnement abschlie\u00dfen wird oder nicht. URL: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing Breast Cancer Wisconsin (diagnostic) Dataset : Dieser Datensatz enth\u00e4lt Details zu den Zellkernmerkmalen von malignen und benignen Brustgewebeproben sowie einer Diagnose, ob eine Probe maligne oder benign ist. Der Datensatz ist gut geeignet f\u00fcr bin\u00e4re Klassifizierungsaufgaben zum Erkennen von Brustkrebs. URL: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)","title":"Ein eigenes Netzwerk entwerfen und trainieren"},{"location":"neuronalesNetz/#fragen-zum-verstandnis","text":"Was ist ein Neuron in einem k\u00fcnstlichen neuronalen Netzwerk? [ ] Eine Datenstruktur, die Informationen speichert [ ] Eine Funktion, die das Ergebnis des Netzes berechnet [ ] Eine Einheit, die Eingaben empf\u00e4ngt und eine Ausgabe basierend auf diesen Eingaben generiert [ ] Ein spezielles Modell von neuronalen Netzwerken Welche der folgenden ist KEINE typische Schicht in einem neuronalen Netzwerk? [ ] Eingabeschicht [ ] Ausgabeschicht [ ] Versteckte Schicht [ ] Datenbankschicht Was ist der Zweck einer Aktivierungsfunktion in einem neuronalen Netzwerk? [ ] Um die Genauigkeit des Modells zu erh\u00f6hen [ ] Um die Gr\u00f6\u00dfe des Modells zu reduzieren [ ] Um nicht-lineare Transformationen einzuf\u00fchren [ ] Um die Geschwindigkeit des Trainings zu erh\u00f6hen Welches Problem kann durch zu viel Training eines neuronalen Netzwerks auftreten? [ ] Overfitting [ ] Underfitting [ ] Datenverlust [ ] Netzwerkfehler Welches der folgenden Verfahren wird verwendet, um ein neuronales Netzwerk zu trainieren? [ ] Random Forest Algorithmus [ ] Backpropagation [ ] Linear Regression [ ] Naive Bayes Klassifikation Was ist die Hauptfunktion der \"Verlustfunktion\" (Loss Function) beim Training eines neuronalen Netzwerks? [ ] Sie definiert die Architektur des Netzwerks [ ] Sie bestimmt die Art der Aktivierungsfunktion in den Neuronen [ ] Sie misst, wie gut das Netzwerk die Ausgabe vorhersagt, und wird f\u00fcr die Optimierung des Netzwerks verwendet [ ] Sie dient dazu, das Netzwerk vor Overfitting zu sch\u00fctzen","title":"Fragen zum Verst\u00e4ndnis"}]}